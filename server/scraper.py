#!/usr/bin/env python3
"""
Google Play Store Review Scraper for Ïö∞Î¶¨Í∞ÄÍ≤å Ìå®ÌÇ§ÏßÄ
"""

import json
import sys
import requests
from datetime import datetime
from google_play_scraper import Sort, reviews
import pandas as pd
import xml.etree.ElementTree as ET

def scrape_google_play_reviews(app_id='com.lguplus.sohoapp', count=100, lang='ko', country='kr'):
    """
    Scrape reviews from Google Play Store
    
    Args:
        app_id: Google Play Store app ID
        count: Number of reviews to fetch
        lang: Language code (default: 'ko')
        country: Country code (default: 'kr')
        
    Returns:
        List of review dictionaries
    """
    try:
        # Fetch reviews from Google Play Store
        result, _ = reviews(
            app_id,
            lang=lang,
            country=country,
            sort=Sort.NEWEST,
            count=count
        )
        
        # Process and clean the data
        processed_reviews = []
        for review in result:
            # Simple sentiment analysis based on score
            sentiment = "positive" if review['score'] >= 4 else "negative"
            
            processed_review = {
                'userId': review['userName'] if review['userName'] else 'ÏùµÎ™Ö',
                'source': 'google_play',
                'rating': review['score'],
                'content': review['content'],
                'sentiment': sentiment,
                'createdAt': review['at'].isoformat() if review['at'] else datetime.now().isoformat()
            }
            processed_reviews.append(processed_review)
        
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping Google Play reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_app_store_reviews(app_id='1571096278', count=100):
    """
    Scrape reviews from Apple App Store
    
    Args:
        app_id: Apple App Store app ID
        count: Number of reviews to fetch (limited by RSS feed)
        
    Returns:
        List of review dictionaries
    """
    try:
        # Construct RSS URL for App Store reviews
        rss_url = f'https://itunes.apple.com/kr/rss/customerreviews/id={app_id}/sortBy=mostRecent/xml'
        
        response = requests.get(rss_url, timeout=10)
        if response.status_code != 200:
            print(f"Failed to fetch App Store reviews: HTTP {response.status_code}", file=sys.stderr)
            return []
        
        root = ET.fromstring(response.content)
        
        # Process reviews (skip first entry which is just metadata)
        processed_reviews = []
        entries = root.findall('.//{http://www.w3.org/2005/Atom}entry')[1:]
        
        for entry in entries[:count]:  # Limit to requested count
            try:
                # Extract review data
                author_elem = entry.find('{http://www.w3.org/2005/Atom}author')
                author = author_elem[0].text if author_elem is not None and len(author_elem) > 0 else 'ÏùµÎ™Ö'
                
                title_elem = entry.find('{http://www.w3.org/2005/Atom}title')
                title = title_elem.text if title_elem is not None else ''
                
                content_elem = entry.find('{http://www.w3.org/2005/Atom}content')
                content = content_elem.text if content_elem is not None else title  # Use title if no content
                
                rating_elem = entry.find('{http://itunes.apple.com/rss}rating')
                rating = int(rating_elem.text) if rating_elem is not None else 3
                
                updated_elem = entry.find('{http://www.w3.org/2005/Atom}updated')
                created_at = updated_elem.text if updated_elem is not None else datetime.now().isoformat()
                
                # Simple sentiment analysis based on rating
                sentiment = "positive" if rating >= 4 else "negative"
                
                processed_review = {
                    'userId': author,
                    'source': 'app_store',
                    'rating': rating,
                    'content': content,
                    'sentiment': sentiment,
                    'createdAt': created_at
                }
                processed_reviews.append(processed_review)
                
            except Exception as entry_error:
                print(f"Error processing App Store review entry: {entry_error}", file=sys.stderr)
                continue
        
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping App Store reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_reviews(app_id_google='com.lguplus.sohoapp', app_id_apple='1571096278', count=100, sources=['google_play']):
    """
    Scrape reviews from multiple sources
    
    Args:
        app_id_google: Google Play Store app ID
        app_id_apple: Apple App Store app ID
        count: Number of reviews to fetch per source
        sources: List of sources to scrape from
        
    Returns:
        List of review dictionaries
    """
    all_reviews = []
    
    if 'google_play' in sources:
        google_reviews = scrape_google_play_reviews(app_id_google, count)
        all_reviews.extend(google_reviews)
        print(f"Collected {len(google_reviews)} reviews from Google Play", file=sys.stderr)
    
    if 'app_store' in sources:
        apple_reviews = scrape_app_store_reviews(app_id_apple, count)
        all_reviews.extend(apple_reviews)
        print(f"Collected {len(apple_reviews)} reviews from App Store", file=sys.stderr)
    
    return all_reviews

def analyze_sentiments(reviews):
    """
    Enhanced HEART framework analysis with dynamic insights generation
    
    Args:
        reviews: List of review dictionaries
        
    Returns:
        Dictionary with insights and word frequency data
    """
    if not reviews:
        return {'insights': [], 'wordCloud': {'positive': [], 'negative': []}}
    
    print(f"Starting enhanced HEART analysis on {len(reviews)} reviews...", file=sys.stderr)
    
    # Debug: Print review ratings
    ratings = [review.get('rating', 3) for review in reviews]
    print(f"Review ratings: {ratings}", file=sys.stderr)
    negative_count = sum(1 for r in ratings if r < 4)
    positive_count = sum(1 for r in ratings if r >= 4)
    print(f"Negative reviews: {negative_count}, Positive reviews: {positive_count}", file=sys.stderr)
    
    # HEART framework analysis with detailed issue tracking
    heart_analysis = {
        'task_success': {'issues': [], 'details': []},
        'happiness': {'issues': [], 'details': []},
        'engagement': {'issues': [], 'details': []},
        'adoption': {'issues': [], 'details': []},
        'retention': {'issues': [], 'details': []}
    }
    
    # Pattern matching for specific issues
    for review in reviews:
        content = review['content'].lower()
        rating = review.get('rating', 3)
        user_id = review.get('userId', 'Unknown')
        
        # Analyze both positive and negative reviews for comprehensive insights
        if rating < 4:  # Negative reviews for problems
            # Task Success - Core functionality problems
            if any(keyword in content for keyword in ['Ïò§Î•ò', 'ÏóêÎü¨', 'Î≤ÑÍ∑∏', 'Ìäï', 'Í∫ºÏßê', 'ÏûëÎèôÏïàÌï®', 'Ïã§ÌñâÏïàÎê®', 'ÎÅäÍπÄ', 'Ïó∞Í≤∞ÏïàÎê®', 'ÏïàÎì§Î¶º', 'ÏÜåÎ¶¨ÏïàÎÇ®', 'ÏïàÎê®', 'ÏïàÎêò', 'ÌÅ¨ÎûòÏãú', 'Ï¢ÖÎ£å', 'Ïû¨ÏãúÏûë']):
                heart_analysis['task_success']['issues'].append(content)
                if 'Ìäï' in content or 'Í∫ºÏßê' in content or 'ÌÅ¨ÎûòÏãú' in content:
                    heart_analysis['task_success']['details'].append('Ïï± ÌÅ¨ÎûòÏãú')
                elif 'Ïó∞Í≤∞' in content and ('ÏïàÎê®' in content or 'ÎÅäÍπÄ' in content):
                    heart_analysis['task_success']['details'].append('ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞')
                elif 'ÏÜåÎ¶¨' in content and 'ÏïàÎÇ®' in content:
                    heart_analysis['task_success']['details'].append('ÏùåÏÑ± Í∏∞Îä•')
                else:
                    heart_analysis['task_success']['details'].append('Í∏∞Îä• Ïò§Î•ò')
            
            # Happiness - User satisfaction issues
            elif any(keyword in content for keyword in ['ÏßúÏ¶ù', 'ÏµúÏïÖ', 'Ïã§Îßù', 'ÌôîÎÇ®', 'Î∂àÎßå', 'Î≥ÑÎ°ú', 'Íµ¨Î¶º', 'Ïã´Ïñ¥', 'ÎãµÎãµ', 'Ïä§Ìä∏Î†àÏä§']):
                heart_analysis['happiness']['issues'].append(content)
                if 'ÏµúÏïÖ' in content or 'ÌôîÎÇ®' in content:
                    heart_analysis['happiness']['details'].append('Í∞ïÌïú Î∂àÎßå')
                else:
                    heart_analysis['happiness']['details'].append('ÎßåÏ°±ÎèÑ Ï†ÄÌïò')
            
            # Engagement - Usage patterns
            elif any(keyword in content for keyword in ['ÏïàÏç®', 'ÏÇ¨Ïö©ÏïàÌï®', 'Ïû¨ÎØ∏ÏóÜ', 'ÏßÄÎ£®', 'Ìù•ÎØ∏ÏóÜ', 'Î≥ÑÎ°úÏïàÏì¥', 'Í∞ÄÎÅîÎßå']):
                heart_analysis['engagement']['issues'].append(content)
                heart_analysis['engagement']['details'].append('ÏÇ¨Ïö© ÎπàÎèÑ Ï†ÄÌïò')
            
            # Retention - Churn indicators
            elif any(keyword in content for keyword in ['ÏÇ≠Ï†ú', 'Ìï¥ÏßÄ', 'Í∑∏Îßå', 'ÏïàÏì∏', 'Îã§Î•∏Í±∞', 'Î∞îÍøÄ', 'ÌÉàÌá¥', 'Ìè¨Í∏∞', 'Ï§ëÎã®']):
                heart_analysis['retention']['issues'].append(content)
                heart_analysis['retention']['details'].append('Ïù¥ÌÉà ÏúÑÌóò')
            
            # Adoption - Onboarding difficulties
            elif any(keyword in content for keyword in ['Ïñ¥Î†§ÏõÄ', 'Î≥µÏû°', 'Î™®Î•¥Í≤†', 'Ìó∑Í∞à', 'Ïñ¥ÎñªÍ≤å', 'ÏÑ§Î™ÖÎ∂ÄÏ°±', 'ÏÇ¨Ïö©Î≤ï', 'Í∞ÄÏù¥Îìú', 'ÎèÑÏõÄÎßê']):
                heart_analysis['adoption']['issues'].append(content)
                heart_analysis['adoption']['details'].append('ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú')
        
        # Also analyze positive reviews for potential improvements
        elif rating >= 4:  # Positive reviews for improvement opportunities
            # Look for mentions of specific features or improvements
            if any(keyword in content for keyword in ['Ï¢ãÏßÄÎßå', 'ÌïòÏßÄÎßå', 'Í∑∏Îü∞Îç∞', 'Îã§Îßå', 'ÏïÑÏâ¨Ïö¥', 'Îçî', 'Ï∂îÍ∞Ä', 'Í∞úÏÑ†', 'Ìñ•ÏÉÅ']):
                # These are positive reviews but with suggestions for improvement
                if 'ÌÜµÌôî' in content or 'Ï†ÑÌôî' in content:
                    heart_analysis['task_success']['issues'].append(content)
                    heart_analysis['task_success']['details'].append('Í∏∞Îä• Í∞úÏÑ† Ï†úÏïà')
                elif 'ÏÇ¨Ïö©' in content or 'Í∏∞Îä•' in content:
                    heart_analysis['engagement']['issues'].append(content)
                    heart_analysis['engagement']['details'].append('ÏÇ¨Ïö©ÏÑ± Í∞úÏÑ† Ï†úÏïà')
                elif 'Ïù∏ÌÑ∞ÌéòÏù¥Ïä§' in content or 'UI' in content or 'ÌôîÎ©¥' in content:
                    heart_analysis['happiness']['issues'].append(content)
                    heart_analysis['happiness']['details'].append('UI/UX Í∞úÏÑ† Ï†úÏïà')
    
    # Generate insights based on actual review content analysis
    insights = []
    insight_id = 1
    
    # Business impact weights
    impact_weights = {
        'task_success': 5,  # Critical - core functionality
        'retention': 4,     # High - user churn
        'happiness': 3,     # Medium - satisfaction
        'engagement': 2,    # Low-Medium - usage
        'adoption': 1       # Low - onboarding
    }
    
    # Debug: Print heart analysis results
    print(f"Heart analysis results:", file=sys.stderr)
    for category, data in heart_analysis.items():
        print(f"  {category}: {len(data['issues'])} issues", file=sys.stderr)
        if data['issues']:
            print(f"    First issue: {data['issues'][0][:50]}...", file=sys.stderr)
    
    for category, data in heart_analysis.items():
        if data['issues']:
            count = len(data['issues'])
            impact_score = count * impact_weights[category]
            
            # Priority calculation (more lenient thresholds)
            if impact_score >= 10 or (category == 'task_success' and count >= 2):
                priority = "critical"
                priority_emoji = "üî¥"
            elif impact_score >= 4 or count >= 2:
                priority = "major"
                priority_emoji = "üü†"
            else:
                priority = "minor"
                priority_emoji = "üü¢"
            
            # Analyze actual review content to identify specific problems and solutions
            actual_issues = []
            for issue_text in data['issues']:
                # Extract key phrases and issues from actual reviews
                if 'ÌÅ¨ÎûòÏãú' in issue_text or 'Í∫ºÏ†∏' in issue_text or 'Í∫ºÏßÄ' in issue_text or 'ÌäïÍ≤®' in issue_text or 'ÌäïÍπÄ' in issue_text or 'ÎÇòÍ∞ÄÎ≤ÑÎ¶º' in issue_text:
                    actual_issues.append('Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å')
                elif ('Ï†ÑÌôî' in issue_text or 'ÌÜµÌôî' in issue_text) and ('ÎÅäÏñ¥' in issue_text or 'Î∞õ' in issue_text or 'ÏïàÎê®' in issue_text or 'ÎÅäÍπÄ' in issue_text):
                    actual_issues.append('ÌÜµÌôî Í∏∞Îä• Ïò§Î•ò')
                elif 'Ïó∞ÎùΩÏ≤ò' in issue_text and ('Í≤ÄÏÉâ' in issue_text or 'Ï°∞Ìöå' in issue_text or 'ÏïàÎ≥¥ÏûÖÎãàÎã§' in issue_text or 'Î™ªÌï¥Ïöî' in issue_text):
                    actual_issues.append('Ïó∞ÎùΩÏ≤ò Í≤ÄÏÉâ/Ï°∞Ìöå Î∂àÍ∞Ä')
                elif 'Ï∞®Îã®' in issue_text and ('ÏûêÎèô' in issue_text or 'Í∞ÄÏ°±' in issue_text or 'Ìï¥Ï†ú' in issue_text):
                    actual_issues.append('ÏûêÎèô Ï∞®Îã® Ïò§Î•ò')
                elif 'Ï†ÑÌôî' in issue_text and ('ÎÅäÍ∏¥Îã§' in issue_text or 'ÎÅäÍπÄ' in issue_text or 'ÏùºÎ¶º' in issue_text):
                    actual_issues.append('ÌÜµÌôî ÎÅäÍπÄ/ÏïåÎ¶º Ïã§Ìå®')
                elif 'Ïï†ÌîåÏõåÏπò' in issue_text or ('ÏõåÏπò' in issue_text and 'Ìò∏Ìôò' in issue_text):
                    actual_issues.append('Ïï†ÌîåÏõåÏπò Ìò∏ÌôòÏÑ± Î¨∏Ï†ú')
                elif 'Î≤ÑÎ≤Ö' in issue_text or ('ÏïÑÏù¥Ìè∞' in issue_text and 'ÌîÑÎ°ú' in issue_text and 'Î≤ÑÎ≤Ö' in issue_text):
                    actual_issues.append('ÏÑ±Îä• Ï†ÄÌïò (ÏµúÏã† Í∏∞Í∏∞)')
                elif 'Îã®Ï∂ïÎ≤àÌò∏' in issue_text or ('Îã®Ï∂ï' in issue_text and 'ÏÑ§Ï†ï' in issue_text):
                    actual_issues.append('Îã®Ï∂ïÎ≤àÌò∏ ÏÑ§Ï†ï Ïò§Î•ò')
                elif 'Î°úÍ∑∏Ïù∏' in issue_text or 'Ïù∏Ï¶ù' in issue_text or 'Î°úÍ∑∏' in issue_text:
                    actual_issues.append('Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú')
                elif 'ÎäêÎ¶º' in issue_text or 'ÏßÄÏó∞' in issue_text:
                    actual_issues.append('ÏÑ±Îä• Ï†ÄÌïò')
                elif 'Ïó∞Í≤∞' in issue_text or 'ÎÑ§Ìä∏ÏõåÌÅ¨' in issue_text or 'Ï†ëÏÜç' in issue_text:
                    actual_issues.append('ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ Î¨∏Ï†ú')
                elif 'Î∏îÎ£®Ìà¨Ïä§' in issue_text or 'ÏùåÏßà' in issue_text or 'ÏÜåÎ¶¨' in issue_text:
                    actual_issues.append('Ïò§ÎîîÏò§ ÌíàÏßà Î¨∏Ï†ú')
                elif 'ÏóÖÎç∞Ïù¥Ìä∏' in issue_text or 'Í∞úÏÑ†' in issue_text:
                    actual_issues.append('ÏóÖÎç∞Ïù¥Ìä∏ Í¥ÄÎ†® Î¨∏Ï†ú')
                elif 'Í∏∞Í∏∞' in issue_text or 'Ìè∞' in issue_text or 'Ìò∏Ìôò' in issue_text:
                    actual_issues.append('Í∏∞Í∏∞ Ìò∏ÌôòÏÑ± Î¨∏Ï†ú')
                elif 'Î∂àÌé∏' in issue_text or 'Î≥µÏû°' in issue_text or 'Ïñ¥Î†§ÏõÄ' in issue_text:
                    actual_issues.append('ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú')
                elif 'ÏÇ≠Ï†ú' in issue_text or 'Ìï¥ÏßÄ' in issue_text or 'Í∑∏Îßå' in issue_text:
                    actual_issues.append('ÏÑúÎπÑÏä§ Ï§ëÎã® ÏùòÎèÑ')
                elif 'Î≤ïÏù∏' in issue_text or 'Ïù¥Ïö©Ï†úÌïú' in issue_text or 'Ï†úÌïú' in issue_text:
                    actual_issues.append('Ïù¥Ïö© Ï†úÌïú Î¨∏Ï†ú')
                elif 'Í≤ÄÏÉâ' in issue_text or 'Ï°∞Ìöå' in issue_text or 'Ï∞æÍ∏∞' in issue_text:
                    actual_issues.append('Í≤ÄÏÉâ/Ï°∞Ìöå Í∏∞Îä• Ïò§Î•ò')
                else:
                    actual_issues.append('Í∏∞ÌÉÄ Î¨∏Ï†ú')
            
            # Find most common actual issue
            if actual_issues:
                most_common_issue = max(set(actual_issues), key=actual_issues.count)
                issue_count = actual_issues.count(most_common_issue)
            else:
                most_common_issue = 'Í∏∞ÌÉÄ Î¨∏Ï†ú'
                issue_count = count
            
            # Generate specific problems and solutions based on actual review content
            if category == 'task_success':
                title = "Task Success: ÌïµÏã¨ Í∏∞Îä• ÏïàÏ†ïÏÑ±"
                problem = f"{most_common_issue} {issue_count}Í±¥ Î∞úÏÉù"
                
                if most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å':
                    solution = "ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î©îÎ™®Î¶¨ ÎàÑÏàò ÏàòÏ†ï, ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî"
                elif most_common_issue == 'ÌÜµÌôî Í∏∞Îä• Ïò§Î•ò':
                    solution = "ÌÜµÌôî Ïó∞Í≤∞ Î°úÏßÅ Í∞úÏÑ†, Í∂åÌïú Í¥ÄÎ¶¨ ÏµúÏ†ÅÌôî, ÌÜµÌôî ÌíàÏßà ÌÖåÏä§Ìä∏"
                elif most_common_issue == 'Ïó∞ÎùΩÏ≤ò Í≤ÄÏÉâ/Ï°∞Ìöå Î∂àÍ∞Ä':
                    solution = "Ïó∞ÎùΩÏ≤ò DB Ïù∏Îç±Ïã± Ïû¨Íµ¨Ï∂ï, Í≤ÄÏÉâ ÏïåÍ≥†Î¶¨Ï¶ò ÏµúÏ†ÅÌôî, Í∂åÌïú Í¥ÄÎ¶¨ Ï†êÍ≤Ä"
                elif most_common_issue == 'ÏûêÎèô Ï∞®Îã® Ïò§Î•ò':
                    solution = "Ï∞®Îã® ÏïåÍ≥†Î¶¨Ï¶ò Î°úÏßÅ ÏàòÏ†ï, Í∞ÄÏ°±/ÏßÄÏù∏ Î≤àÌò∏ ÌôîÏù¥Ìä∏Î¶¨Ïä§Ìä∏ Í∏∞Îä• Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÌÜµÌôî ÎÅäÍπÄ/ÏïåÎ¶º Ïã§Ìå®':
                    solution = "ÌÜµÌôî Ïó∞Í≤∞ ÏïàÏ†ïÏÑ± Í∞ïÌôî, Ìë∏Ïãú ÏïåÎ¶º ÏãúÏä§ÌÖú Ï†êÍ≤Ä, Î∞±Í∑∏ÎùºÏö¥Îìú Ï≤òÎ¶¨ Í∞úÏÑ†"
                elif most_common_issue == 'Ïï†ÌîåÏõåÏπò Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    solution = "WatchOS Ïó∞Îèô API ÏóÖÎç∞Ïù¥Ìä∏, ÏõåÏπò Ï†ÑÏö© UI/UX Í∞úÎ∞ú, Í±∞Ï†à/ÏäπÏù∏ Î≤ÑÌäº Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò (ÏµúÏã† Í∏∞Í∏∞)':
                    solution = "ÏµúÏã† iOS ÏµúÏ†ÅÌôî, Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Î∂ÑÏÑù Î∞è Í∞úÏÑ†, GPU Î†åÎçîÎßÅ ÏµúÏ†ÅÌôî"
                elif most_common_issue == 'Îã®Ï∂ïÎ≤àÌò∏ ÏÑ§Ï†ï Ïò§Î•ò':
                    solution = "Îã®Ï∂ïÎ≤àÌò∏ API Î°úÏßÅ Ïû¨ÏûëÏÑ±, ÏÑ§Ï†ï Ï†ÄÏû•/Î∂àÎü¨Ïò§Í∏∞ Í∏∞Îä• Í∞úÏÑ†"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    solution = "Ïù∏Ï¶ù ÏÑúÎ≤Ñ ÏïàÏ†ïÏÑ± Í∞úÏÑ†, ÌÜ†ÌÅ∞ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú Ï†êÍ≤Ä"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò':
                    solution = "UI Î†åÎçîÎßÅ ÏµúÏ†ÅÌôî, Î∞±Í∑∏ÎùºÏö¥Îìú Ï≤òÎ¶¨ Í∞úÏÑ†"
                elif most_common_issue == 'ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ Î¨∏Ï†ú':
                    solution = "ÎÑ§Ìä∏ÏõåÌÅ¨ Ïû¨ÏãúÎèÑ Î°úÏßÅ Ï∂îÍ∞Ä, Ïò§ÌîÑÎùºÏù∏ Î™®Îìú ÏßÄÏõê"
                elif most_common_issue == 'Ïò§ÎîîÏò§ ÌíàÏßà Î¨∏Ï†ú':
                    solution = "Ïò§ÎîîÏò§ ÏΩîÎç± ÏµúÏ†ÅÌôî, Î∏îÎ£®Ìà¨Ïä§ Ìò∏ÌôòÏÑ± Í∞úÏÑ†"
                elif most_common_issue == 'Í≤ÄÏÉâ/Ï°∞Ìöå Í∏∞Îä• Ïò§Î•ò':
                    solution = "Í≤ÄÏÉâ Ïù∏Îç±Ïä§ ÏµúÏ†ÅÌôî, Ï°∞Ìöå ÏøºÎ¶¨ ÏÑ±Îä• Í∞úÏÑ†, Îç∞Ïù¥ÌÑ∞ Ï∫êÏã± Í∞ïÌôî"
                elif most_common_issue == 'Ïù¥Ïö© Ï†úÌïú Î¨∏Ï†ú':
                    solution = "Î≤ïÏù∏ ÏÇ¨Ïö©Ïûê Ï†ïÏ±Ö Í≤ÄÌÜ†, Ïù¥Ïö© Ï†úÌïú Ï°∞Í±¥ ÏôÑÌôî, ÏÇ¨Ïö©Ïûê Í∂åÌïú Í¥ÄÎ¶¨ Í∞úÏÑ†"
                else:
                    solution = "ÌïµÏã¨ Í∏∞Îä• QA ÌÖåÏä§Ìä∏ Í∞ïÌôî, Î≤ÑÍ∑∏ ÏàòÏ†ï ÌîÑÎ°úÏÑ∏Ïä§ Í∞úÏÑ†"
                    
            elif category == 'happiness':
                title = "Happiness: ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ Í∞úÏÑ†"
                problem = f"{most_common_issue} {issue_count}Í±¥ÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Î∂àÎßå"
                
                if most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å':
                    solution = "ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î©îÎ™®Î¶¨ ÎàÑÏàò ÏàòÏ†ï, ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    solution = "Ïù∏Ï¶ù ÏÑúÎ≤Ñ ÏïàÏ†ïÏÑ± Í∞úÏÑ†, ÌÜ†ÌÅ∞ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú Ï†êÍ≤Ä"
                elif most_common_issue == 'Ïï†ÌîåÏõåÏπò Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    solution = "WatchOS Ïó∞Îèô API ÏóÖÎç∞Ïù¥Ìä∏, ÏõåÏπò Ï†ÑÏö© UI/UX Í∞úÎ∞ú, Í±∞Ï†à/ÏäπÏù∏ Î≤ÑÌäº Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò (ÏµúÏã† Í∏∞Í∏∞)':
                    solution = "ÏµúÏã† iOS ÏµúÏ†ÅÌôî, Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Î∂ÑÏÑù Î∞è Í∞úÏÑ†, GPU Î†åÎçîÎßÅ ÏµúÏ†ÅÌôî"
                elif most_common_issue == 'ÏûêÎèô Ï∞®Îã® Ïò§Î•ò':
                    solution = "Ï∞®Îã® ÏïåÍ≥†Î¶¨Ï¶ò Î°úÏßÅ ÏàòÏ†ï, Í∞ÄÏ°±/ÏßÄÏù∏ Î≤àÌò∏ ÌôîÏù¥Ìä∏Î¶¨Ïä§Ìä∏ Í∏∞Îä• Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    solution = "UI/UX Í∞úÏÑ†, ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± Î∞òÏòÅÌïú Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïû¨ÏÑ§Í≥Ñ"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò':
                    solution = "Ïï± ÏÑ±Îä• ÏµúÏ†ÅÌôî, Î°úÎî© ÏãúÍ∞Ñ Îã®Ï∂ï, Î∞òÏùëÏÑ± Í∞úÏÑ†"
                elif most_common_issue == 'Í∏∞Í∏∞ Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    solution = "Îã§ÏñëÌïú Í∏∞Í∏∞ ÌÖåÏä§Ìä∏ ÌôïÎåÄ, Ìò∏ÌôòÏÑ± Îß§Ìä∏Î¶≠Ïä§ Íµ¨Ï∂ï"
                else:
                    solution = "ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ Ï°∞ÏÇ¨ Ïã§Ïãú, Ï£ºÏöî Î∂àÎßå ÏÇ¨Ìï≠ Ïö∞ÏÑ† Ìï¥Í≤∞"
                
            elif category == 'engagement':
                title = "Engagement: ÏÇ¨Ïö©Ïûê Ï∞∏Ïó¨ÎèÑ Ï¶ùÎåÄ"
                problem = f"{most_common_issue} {issue_count}Í±¥ÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö© ÎπàÎèÑ Ï†ÄÌïò"
                
                if most_common_issue == 'ÏóÖÎç∞Ïù¥Ìä∏ Í¥ÄÎ†® Î¨∏Ï†ú':
                    solution = "Ï†ïÍ∏∞Ï†ÅÏù∏ Í∏∞Îä• ÏóÖÎç∞Ïù¥Ìä∏, ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ Í∏∞Îä• Ïö∞ÏÑ† Í∞úÎ∞ú"
                elif most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    solution = "ÌïµÏã¨ Í∏∞Îä• Ï†ëÍ∑ºÏÑ± Í∞úÏÑ†, ÏßÅÍ¥ÄÏ†ÅÏù∏ ÎÑ§ÎπÑÍ≤åÏù¥ÏÖò Ï†úÍ≥µ"
                else:
                    solution = "ÏÇ¨Ïö©Ïûê Ï∞∏Ïó¨Î•º ÎÜíÏù¥Îäî ÏÉàÎ°úÏö¥ Í∏∞Îä• Ï∂îÍ∞Ä, Í∞úÏù∏Ìôî ÏÑúÎπÑÏä§ Í∞ïÌôî"
                
            elif category == 'retention':
                title = "Retention: ÏÇ¨Ïö©Ïûê Ïú†ÏßÄÏú® Í∞úÏÑ†"
                problem = f"{most_common_issue} {issue_count}Í±¥ÏúºÎ°ú Ïù∏Ìïú Ïù¥ÌÉà ÏúÑÌóò"
                
                if most_common_issue == 'ÏÑúÎπÑÏä§ Ï§ëÎã® ÏùòÎèÑ':
                    solution = "Ïù¥ÌÉà ÏòàÎ∞© ÌîÑÎ°úÍ∑∏Îû® Ïö¥ÏòÅ, Í≥†Í∞ù ÏÑúÎπÑÏä§ Í∞ïÌôî, ÌïµÏã¨ Í∞ÄÏπò Ïû¨Í∞ïÏ°∞"
                elif most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å':
                    solution = "ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î©îÎ™®Î¶¨ ÎàÑÏàò ÏàòÏ†ï, ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    solution = "Ïù∏Ï¶ù ÏÑúÎ≤Ñ ÏïàÏ†ïÏÑ± Í∞úÏÑ†, ÌÜ†ÌÅ∞ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú Ï†êÍ≤Ä"
                elif most_common_issue == 'Ïó∞ÎùΩÏ≤ò Í≤ÄÏÉâ/Ï°∞Ìöå Î∂àÍ∞Ä':
                    solution = "Ïó∞ÎùΩÏ≤ò DB Ïù∏Îç±Ïã± Ïû¨Íµ¨Ï∂ï, Í≤ÄÏÉâ ÏïåÍ≥†Î¶¨Ï¶ò ÏµúÏ†ÅÌôî, Í∂åÌïú Í¥ÄÎ¶¨ Ï†êÍ≤Ä"
                elif most_common_issue == 'ÏûêÎèô Ï∞®Îã® Ïò§Î•ò':
                    solution = "Ï∞®Îã® ÏïåÍ≥†Î¶¨Ï¶ò Î°úÏßÅ ÏàòÏ†ï, Í∞ÄÏ°±/ÏßÄÏù∏ Î≤àÌò∏ ÌôîÏù¥Ìä∏Î¶¨Ïä§Ìä∏ Í∏∞Îä• Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÌÜµÌôî ÎÅäÍπÄ/ÏïåÎ¶º Ïã§Ìå®':
                    solution = "ÌÜµÌôî Ïó∞Í≤∞ ÏïàÏ†ïÏÑ± Í∞ïÌôî, Ìë∏Ïãú ÏïåÎ¶º ÏãúÏä§ÌÖú Ï†êÍ≤Ä, Î∞±Í∑∏ÎùºÏö¥Îìú Ï≤òÎ¶¨ Í∞úÏÑ†"
                elif most_common_issue == 'Ïï†ÌîåÏõåÏπò Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    solution = "WatchOS Ïó∞Îèô API ÏóÖÎç∞Ïù¥Ìä∏, ÏõåÏπò Ï†ÑÏö© UI/UX Í∞úÎ∞ú, Í±∞Ï†à/ÏäπÏù∏ Î≤ÑÌäº Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò (ÏµúÏã† Í∏∞Í∏∞)':
                    solution = "ÏµúÏã† iOS ÏµúÏ†ÅÌôî, Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Î∂ÑÏÑù Î∞è Í∞úÏÑ†, GPU Î†åÎçîÎßÅ ÏµúÏ†ÅÌôî"
                elif most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    solution = "ÏÇ¨Ïö©Ïûê Ïò®Î≥¥Îî© Í∞úÏÑ†, ÏßÄÏÜçÏ†ÅÏù∏ Í∞ÄÏπò Ï†úÍ≥µ Î∞©Ïïà ÎßàÎ†®"
                else:
                    solution = "ÏÇ¨Ïö©Ïûê Ïú†ÏßÄÏú® Î∂ÑÏÑù, ÎßûÏ∂§Ìòï Î¶¨ÌÖêÏÖò Ï†ÑÎûµ ÏàòÎ¶Ω"
                
            elif category == 'adoption':
                title = "Adoption: Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ï†ÅÏùë ÏßÄÏõê"
                problem = f"{most_common_issue} {issue_count}Í±¥ÏúºÎ°ú Ïù∏Ìïú Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ï†ÅÏùë Ïñ¥Î†§ÏõÄ"
                
                if most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å':
                    solution = "ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î©îÎ™®Î¶¨ ÎàÑÏàò ÏàòÏ†ï, ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    solution = "Ïù∏Ï¶ù ÏÑúÎ≤Ñ ÏïàÏ†ïÏÑ± Í∞úÏÑ†, ÌÜ†ÌÅ∞ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú Ï†êÍ≤Ä"
                elif most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    solution = "Ïò®Î≥¥Îî© ÌîÑÎ°úÏÑ∏Ïä§ Í∞ÑÏÜåÌôî, Îã®Í≥ÑÎ≥Ñ Í∞ÄÏù¥Îìú Ï†úÍ≥µ, ÌäúÌÜ†Î¶¨Ïñº Í∞úÏÑ†"
                elif most_common_issue == 'Í∏∞Í∏∞ Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    solution = "Îã§ÏñëÌïú Í∏∞Í∏∞ ÏßÄÏõê ÌôïÎåÄ, ÏÑ§Ïπò Í∞ÄÏù¥Îìú Í∞úÏÑ†"
                else:
                    solution = "Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Í≤ΩÌóò ÏµúÏ†ÅÌôî, Ï¥àÍ∏∞ ÏÇ¨Ïö© Ïû•Î≤Ω Ï†úÍ±∞"
            
            # Generate realistic problem prediction and solution based on HEART category
            predicted_problem = ""
            realistic_solution = ""
            
            if category == 'task_success':
                if most_common_issue == 'ÌÜµÌôî Í∏∞Îä• Ïò§Î•ò':
                    predicted_problem = "ÌÜµÌôî Ïó∞Í≤∞ Ïã§Ìå®Î°ú Ïù∏Ìïú ÌïµÏã¨ Í∏∞Îä• ÏàòÌñâ Î∂àÍ∞Ä"
                    realistic_solution = "ÌÜµÌôî Ïó∞Í≤∞ Î°úÏßÅ Ï†êÍ≤Ä, VoIP ÏÑúÎ≤Ñ ÏïàÏ†ïÏÑ± Í∞ïÌôî, ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉÅÌÉúÎ≥Ñ ÎåÄÏùë Î°úÏßÅ Í∞úÎ∞ú"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    predicted_problem = "Î°úÍ∑∏Ïù∏ Ïã§Ìå®Î°ú Ïù∏Ìïú ÏÑúÎπÑÏä§ Ï†ëÍ∑º Î∂àÍ∞Ä"
                    realistic_solution = "Ïù∏Ï¶ù ÏÑúÎ≤Ñ Î™®ÎãàÌÑ∞ÎßÅ Í∞ïÌôî, Îã§Ï§ë Ïù∏Ï¶ù Î∞©Ïãù Ï†úÍ≥µ, Î°úÍ∑∏Ïù∏ Ïã§Ìå® Ïãú Î™ÖÌôïÌïú ÏïàÎÇ¥ Î©îÏãúÏßÄ"
                else:
                    predicted_problem = "ÌïµÏã¨ Í∏∞Îä• Ïò§Î•òÎ°ú Ïù∏Ìïú ÏûëÏóÖ ÏôÑÎ£å Î∂àÍ∞Ä"
                    realistic_solution = "Í∏∞Îä•Î≥Ñ ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî, Ïò§Î•ò Î∞úÏÉù Ïãú Î≥µÍµ¨ Î©îÏª§ÎãàÏ¶ò Íµ¨Ï∂ï"
                    
            elif category == 'happiness':
                if most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    predicted_problem = "ÏßÅÍ¥ÄÏ†ÅÏù¥ÏßÄ ÏïäÏùÄ UI/UXÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ïä§Ìä∏Î†àÏä§"
                    realistic_solution = "ÏÇ¨Ïö©Ïûê ÌÖåÏä§Ìä∏ Ïã§Ïãú, ÎÑ§ÎπÑÍ≤åÏù¥ÏÖò Íµ¨Ï°∞ Îã®ÏàúÌôî, Ï£ºÏöî Í∏∞Îä• Ï†ëÍ∑ºÏÑ± Í∞úÏÑ†"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò':
                    predicted_problem = "Ïï± Î°úÎî© ÏßÄÏó∞ÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê ÎãµÎãµÌï®"
                    realistic_solution = "ÏΩîÎìú ÏµúÏ†ÅÌôî, Ïù¥ÎØ∏ÏßÄ ÏïïÏ∂ï, Ï∫êÏã± Ï†ÑÎûµ Í∞úÏÑ†, Î°úÎî© Ïù∏ÎîîÏºÄÏù¥ÌÑ∞ Ï∂îÍ∞Ä"
                else:
                    predicted_problem = "ÏÇ¨Ïö©Ïûê Í∏∞ÎåÄÏôÄ Ïã§Ï†ú Í≤ΩÌóò Í∞ÑÏùò Í¥¥Î¶¨"
                    realistic_solution = "ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± Ï†ïÍ∏∞ ÏàòÏßë, ÌïµÏã¨ Î∂àÎßå ÏÇ¨Ìï≠ Ïö∞ÏÑ† Ìï¥Í≤∞, UX Í∞úÏÑ† ÌîÑÎ°úÏÑ∏Ïä§ Íµ¨Ï∂ï"
                    
            elif category == 'engagement':
                if most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    predicted_problem = "Î≥µÏû°Ìïú Í∏∞Îä• Íµ¨Ï°∞Î°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ï∞∏Ïó¨ÎèÑ Í∞êÏÜå"
                    realistic_solution = "ÌïµÏã¨ Í∏∞Îä• Ï†ëÍ∑ºÏÑ± Í∞úÏÑ†, Í∞úÏù∏Ìôî ÏïåÎ¶º ÏÑ§Ï†ï, ÏÇ¨Ïö© Ìå®ÌÑ¥ Î∂ÑÏÑù Í∏∞Î∞ò Í∏∞Îä• Ï∂îÏ≤ú"
                else:
                    predicted_problem = "Ïû¨Î∞©Î¨∏ ÎèôÍ∏∞ Î∂ÄÏ°±ÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö© ÎπàÎèÑ Ï†ÄÌïò"
                    realistic_solution = "Ìë∏Ïãú ÏïåÎ¶º Í∞úÏù∏Ìôî, ÏÇ¨Ïö©ÏûêÎ≥Ñ ÎßûÏ∂§ ÏΩòÌÖêÏ∏† Ï†úÍ≥µ, Ï†ïÍ∏∞Ï†Å ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Ïù¥Î≤§Ìä∏ ÏßÑÌñâ"
                    
            elif category == 'retention':
                if most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å':
                    predicted_problem = "Ïï± ÏïàÏ†ïÏÑ± Î¨∏Ï†úÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ïù¥ÌÉà"
                    realistic_solution = "ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î≤ÑÍ∑∏ ÏàòÏ†ï, ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî, Í∏¥Í∏â Ìå®Ïπò ÌîÑÎ°úÏÑ∏Ïä§ Íµ¨Ï∂ï"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    predicted_problem = "Î∞òÎ≥µÏ†ÅÏù∏ Î°úÍ∑∏Ïù∏ Ïã§Ìå®Î°ú Ïù∏Ìïú Ïû¨Î∞©Î¨∏Ïú® Í∞êÏÜå"
                    realistic_solution = "ÏûêÎèô Î°úÍ∑∏Ïù∏ Í∏∞Îä• Í∞úÏÑ†, ÏÜåÏÖú Î°úÍ∑∏Ïù∏ Ïó∞Îèô, ÎπÑÎ∞ÄÎ≤àÌò∏ Ï∞æÍ∏∞ ÌîÑÎ°úÏÑ∏Ïä§ Í∞ÑÏÜåÌôî"
                else:
                    predicted_problem = "ÏßÄÏÜçÏ†ÅÏù∏ Í∞ÄÏπò Ï†úÍ≥µ Ïã§Ìå®Î°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ïù¥ÌÉà"
                    realistic_solution = "ÏÇ¨Ïö©Ïûê ÏÉùÎ™ÖÏ£ºÍ∏∞Î≥Ñ ÎßûÏ∂§ ÏÑúÎπÑÏä§ Ï†úÍ≥µ, Ïû¨Î∞©Î¨∏ Ïú†ÎèÑ ÏïåÎ¶º ÏµúÏ†ÅÌôî"
                    
            elif category == 'adoption':
                if most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    predicted_problem = "Î≥µÏû°Ìïú Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î°ú Ïù∏Ìïú Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ïò®Î≥¥Îî© Ïù¥ÌÉà"
                    realistic_solution = "Ïò®Î≥¥Îî© ÌîåÎ°úÏö∞ Îã®ÏàúÌôî, Îã®Í≥ÑÎ≥Ñ Í∞ÄÏù¥Îìú Ï†úÍ≥µ, ÌïÑÏàò Í∏∞Îä• Ï§ëÏã¨ ÌäúÌÜ†Î¶¨Ïñº Íµ¨ÏÑ±"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    predicted_problem = "Ïù∏Ï¶ù Ïò§Î•òÎ°ú Ïù∏Ìïú Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ïú†ÏûÖ Ïã§Ìå®"
                    realistic_solution = "Í∞ÑÌé∏ ÌöåÏõêÍ∞ÄÏûÖ ÏòµÏÖò Ï†úÍ≥µ, Ïù∏Ï¶ù Í≥ºÏ†ï ÏµúÏÜåÌôî, ÏÑ§Ï†ï ÏûêÎèôÌôî Í∏∞Îä• Í∞ïÌôî"
                else:
                    predicted_problem = "ÌïµÏã¨ Í∞ÄÏπò Ïù¥Ìï¥ Î∂ÄÏ°±ÏúºÎ°ú Ïù∏Ìïú Ï¥àÍ∏∞ Ïù¥ÌÉàÎ•† Ï¶ùÍ∞Ä"
                    realistic_solution = "ÌïµÏã¨ Í∏∞Îä• Ïö∞ÏÑ† ÎÖ∏Ï∂ú, ÏÇ¨Ïö©Ïûê Ïú†ÌòïÎ≥Ñ ÎßûÏ∂§ Ïò®Î≥¥Îî©, Ï≤´ ÏÑ±Í≥µ Í≤ΩÌóò Î≥¥Ïû•"
            
            insights.append({
                'id': insight_id,
                'title': f"{priority_emoji} {priority.title()} | {category.title().replace('_', ' ')}",
                'description': f"üì¢ ÏòàÏ∏°ÎêòÎäî Î¨∏Ï†úÏ†ê\n{predicted_problem}\n\nüí° Ìï¥Í≤∞ Î∞©Î≤ï\n{realistic_solution} üìä Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò ÎßûÏ∂§Ìòï Ìï¥Í≤∞Ï±Ö: {solution}",
                'priority': priority,
                'mentionCount': count,
                'trend': 'stable',
                'category': category
            })
            insight_id += 1
    
    # Sort by priority and impact
    priority_order = {'critical': 3, 'major': 2, 'minor': 1}
    insights.sort(key=lambda x: (priority_order[x['priority']], x['mentionCount']), reverse=True)
    
    # Limit to top 5 insights
    insights = insights[:5]
    
    # Enhanced Korean word frequency analysis (limit to top 10 each)
    positive_words = {}
    negative_words = {}
    
    # Stop words to exclude (including neutral terms)
    stop_words = ['Ïù¥Í≤É', 'Í∑∏Í≤É', 'Ï†ÄÍ≤É', 'ÏûàÎäî', 'ÏóÜÎäî', 'Í∞ôÏùÄ', 'Îã§Î•∏', 'Ïù¥Îü∞', 'Í∑∏Îü∞', 'Ï†ÄÎü∞', 'ÏóêÏÑú', 'ÏúºÎ°ú', 'ÏóêÍ≤å', 'ÌïúÌÖå', 'ÏóêÏÑúÎäî', 'Í∑∏Î¶¨Í≥†', 'ÌïòÏßÄÎßå', 'Í∑∏ÎûòÏÑú', 'Í∑∏Îü∞Îç∞', 'ÏûÖÎãàÎã§', 'Ìï©ÎãàÎã§', 'ÌñàÏäµÎãàÎã§', 'ÏûàÏäµÎãàÎã§', 'Îê©ÎãàÎã§', 'ÏóÜÏäµÎãàÎã§', 'Ïù¥ÏóêÏöî', 'ÏòàÏöî', 'ÎùºÍ≥†', 'Ïù¥ÎùºÍ≥†', 'ÌïòÎäî', 'ÌïòÍ≥†', 'Ìï¥ÏÑú', 'Ìï¥ÎèÑ', 'ÎïåÎ¨∏Ïóê', 'Í∑∏ÎÉ•', 'Ï†ïÎßê', 'ÏßÑÏßú', 'ÎÑàÎ¨¥', 'ÏïÑÏ£º', 'Îß§Ïö∞', 'Ï¢Ä', 'Ï°∞Í∏à', 'ÎßéÏù¥', 'Ïûò', 'Î™ª', 'Ïïà', 'Ï†úÍ∞Ä', 'Ï†ÄÎäî', 'ÎÇòÎäî', 'Ïö∞Î¶¨', 'Ï†ÄÌù¨', 'Í∑∏Í≤å', 'Í∑∏Í±∞', 'Ïù¥Í±∞', 'Ï†ÄÍ±∞', 'Ïó¨Í∏∞', 'Í±∞Í∏∞', 'Ï†ÄÍ∏∞', 'Îïå', 'ÌõÑ', 'Ï†Ñ', 'ÏßÄÍ∏à', 'Ïò§Îäò', 'ÎÇ¥Ïùº', 'Ïñ¥Ï†ú', 'ÏöîÏ¶ò', 'ÏµúÍ∑º', 'Ïù¥Î≤à', 'Îã§Ïùå', 'ÏúÑÌï¥', 'ÏúÑÌïòÏó¨', 'ÏúÑÌïú', 'ÌïúÎ≤à', 'ÎëêÎ≤à', 'ÏÑ∏Î≤à', 'Î®ºÏ†Ä', 'Îã§Ïãú', 'Îòê', 'ÎòêÌïú', 'Í∑∏Î¶¨Í≥†', 'Í∑∏ÎûòÏÑú', 'Í∑∏Îü∞Îç∞', 'ÌïòÏßÄÎßå', 'Í∑∏Îü¨ÎÇò', 'Í∑∏Îü¨Î©¥', 'Í∑∏Îü¨ÎØÄÎ°ú', 'Îî∞ÎùºÏÑú', 'ÎßåÏïΩ', 'ÎßåÏùº', 'Í≤ΩÏö∞', 'ÎïåÎ¨∏', 'ÎçïÎ∂Ñ', 'ÎçïÏóê', 'ÎßêÍ≥†', 'ÎßêÍµ¨', 'Ïô∏Ïóê', 'ÎπºÍ≥†', 'Ï†úÏô∏', 'Ìè¨Ìï®', 'Ï∂îÍ∞Ä', 'ÏóÖÎç∞Ïù¥Ìä∏', 'Î≤ÑÏ†Ñ', 'Í∏∞Îä•', 'ÏÑúÎπÑÏä§', 'ÏãúÏä§ÌÖú', 'ÌîÑÎ°úÍ∑∏Îû®', 'ÏÜåÌîÑÌä∏Ïõ®Ïñ¥', 'ÌïòÎìúÏõ®Ïñ¥', 'Ïù∏ÌÑ∞ÌéòÏù¥Ïä§', 'ÏÇ¨Ïö©Ïûê', 'Í≥†Í∞ù', 'ÌöåÏõê', 'Í≥ÑÏ†ï', 'Î°úÍ∑∏Ïù∏', 'ÌöåÏõêÍ∞ÄÏûÖ', 'ÏÑ§Ï†ï', 'ÌôòÍ≤Ω', 'ÏÉÅÌô©', 'ÏÉÅÌÉú', 'Ï†ïÎ≥¥', 'Îç∞Ïù¥ÌÑ∞', 'ÎÇ¥Ïö©', 'Î∞©Î≤ï', 'Î∞©Ïãù', 'ÌòïÌÉú', 'Ï¢ÖÎ•ò', 'ÌÉÄÏûÖ', 'ÏÇ¨Ïù¥Ìä∏', 'ÌôàÌéòÏù¥ÏßÄ', 'ÏõπÏÇ¨Ïù¥Ìä∏', 'ÌéòÏù¥ÏßÄ', 'ÌôîÎ©¥', 'Ï∞Ω', 'Î≤ÑÌäº', 'Î©îÎâ¥', 'ÎßÅÌÅ¨', 'ÏïÑÏù¥ÏΩò', 'Ïù¥ÎØ∏ÏßÄ', 'ÏÇ¨ÏßÑ', 'ÌååÏùº', 'Ìè¥Îçî', 'ÎîîÎ†âÌÜ†Î¶¨', 'Í≤ΩÎ°ú', 'Ï£ºÏÜå', 'Î≤àÌò∏', 'ÏΩîÎìú', 'Í∞í', 'Î≥ÄÏàò', 'Îß§Í∞úÎ≥ÄÏàò', 'ÌååÎùºÎØ∏ÌÑ∞', 'ÏòµÏÖò', 'ÏÑ†ÌÉù', 'Ìï≠Î™©', 'Î¶¨Ïä§Ìä∏', 'Î™©Î°ù', 'ÌÖåÏù¥Î∏î', 'Ìëú', 'Ï∞®Ìä∏', 'Í∑∏ÎûòÌîÑ', 'ÎèÑÌëú', 'Î¨∏ÏÑú', 'ÌÖçÏä§Ìä∏', 'Í∏Ä', 'Í∏ÄÏûê', 'Î¨∏Ïûê', 'Îã®Ïñ¥', 'Î¨∏Ïû•', 'Îã®ÎùΩ', 'Ï†úÎ™©', 'Î∂ÄÏ†úÎ™©', 'Ìó§Îçî', 'ÌíãÌÑ∞', 'Î©îÏù∏', 'ÏÑúÎ∏å', 'Ï¢åÏ∏°', 'Ïö∞Ï∏°', 'ÏÉÅÎã®', 'ÌïòÎã®', 'Ï§ëÏïô', 'Í∞ÄÏö¥Îç∞', 'ÏôºÏ™Ω', 'Ïò§Î•∏Ï™Ω', 'ÏúÑÏ™Ω', 'ÏïÑÎûòÏ™Ω', 'ÏïûÏ™Ω', 'Îí§Ï™Ω', 'ÏïàÏ™Ω', 'Î∞îÍπ•Ï™Ω', 'ÎÇ¥Î∂Ä', 'Ïô∏Î∂Ä', 'Ï†ÑÏ≤¥', 'Î∂ÄÎ∂Ñ', 'ÏùºÎ∂Ä', 'Ï†ÑÎ∂Ä', 'Î™®Îì†', 'Í∞ÅÍ∞Å', 'Í∞úÎ≥Ñ', 'Í≥µÌÜµ', 'ÏùºÎ∞ò', 'ÌäπÎ≥Ñ', 'ÌäπÏàò', 'Í∏∞Î≥∏', 'ÌëúÏ§Ä', 'Í≥†Í∏â', 'Ï¥àÍ∏â', 'Ï§ëÍ∏â', 'ÏÉÅÍ∏â', 'ÏµúÍ≥†', 'ÏµúÏ†Ä', 'ÏµúÎåÄ', 'ÏµúÏÜå', 'ÏµúÏã†', 'ÏµúÍ∑º', 'Ïù¥Ï†Ñ', 'Í≥ºÍ±∞', 'ÌòÑÏû¨', 'ÎØ∏Îûò', 'Îã§Ïùå', 'Ïù¥ÌõÑ', 'Ïù¥Ï†Ñ', 'ÏßÄÎÇú', 'Ïò¨Ìï¥', 'ÏûëÎÖÑ', 'ÎÇ¥ÎÖÑ', 'Ïù¥Îã¨', 'ÏßÄÎÇúÎã¨', 'Îã§ÏùåÎã¨', 'Ïù¥Î≤àÏ£º', 'ÏßÄÎÇúÏ£º', 'Îã§ÏùåÏ£º', 'ÏõîÏöîÏùº', 'ÌôîÏöîÏùº', 'ÏàòÏöîÏùº', 'Î™©ÏöîÏùº', 'Í∏àÏöîÏùº', 'ÌÜ†ÏöîÏùº', 'ÏùºÏöîÏùº', 'Ï£ºÎßê', 'Ï£ºÏ§ë', 'ÌèâÏùº', 'Ìú¥Ïùº', 'Í≥µÌú¥Ïùº', 'Î∞©Ìïô', 'Í∞úÌïô', 'ÏãúÏûë', 'Ï¢ÖÎ£å', 'ÏôÑÎ£å', 'ÏßÑÌñâ', 'Ï≤òÎ¶¨', 'ÏûëÏóÖ', 'ÏóÖÎ¨¥', 'Ïùº', 'Í≥ºÏ†ú', 'Î¨∏Ï†ú', 'Ìï¥Í≤∞', 'Îãµ', 'ÏßàÎ¨∏', 'ÏöîÏ≤≠', 'Ïã†Ï≤≠', 'Ï†ëÏàò', 'Îì±Î°ù', 'Í∞ÄÏûÖ', 'ÌÉàÌá¥', 'Ìï¥ÏßÄ', 'Ìï¥Ï†ú', 'Ï∑®ÏÜå', 'ÏÇ≠Ï†ú', 'Ï†úÍ±∞', 'Ï∂îÍ∞Ä', 'Î≥ÄÍ≤Ω', 'ÏàòÏ†ï', 'Ìé∏Ïßë', 'Ï†ÄÏû•', 'Î∞±ÏóÖ', 'Î≥µÏõê', 'Î≥µÍµ¨', 'Î≥µÏÇ¨', 'Î∂ôÏó¨ÎÑ£Í∏∞', 'ÏûòÎùºÎÇ¥Í∏∞', 'ÏÑ†ÌÉù', 'ÌÅ¥Î¶≠', 'ÎçîÎ∏îÌÅ¥Î¶≠', 'Ïö∞ÌÅ¥Î¶≠', 'ÌÑ∞Ïπò', 'Ïä§ÏôÄÏù¥ÌîÑ', 'ÎìúÎûòÍ∑∏', 'ÎìúÎ°≠', 'Ïä§ÌÅ¨Î°§', 'ÌôïÎåÄ', 'Ï∂ïÏÜå', 'ÌöåÏ†Ñ', 'Ïù¥Îèô', 'ÏúÑÏπò', 'ÌÅ¨Í∏∞', 'ÎÜíÏù¥', 'ÎÑàÎπÑ', 'Í∏∏Ïù¥', 'Ìè≠', 'ÍπäÏù¥', 'ÎëêÍªò', 'Î¨¥Í≤å', 'ÏÉâÏÉÅ', 'ÏÉâÍπî', 'Îπ®Í∞ÑÏÉâ', 'ÌååÎûÄÏÉâ', 'ÎÖ∏ÎûÄÏÉâ', 'Ï¥àÎ°ùÏÉâ', 'Í≤ÄÏùÄÏÉâ', 'Ìù∞ÏÉâ', 'ÌöåÏÉâ', 'Î≥¥ÎùºÏÉâ', 'Ï£ºÌô©ÏÉâ', 'Î∂ÑÌôçÏÉâ', 'Í∞àÏÉâ', 'Í∏àÏÉâ', 'ÏùÄÏÉâ', 'Ìà¨Î™Ö', 'Î∂àÌà¨Î™Ö', 'Î∞ùÏùÄ', 'Ïñ¥ÎëêÏö¥', 'ÏßÑÌïú', 'Ïó∞Ìïú', 'ÏÑ†Î™ÖÌïú', 'ÌùêÎ¶∞', 'Íπ®ÎÅóÌïú', 'ÎçîÎü¨Ïö¥', 'ÏÉàÎ°úÏö¥', 'ÎÇ°ÏùÄ', 'Ïò§ÎûòÎêú', 'ÏµúÏã†Ïùò', 'Íµ¨ÏãùÏùò', 'ÌòÑÎåÄÏùò', 'Ï†ÑÌÜµÏ†ÅÏù∏', 'ÏùºÎ∞òÏ†ÅÏù∏', 'ÌäπÎ≥ÑÌïú', 'ÎèÖÌäπÌïú', 'Ïú†ÏùºÌïú', 'ÌùîÌïú', 'ÎìúÎ¨∏', 'ÎßéÏùÄ', 'Ï†ÅÏùÄ', 'ÌÅ∞', 'ÏûëÏùÄ', 'ÎÜíÏùÄ', 'ÎÇÆÏùÄ', 'Îπ†Î•∏', 'ÎäêÎ¶∞', 'Ïâ¨Ïö¥', 'Ïñ¥Î†§Ïö¥', 'Í∞ÑÎã®Ìïú', 'Î≥µÏû°Ìïú', 'Îã®ÏàúÌïú', 'Ï†ïÌôïÌïú', 'Î∂ÄÏ†ïÌôïÌïú', 'Ïò¨Î∞îÎ•∏', 'ÏûòÎ™ªÎêú', 'ÎßûÎäî', 'ÌãÄÎ¶∞', 'Ï¢ãÏùÄ', 'ÎÇòÏÅú', 'Ïö∞ÏàòÌïú', 'Ïó¥Îì±Ìïú', 'ÏôÑÎ≤ΩÌïú', 'Î∂àÏôÑÏ†ÑÌïú', 'ÏÑ±Í≥µÌïú', 'Ïã§Ìå®Ìïú', 'Ìö®Í≥ºÏ†ÅÏù∏', 'ÎπÑÌö®Í≥ºÏ†ÅÏù∏', 'Ïú†Ïö©Ìïú', 'Î¨¥Ïö©Ìïú', 'ÌïÑÏöîÌïú', 'Î∂àÌïÑÏöîÌïú', 'Ï§ëÏöîÌïú', 'ÏÇ¨ÏÜåÌïú', 'Ï£ºÏöîÌïú', 'Î∂ÄÏ∞®Ï†ÅÏù∏', 'ÌïµÏã¨Ï†ÅÏù∏', 'Î∂ÄÏàòÏ†ÅÏù∏', 'ÏßÅÏ†ëÏ†ÅÏù∏', 'Í∞ÑÏ†ëÏ†ÅÏù∏', 'Î™ÖÌôïÌïú', 'Î∂àÎ™ÖÌôïÌïú', 'Íµ¨Ï≤¥Ï†ÅÏù∏', 'Ï∂îÏÉÅÏ†ÅÏù∏', 'ÌòÑÏã§Ï†ÅÏù∏', 'Ïù¥ÏÉÅÏ†ÅÏù∏', 'Í∞ÄÎä•Ìïú', 'Î∂àÍ∞ÄÎä•Ìïú', 'ÌôïÏã§Ìïú', 'Î∂àÌôïÏã§Ìïú', 'ÏïàÏ†ÑÌïú', 'ÏúÑÌóòÌïú', 'Í≥µÍ∞úÏ†ÅÏù∏', 'ÎπÑÍ≥µÍ∞úÏ†ÅÏù∏', 'Í≥µÏãùÏ†ÅÏù∏', 'ÎπÑÍ≥µÏãùÏ†ÅÏù∏', 'Ï†ïÏãùÏùò', 'ÏûÑÏãúÏùò', 'ÏòÅÍµ¨Ï†ÅÏù∏', 'ÏùºÏãúÏ†ÅÏù∏', 'ÏßÄÏÜçÏ†ÅÏù∏', 'ÏùºÌöåÏÑ±Ïùò', 'Î∞òÎ≥µÏ†ÅÏù∏', 'Îã®Î∞úÏÑ±Ïùò', 'Ïó∞ÏÜçÏ†ÅÏù∏', 'Î∂àÏó∞ÏÜçÏ†ÅÏù∏', 'ÏûêÎèôÏ†ÅÏù∏', 'ÏàòÎèôÏ†ÅÏù∏', 'Îä•ÎèôÏ†ÅÏù∏', 'ÏàòÎèôÏ†ÅÏù∏', 'Ï†ÅÍ∑πÏ†ÅÏù∏', 'ÏÜåÍ∑πÏ†ÅÏù∏', 'Í∏çÏ†ïÏ†ÅÏù∏', 'Î∂ÄÏ†ïÏ†ÅÏù∏', 'ÎÇôÍ¥ÄÏ†ÅÏù∏', 'ÎπÑÍ¥ÄÏ†ÅÏù∏', 'Ìù¨ÎßùÏ†ÅÏù∏', 'Ï†àÎßùÏ†ÅÏù∏', 'ÎßåÏ°±Ïä§Îü¨Ïö¥', 'Î∂àÎßåÏ°±Ïä§Îü¨Ïö¥', 'ÌñâÎ≥µÌïú', 'Î∂àÌñâÌïú', 'Ï¶êÍ±∞Ïö¥', 'Í¥¥Î°úÏö¥', 'Í∏∞ÏÅú', 'Ïä¨Ìîà', 'ÏõÉÍ∏¥', 'Ïû¨ÎØ∏ÏûàÎäî', 'ÏßÄÎ£®Ìïú', 'Ìù•ÎØ∏Î°úÏö¥', 'ÎÜÄÎùºÏö¥', 'ÎãπÏó∞Ìïú', 'ÏòàÏÉÅÎêú', 'ÏòàÏÉÅÏπòÎ™ªÌïú', 'ÏùµÏàôÌïú', 'ÎÇØÏÑ†', 'Ìé∏Ìïú', 'Î∂àÌé∏Ìïú', 'ÏûêÏó∞Ïä§Îü¨Ïö¥', 'Ïñ¥ÏÉâÌïú', 'Ï†ïÏÉÅÏ†ÅÏù∏', 'ÎπÑÏ†ïÏÉÅÏ†ÅÏù∏', 'ÏùºÎ∞òÏ†ÅÏù∏', 'ÌäπÎ≥ÑÌïú', 'Î≥¥ÌÜµÏùò', 'ÌèâÎ≤îÌïú', 'ÎèÖÌäπÌïú', 'ÌäπÏù¥Ìïú']
    
    # Define positive and negative keywords for better classification
    positive_keywords = ['Ï¢ã', 'ÎßåÏ°±', 'ÌõåÎ•≠', 'Ïö∞Ïàò', 'Îõ∞Ïñ¥ÎÇú', 'ÏôÑÎ≤Ω', 'ÏµúÍ≥†', 'Ï∂îÏ≤ú', 'Í∞êÏÇ¨', 'Í≥†ÎßàÏõå', 'ÎèÑÏõÄ', 'Ìé∏Î¶¨', 'Ïâ¨Ïö¥', 'Í∞ÑÌé∏', 'Îπ†Î•∏', 'Ï†ïÌôï', 'ÏïàÏ†ï', 'Ïã†Î¢∞', 'ÏÑ±Í≥µ', 'Ìö®Í≥º', 'Ïú†Ïö©', 'ÌïÑÏöî', 'Ï§ëÏöî', 'ÌïµÏã¨', 'Î≥¥Ïù¥Ïä§ÌîºÏã±', 'ÎßâÏïÑ', 'ÏöîÏïΩ', 'ÌÖçÏä§Ìä∏', 'Ïú†Ïö©ÌïòÍ≥†', 'Ï¢ãÏïÑÏöî', 'Í∞êÏÇ¨Ìï©ÎãàÎã§', 'ÎßåÏ°±', 'Ìé∏Î¶¨', 'ÎèÑÏõÄ', 'Í∞úÏÑ†', 'Ìñ•ÏÉÅ', 'Î∞úÏ†Ñ', 'ÏÑ±Ïû•', 'Ï¶ùÍ∞Ä', 'ÏÉÅÏäπ', 'Ïò¨Îùº', 'ÎÜíÏïÑ', 'Í∞ïÌôî', 'Î≥¥Í∞ï', 'Í∞úÏÑ†', 'Ìñ•ÏÉÅ', 'ÏôÑÏÑ±', 'Îã¨ÏÑ±', 'ÏÑ±Ï∑®', 'Ìï¥Í≤∞', 'Í∑πÎ≥µ', 'Ïù¥Í≤®', 'ÏÑ±Í≥µ', 'ÏäπÎ¶¨', 'Ïö∞Ïäπ', 'ÏµúÍ≥†', '1Îì±', 'ÏúºÎú∏', 'ÏµúÏÉÅ', 'ÏµúÏö∞Ïàò', 'Ïö∞Ïàò', 'ÌõåÎ•≠', 'Îõ∞Ïñ¥ÎÇú', 'ÏôÑÎ≤Ω', 'Ïù¥ÏÉÅÏ†Å', 'ÏôÑÏ†Ñ', 'Ï†ÑÏ≤¥', 'Î™®Îì†', 'Îã§', 'Ï†ÑÎ∂Ä', 'ÏôÑÏ†ÑÌûà', 'Ï†ïÎßê', 'ÏßÑÏßú', 'Ï∞∏', 'ÌôïÏã§', 'Î™ÖÌôï', 'Î∂ÑÎ™Ö', 'ÌãÄÎ¶ºÏóÜÏù¥', 'ÎãπÏó∞Ìûà', 'Î¨ºÎ°†', 'Ïó≠Ïãú', 'ÏòàÏÉÅÎåÄÎ°ú', 'Í∏∞ÎåÄÌïúÎåÄÎ°ú', 'Î∞îÎùºÎçòÎåÄÎ°ú', 'ÏõêÌïòÎçòÎåÄÎ°ú', 'ÌïÑÏöîÌïúÎåÄÎ°ú', 'Ï†ÅÏ†àÌïú', 'ÏïåÎßûÏùÄ', 'Ï†ÅÌï©Ìïú', 'ÎßûÎäî', 'Ïò¨Î∞îÎ•∏', 'Ï†ïÌôïÌïú', 'Ï†ïÎßê', 'ÏßÑÏßú', 'Ï∞∏', 'ÌôïÏã§', 'Î™ÖÌôï', 'Î∂ÑÎ™Ö', 'ÌãÄÎ¶ºÏóÜÏù¥', 'ÎãπÏó∞Ìûà', 'Î¨ºÎ°†', 'Ïó≠Ïãú', 'ÏòàÏÉÅÎåÄÎ°ú', 'Í∏∞ÎåÄÌïúÎåÄÎ°ú', 'Î∞îÎùºÎçòÎåÄÎ°ú', 'ÏõêÌïòÎçòÎåÄÎ°ú', 'ÌïÑÏöîÌïúÎåÄÎ°ú', 'Ï†ÅÏ†àÌïú', 'ÏïåÎßûÏùÄ', 'Ï†ÅÌï©Ìïú', 'ÎßûÎäî', 'Ïò¨Î∞îÎ•∏', 'Ï†ïÌôïÌïú']
    
    negative_keywords = ['ÎÇòÏÅú', 'ÏïàÏ¢ã', 'Î∂àÎßå', 'Î¨∏Ï†ú', 'Ïò§Î•ò', 'ÏóêÎü¨', 'Î≤ÑÍ∑∏', 'Ïã§Ìå®', 'ÏïàÎê®', 'ÏïàÎêò', 'Î™ªÌï¥', 'Ïñ¥Î†§Ïö¥', 'Î≥µÏû°', 'ÎäêÎ¶∞', 'ÎÅäÏñ¥', 'ÎÅäÍπÄ', 'ÌäïÍπÄ', 'ÌÅ¨ÎûòÏãú', 'Î©àÏ∂§', 'Ï§ëÎã®', 'ÏßÄÏó∞', 'Îä¶', 'Ïã§Îßù', 'ÌôîÎÇò', 'ÏßúÏ¶ù', 'Ïä§Ìä∏Î†àÏä§', 'Î∂àÌé∏', 'Í∑ÄÏ∞Æ', 'Î≤àÍ±∞Î°≠', 'ÌûòÎì§', 'Ïñ¥Î†µ', 'Î≥µÏû°', 'Ìó∑Í∞à', 'ÌòºÎûÄ', 'Ïï†Îß§', 'Î∂àÎ∂ÑÎ™Ö', 'Î∂àÏïà', 'Í±±Ï†ï', 'Ïö∞Î†§', 'ÏùòÏã¨', 'ÏùòÎ¨∏', 'Í∂ÅÍ∏à', 'Î™®Î•¥', 'Î™∞Îùº', 'Î™®Î¶Ñ', 'Ìó∑Í∞à', 'ÌòºÎûÄ', 'Ïï†Îß§', 'Î∂àÎ∂ÑÎ™Ö', 'Î∂àÏïà', 'Í±±Ï†ï', 'Ïö∞Î†§', 'ÏùòÏã¨', 'ÏùòÎ¨∏', 'Í∂ÅÍ∏à', 'Î™®Î•¥', 'Î™∞Îùº', 'Î™®Î¶Ñ', 'ÌÜµÌôî', 'ÏàòÎ∞úÏã†', 'ÌïúÎ≤àÏóê', 'ÏïàÎêòÍ≥†', 'Î™©ÏÜåÎ¶¨Îûë', 'Îî∞Î°ú', 'ÏïÑÏù¥Îîî', 'ÎßåÎì§Ïñ¥Ïïº', 'ÌÉàÌöåÏ°∞Ï∞®', 'ÎßàÏùåÎåÄÎ°ú', 'Î™ªÌïòÎäî', 'Ïñ¥Ìîå', 'Ï†ÑÌôîÏó∞Í≤∞Ïù¥', 'ÏòàÍ≥†ÏóÜÏù¥', 'ÎÅàÏñ¥ÏßêÏùÄ', 'Í∏∞Î≥∏Ïù¥Í≥†', 'ÏãúÎÅÑÎü¨ÏõåÏ£ΩÍ≤†ÏäµÎãàÎã§', 'Î∞îÍøâÏãúÎã§', 'ÌÜµÌôîÏó∞Í≤∞ÏùåÏ¢Ä', 'ÎÅäÏñ¥ÏßÄÍ≥†', 'ÎÅäÍπÄ', 'ÌäïÍπÄ', 'ÏïàÎê®', 'Ïã§Ìå®', 'Ïò§Î•ò', 'ÏóêÎü¨', 'Î≤ÑÍ∑∏', 'Î¨∏Ï†ú', 'Î∂àÌé∏', 'Ïñ¥Î†§ÏõÄ', 'Î≥µÏû°', 'ÎäêÎ¶º', 'ÏßÄÏó∞', 'Ï§ëÎã®', 'Ï†ïÏßÄ', 'Î©àÏ∂§', 'ÌÅ¨ÎûòÏãú', 'Îã§Ïö¥', 'Ï£ΩÏùå', 'Í∫ºÏßê', 'Ï¢ÖÎ£å', 'ÎÇòÍ∞ê', 'Îπ†Ïßê', 'ÌÉàÏ∂ú', 'Ïù¥ÌÉà', 'Ìè¨Í∏∞', 'Ï§ëÎã®', 'Ìï¥ÏßÄ', 'Ï∑®ÏÜå', 'ÏÇ≠Ï†ú', 'Ï†úÍ±∞', 'ÏóÜÏï∞', 'Î≤ÑÎ¶º', 'ÎçòÏßê', 'ÌùòÎ¶º', 'Îñ®Ïñ¥Îú®Î¶º', 'ÎÜìÏπ®', 'ÏûÉÏùå', 'ÏÉÅÏã§', 'ÏÜêÏã§', 'ÌîºÌï¥', 'ÏÜêÌï¥', 'ÌÉÄÍ≤©', 'Ï∂©Í≤©', 'ÏáºÌÅ¨', 'ÎÜÄÎûå', 'ÎãπÌô©', 'ÌòºÎûÄ', 'Ïñ¥ÏßÄÎü¨ÏõÄ', 'Ìó∑Í∞àÎ¶º', 'Ï∞©Í∞Å', 'Ïã§Ïàò', 'ÏûòÎ™ª', 'ÌãÄÎ¶º', 'Ïò§Ìï¥', 'Ïò§Î•ò', 'ÏûòÎ™ªÎêú', 'ÌãÄÎ¶∞', 'Î∂ÄÏ†ïÌôïÌïú', 'ÏûòÎ™ªÎêú', 'ÎπÑÏ†ïÏÉÅÏ†ÅÏù∏', 'Ïù¥ÏÉÅÌïú', 'ÏàòÏÉÅÌïú', 'ÏùòÏã¨Ïä§Îü¨Ïö¥', 'Î∂àÏïàÌïú', 'Í±±Ï†ïÏä§Îü¨Ïö¥', 'Ïö∞Î†§Ïä§Îü¨Ïö¥', 'ÎëêÎ†§Ïö¥', 'Î¨¥ÏÑúÏö¥', 'Í≥µÌè¨Ïä§Îü¨Ïö¥', 'ÎÅîÏ∞çÌïú', 'Ï∞∏ÌòπÌïú', 'ÎπÑÏ∞∏Ìïú', 'Ïä¨Ìîà', 'Ïö∏Ï†ÅÌïú', 'Ïö∞Ïö∏Ìïú', 'Ïπ®Ïö∏Ìïú', 'ÎãµÎãµÌïú', 'ÎßâÎßâÌïú', 'Ï†àÎßùÏ†ÅÏù∏', 'Ìè¨Í∏∞ÌïòÍ≥†Ïã∂ÏùÄ', 'Í∑∏ÎßåÎëêÍ≥†Ïã∂ÏùÄ', 'ÎïåÎ†§ÏπòÏö∞Í≥†Ïã∂ÏùÄ', 'Í¥ÄÎëêÍ≥†Ïã∂ÏùÄ', 'ÏπòÏö∞Í≥†Ïã∂ÏùÄ', 'ÎçòÏßÄÍ≥†Ïã∂ÏùÄ', 'Î≤ÑÎ¶¨Í≥†Ïã∂ÏùÄ', 'ÏÇ≠Ï†úÌïòÍ≥†Ïã∂ÏùÄ', 'Ìï¥ÏßÄÌïòÍ≥†Ïã∂ÏùÄ', 'Ï∑®ÏÜåÌïòÍ≥†Ïã∂ÏùÄ', 'Î∞òÎÇ©ÌïòÍ≥†Ïã∂ÏùÄ', 'ÌôòÎ∂àÎ∞õÍ≥†Ïã∂ÏùÄ', 'ÎèåÎ†§Î∞õÍ≥†Ïã∂ÏùÄ']
    
    import re
    
    for review in reviews:
        content = review['content']
        rating = review.get('rating', 3)
        
        # Clean Korean text
        cleaned = re.sub(r'[^\w\sÍ∞Ä-Ìû£]', ' ', content)
        words = cleaned.split()
        
        # Filter meaningful Korean words
        korean_words = []
        for word in words:
            word = word.strip()
            if len(word) >= 2 and not word.isdigit() and word not in stop_words:
                # Check if word contains Korean characters
                if any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in word):
                    korean_words.append(word)
        
        # Classify by sentiment with keyword filtering
        if rating >= 4:  # Positive reviews
            for word in korean_words:
                # Only include if it contains positive keywords or has clear positive context
                if any(pos_keyword in word for pos_keyword in positive_keywords) or word in positive_keywords:
                    positive_words[word] = positive_words.get(word, 0) + 1
        else:  # Negative reviews
            for word in korean_words:
                # Only include if it contains negative keywords or has clear negative context
                if any(neg_keyword in word for neg_keyword in negative_keywords) or word in negative_keywords:
                    negative_words[word] = negative_words.get(word, 0) + 1
    
    # Convert to word cloud format (top 10 each as requested)
    positive_cloud = [{'word': word, 'frequency': freq, 'sentiment': 'positive'} 
                      for word, freq in sorted(positive_words.items(), key=lambda x: x[1], reverse=True)[:10]]
    negative_cloud = [{'word': word, 'frequency': freq, 'sentiment': 'negative'} 
                      for word, freq in sorted(negative_words.items(), key=lambda x: x[1], reverse=True)[:10]]
    
    print(f"Generated {len(insights)} HEART insights, {len(positive_cloud)} positive words, {len(negative_cloud)} negative words", file=sys.stderr)
    
    return {
        'insights': insights,
        'wordCloud': {
            'positive': positive_cloud,
            'negative': negative_cloud
        }
    }

def main():
    """Main function to run the scraper"""
    try:
        # Parse command line arguments
        analyze_mode = '--analyze' in sys.argv
        
        if analyze_mode:
            # Remove --analyze flag from arguments
            args = [arg for arg in sys.argv[1:] if arg != '--analyze']
            
            if len(args) >= 3:
                # Format: python scraper.py --analyze google_app_id apple_app_id count sources
                app_id_google = args[0]
                app_id_apple = args[1]
                count = int(args[2])
                sources = args[3].split(',') if len(args) > 3 else ['google_play']
            else:
                # Legacy format: python scraper.py --analyze app_id count
                app_id_google = args[0] if len(args) > 0 else 'com.lguplus.sohoapp'
                app_id_apple = '1571096278'
                count = int(args[1]) if len(args) > 1 else 100
                sources = ['google_play']
        else:
            if len(sys.argv) > 3:
                # Format: python scraper.py google_app_id apple_app_id count sources
                app_id_google = sys.argv[1]
                app_id_apple = sys.argv[2]
                count = int(sys.argv[3])
                sources = sys.argv[4].split(',') if len(sys.argv) > 4 else ['google_play']
            else:
                # Legacy format: python scraper.py app_id count
                app_id_google = sys.argv[1] if len(sys.argv) > 1 else 'com.lguplus.sohoapp'
                app_id_apple = '1571096278'
                count = int(sys.argv[2]) if len(sys.argv) > 2 else 100
                sources = ['google_play']
        
        # Get reviews from specified sources
        reviews_data = scrape_reviews(app_id_google, app_id_apple, count, sources)
        
        if analyze_mode:
            # Perform analysis
            analysis_result = analyze_sentiments(reviews_data)
            result = {
                'success': True,
                'message': f'{len(reviews_data)}Í∞úÏùò Î¶¨Î∑∞Î•º Î∂ÑÏÑùÌñàÏäµÎãàÎã§.',
                'reviewCount': len(reviews_data),
                'insights': analysis_result['insights'],
                'wordCloud': analysis_result['wordCloud']
            }
        else:
            # Always include analysis for collection
            analysis_result = analyze_sentiments(reviews_data)
            result = {
                'success': True,
                'reviews': reviews_data,
                'reviewCount': len(reviews_data),
                'message': f'{len(reviews_data)}Í∞úÏùò Î¶¨Î∑∞Î•º ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏàòÏßëÌñàÏäµÎãàÎã§.',
                'analysis': analysis_result,
                'sources': sources,
                'counts': {
                    'google_play': len([r for r in reviews_data if r['source'] == 'google_play']),
                    'app_store': len([r for r in reviews_data if r['source'] == 'app_store'])
                }
            }
        
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
    except Exception as e:
        error_result = {
            'success': False,
            'error': str(e),
            'message': 'Î¶¨Î∑∞ ÏàòÏßë Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.'
        }
        print(json.dumps(error_result, ensure_ascii=False, indent=2))
        sys.exit(1)

if __name__ == "__main__":
    main()