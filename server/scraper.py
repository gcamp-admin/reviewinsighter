#!/usr/bin/env python3
"""
Google Play Store Review Scraper for ìš°ë¦¬ê°€ê²Œ íŒ¨í‚¤ì§€
"""

import json
import sys
import requests
from datetime import datetime
from google_play_scraper import Sort, reviews
import pandas as pd
import xml.etree.ElementTree as ET
import re
from service_data import get_service_keywords, get_service_info
from naver_api import search_naver, extract_text_from_html

# Enhanced Korean text processing
try:
    from konlpy.tag import Okt, Kkma
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    ADVANCED_PROCESSING = True
except ImportError:
    ADVANCED_PROCESSING = False
    print("Advanced Korean processing libraries not available, using basic text processing", file=sys.stderr)

def extract_korean_words_advanced(text_list, sentiment='positive', max_words=10):
    """
    Enhanced Korean word extraction using KoNLPy for morphological analysis
    
    Args:
        text_list: List of text strings to analyze
        sentiment: Target sentiment ('positive' or 'negative')
        max_words: Maximum number of words to return
        
    Returns:
        List of word frequency dictionaries
    """
    if not ADVANCED_PROCESSING or not text_list:
        return extract_korean_words_basic(text_list, sentiment, max_words)
    
    try:
        # Initialize Korean morphological analyzer
        okt = Okt()
        word_freq = {}
        
        for text in text_list:
            if not text or not isinstance(text, str):
                continue
                
            # Extract nouns and adjectives (most meaningful for sentiment analysis)
            morphs = okt.pos(text, stem=True)
            
            # Filter for meaningful Korean words
            for word, pos in morphs:
                # Include nouns, adjectives, and verbs
                if pos in ['Noun', 'Adjective', 'Verb'] and len(word) >= 2:
                    # Remove common stop words
                    if word not in ['ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'í•˜ë‹¤', 'ì´ë‹¤', 'ê·¸ë ‡ë‹¤', 'ê°™ë‹¤', 'ë‹¤ë¥´ë‹¤', 'ë§ë‹¤', 'ì ë‹¤', 'í¬ë‹¤', 'ì‘ë‹¤', 'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ìƒˆë¡­ë‹¤', 'ì˜¤ë˜ë˜ë‹¤']:
                        word_freq[word] = word_freq.get(word, 0) + 1
        
        # Sort by frequency and return top words
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        
        result = []
        for word, freq in sorted_words[:max_words]:
            result.append({
                'word': word,
                'frequency': freq,
                'sentiment': sentiment
            })
        
        print(f"Advanced Korean processing extracted {len(result)} {sentiment} words", file=sys.stderr)
        return result
        
    except Exception as e:
        print(f"Error in advanced Korean processing: {e}, falling back to basic processing", file=sys.stderr)
        return extract_korean_words_basic(text_list, sentiment, max_words)

def extract_korean_words_basic(text_list, sentiment='positive', max_words=10):
    """
    Basic Korean word extraction using regex and frequency analysis
    """
    word_freq = {}
    
    for text in text_list:
        if not text or not isinstance(text, str):
            continue
            
        # Remove punctuation and split into words
        import re
        korean_words = re.findall(r'[ê°€-í£]+', text)
        
        for word in korean_words:
            if len(word) >= 2:  # Filter out single characters
                # Skip common words and particles
                skip_words = ['ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ì´ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ', 'ì €ë ‡ê²Œ', 'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ì—ê²Œ', 'í•œí…Œ', 'ì—ë„', 'ë„', 'ëŠ”', 'ì€', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ê³¼', 'ì™€', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ê°™ì´', 'ë§ˆë‹¤', 'ë§ˆì €', 'ì¡°ì°¨', 'ë°–ì—', 'ì™¸ì—', 'ëŒ€ì‹ ', 'ë§ê³ ']
                
                if word not in skip_words:
                    word_freq[word] = word_freq.get(word, 0) + 1
    
    # Sort by frequency and return top words
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    
    result = []
    for word, freq in sorted_words[:max_words]:
        result.append({
            'word': word,
            'frequency': freq,
            'sentiment': sentiment
        })
    
    return result

def analyze_text_sentiment(text):
    """
    Korean text-based sentiment analysis
    Analyzes review content to determine sentiment regardless of star ratings
    
    Args:
        text: Review text content
        
    Returns:
        String: 'positive' or 'negative'
    """
    if not text or not isinstance(text, str):
        return "positive"  # Default to positive for empty/invalid text
    
    # Convert to lowercase for analysis
    content = text.lower()
    
    # Priority rule: Any review containing 'ë¶ˆí¸' is automatically negative
    if 'ë¶ˆí¸' in content:
        return "negative"
    
    # Enhanced negative keywords in Korean
    strong_negative_keywords = [
        # Original priority keyword
        "ë¶ˆí¸", "ê·€ì°®", "ì§œì¦", "í™”ë‚¨", "ê±°ìŠ¬ë¦¼", "ìŠ¤íŠ¸ë ˆìŠ¤",
        "í˜ë“¤", "ì–´ë µ", "ë³„ë¡œ", "ìµœì•…", "í˜•í¸ì—†", "êµ¬ë¦¬", "ì‹¤ë§",
        "ë‚˜ì˜", "ì•ˆë¨", "ì•ˆë¼", "ì•ˆë˜", "ì—ëŸ¬", "ì˜¤ë¥˜", "ë¨¹í†µ",
        "ë©ˆì¶¤", "íŠ•ê¹€", "ëŠë¦¼", "ë ‰", "ë³µì¡", "ì§ê´€", "ë‹µë³€ ì—†ìŒ", "ê´‘ê³  ë§", "ê°•ì œ",
        
        # Critical issues
        'ë²„ê·¸', 'íŠ•ê¸´ë‹¤', 'ë‚˜ê°€ë²„ë¦¼', 'êº¼ì§', 'í¬ë˜ì‹œ', 'ì¢…ë£Œ', 'ì¬ì‹œì‘',
        'ì‘ë™ì•ˆí•¨', 'ì‹¤í–‰ì•ˆë¨', 'ì•ˆë°›ì•„ì ¸', 'ë°›ì•„ì§€ì§€', 'ì‹¤í–‰ë˜ì§€', 'ì‘ë™í•˜ì§€',
        'ëŠê¹€', 'ëŠì–´ì§€', 'ëŠê¸´ë‹¤', 'ì—°ê²°ì•ˆë¨', 'ì•ˆë“¤ë¦¼', 'ì†Œë¦¬ì•ˆë‚¨',
        
        # Strong dissatisfaction
        'ì“°ë ˆê¸°', 'ë¹¡ì¹¨', 'ì—´ë°›', 'ë¶ˆë§Œ', 'ì‹«ì–´', 'ë‹µë‹µ', 'ë‹¹í™©ìŠ¤ëŸ¬ìš´',
        'ë¬¸ì œ', 'ê³ ì¥', 'ë§í•¨', 'ì—‰ë§',
        
        # Deletion/cancellation intent
        'ì‚­ì œ', 'ì§€ì›€', 'í•´ì§€', 'ê·¸ë§Œ', 'ì•ˆì“¸', 'ë‹¤ë¥¸ê±°', 'ë°”ê¿€', 'íƒˆí‡´', 'í¬ê¸°', 'ì¤‘ë‹¨',
        'ì•ˆì¨', 'ì‚¬ìš©ì•ˆí•¨', 'ëª»ì“°ê² ', 'ì“¸ëª¨ì—†',
        
        # Specific app issues
        'í†µí™”ì¤‘ ëŒ€ê¸°', 'ì•ˆì§€ì›', 'ë³¼ë¥¨ë²„íŠ¼', 'ì§„ë™', 'ë°±ê·¸ë¼ìš´ë“œ', 'ìë™ìœ¼ë¡œ', 'ìŠ¬ë¼ì´ë“œ',
        'ìŠ¤íŒ¸ì •ë³´', 'ë”¸ë ¤ì™€ì„œ', 'ë²ˆí˜¸í™•ì¸', 'ê¸°ë‹¤ë ¤ì•¼', 'ì°¨ëŸ‰', 'ë¸”íˆ¬', 'í†µí™”ì¢…ë£Œ',
        
        # Emotional expressions
        'í™”ë©´ í™•ëŒ€ ì•ˆë¨', 'ëª»ì•Œì•„', 'ì§€ë‚˜ì¹˜ëŠ”', 'ì• í”Œì´ë“ ', 'ì‚¼ì„±ì´ë“ ',
        'ì €ê²©í•˜ë ¤ê³ ', 'ì•Œëœ°í° ì•ˆëœë‹¤', 'ì§œì¦ë‚˜ì£ ', 'ì•ˆë ê±°', 'ì™œ ì•ˆë©ë‹ˆê¹Œ', 'ë‚œë¦¬ë‚¬ìŒ',
        'ìƒëŒ€ë°©ê³¼ ë‚˜ì˜ ëª©ì†Œë¦¬ì˜ ì‹±í¬ê°€ ë§ì§€ ì•Šê³ ', 'ìš¸ë¦¬ì§€ì•Šê±°ë‚˜', 'ë¶€ì¬ì¤‘', 'ë°”ë¡œ ëŠê¸°ê³ ',
        'ì•ˆê±¸ë¦¬ëŠ”', 'ë¹ˆë²ˆí•¨', 'ì‹œë„ëŸ¬ì›Œì£½ê² ìŠµë‹ˆë‹¤', 'ë‹¹í™©ìŠ¤ëŸ¬ìš´', 'ë¶ˆí¸í•˜ë„¤ìš”'
    ]
    
    # Positive indicators (Korean expressions)
    positive_keywords = [
        # Direct praise
        'ì¢‹ì•„', 'ì¢‹ë‹¤', 'ì¢‹ë„¤', 'ì¢‹ìŒ', 'í›Œë¥­', 'ìš°ìˆ˜', 'ìµœê³ ', 'ëŒ€ë°•', 'ì™„ë²½', 'ë§Œì¡±',
        'ì˜', 'í¸ë¦¬', 'ìœ ìš©', 'ë„ì›€', 'ê°ì‚¬', 'ê³ ë§ˆì›Œ', 'ì¶”ì²œ', 'ê´œì°®', 'ë‚˜ì˜ì§€ì•Š',
        
        # Functional satisfaction
        'ì˜ì‚¬ìš©', 'ì˜ì“°', 'ì˜ë¨', 'ì˜ë˜', 'ì˜ì‘ë™', 'ì •ìƒ', 'ì›í™œ', 'ë¶€ë“œëŸ½', 'ë¹ ë¥´',
        'ê°„í¸', 'ì‰½', 'í¸í•´', 'ê¹”ë”', 'ì•ˆì •', 'ì‹ ë¢°',
        
        # Appreciation
        'ìœ ìš©í•˜ê³ ', 'ì¢‹ì•„ìš”', 'ë§‰ì•„ì¤˜ì„œ', 'ìš”ì•½ë˜ê³ ', 'í…ìŠ¤íŠ¸ë¡œ', 'ì¨ì ¸ì„œ',
        'ë³´ì´ìŠ¤í”¼ì‹±', 'ë§‰ì•„ì¤˜ì„œ', 'ì¢‹ì•„ìš”', 'aiê³ ', 'í†µí™”ë‚´ìš©',
        
        # Mild complaints that are still generally positive
        'ì¢‹ì§€ë§Œ', 'ë§Œì¡±í•©ë‹ˆë‹¤ë§Œ', 'ì „ë°˜ì ì¸ ê¸°ëŠ¥ì€ ë§Œì¡±', 'ì˜ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤',
        'ë”± í•œê°€ì§€ ì•„ì‰¬ìš´ê²Œ', 'ì´ê²ƒë§Œ ëœë‹¤ë©´', 'ì •ë§ ì™„ë²½í• ê±°'
    ]
    
    # Count negative and positive indicators
    negative_count = 0
    positive_count = 0
    
    for keyword in strong_negative_keywords:
        if keyword in content:
            negative_count += 1
    
    for keyword in positive_keywords:
        if keyword in content:
            positive_count += 1
    
    # Special handling for mixed sentiment reviews
    # If review contains both positive and negative elements, analyze overall tone
    if negative_count > 0 and positive_count > 0:
        # Check for constructive feedback patterns
        constructive_patterns = [
            'ì¢‹ì§€ë§Œ', 'ë§Œì¡±í•©ë‹ˆë‹¤ë§Œ', 'ì¢‹ê² ë„¤ìš”', 'ëœë‹¤ë©´', 'ì§€ì›í•´ì¤„ìˆ˜', 'ê°œì„ ',
            'ì¶”ê°€', 'í–¥ìƒ', 'ì—…ë°ì´íŠ¸', 'ë°”ê¿‰ì‹œë‹¤', 'í•˜ë©´ ì¢‹ê² '
        ]
        
        is_constructive = any(pattern in content for pattern in constructive_patterns)
        
        # If it's constructive feedback, weight it based on severity
        if is_constructive:
            # Count severity of negative issues
            critical_issues = ['ì˜¤ë¥˜', 'ì—ëŸ¬', 'ë²„ê·¸', 'íŠ•ê¹€', 'í¬ë˜ì‹œ', 'ìµœì•…', 'ì“°ë ˆê¸°', 'ì‚­ì œ']
            has_critical = any(issue in content for issue in critical_issues)
            
            if has_critical:
                return "negative"
            else:
                # Constructive feedback without critical issues = positive
                return "positive"
        else:
            # Mixed sentiment without constructive tone - go with majority
            return "negative" if negative_count > positive_count else "positive"
    
    # Pure sentiment analysis
    if negative_count > 0:
        return "negative"
    elif positive_count > 0:
        return "positive"
    else:
        # No clear sentiment indicators - analyze length and structure
        # Very short reviews or question-only reviews default to positive
        if len(content.strip()) < 10:
            return "positive"
        
        # Check for question marks (often constructive)
        if '?' in content or 'ì–¸ì œ' in content or 'ì–´ë–»ê²Œ' in content:
            return "positive"
        
        # Default to positive for neutral content
        return "positive"

def scrape_google_play_reviews(app_id='com.lguplus.sohoapp', count=100, lang='ko', country='kr'):
    """
    Scrape reviews from Google Play Store
    
    Args:
        app_id: Google Play Store app ID
        count: Number of reviews to fetch
        lang: Language code (default: 'ko')
        country: Country code (default: 'kr')
        
    Returns:
        List of review dictionaries
    """
    try:
        # Fetch reviews from Google Play Store
        result, _ = reviews(
            app_id,
            lang=lang,
            country=country,
            sort=Sort.NEWEST,
            count=count
        )
        
        # Process and clean the data
        processed_reviews = []
        for review in result:
            # Text-based sentiment analysis - ignore star ratings completely
            sentiment = analyze_text_sentiment(review['content'])
            
            processed_review = {
                'userId': review['userName'] if review['userName'] else 'ìµëª…',
                'source': 'google_play',
                'rating': review['score'],
                'content': review['content'],
                'sentiment': sentiment,
                'createdAt': review['at'].isoformat() if review['at'] else datetime.now().isoformat()
            }
            processed_reviews.append(processed_review)
        
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping Google Play reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_app_store_reviews(app_id='1571096278', count=100):
    """
    Scrape reviews from Apple App Store
    
    Args:
        app_id: Apple App Store app ID
        count: Number of reviews to fetch (limited by RSS feed)
        
    Returns:
        List of review dictionaries
    """
    try:
        # Construct RSS URL for App Store reviews
        rss_url = f'https://itunes.apple.com/kr/rss/customerreviews/id={app_id}/sortBy=mostRecent/xml'
        
        response = requests.get(rss_url, timeout=10)
        if response.status_code != 200:
            print(f"Failed to fetch App Store reviews: HTTP {response.status_code}", file=sys.stderr)
            return []
        
        root = ET.fromstring(response.content)
        
        # Process reviews (skip first entry which is just metadata)
        processed_reviews = []
        entries = root.findall('.//{http://www.w3.org/2005/Atom}entry')[1:]
        
        for entry in entries[:count]:  # Limit to requested count
            try:
                # Extract review data
                author_elem = entry.find('{http://www.w3.org/2005/Atom}author')
                author = author_elem[0].text if author_elem is not None and len(author_elem) > 0 else 'ìµëª…'
                
                title_elem = entry.find('{http://www.w3.org/2005/Atom}title')
                title = title_elem.text if title_elem is not None else ''
                
                content_elem = entry.find('{http://www.w3.org/2005/Atom}content')
                content = content_elem.text if content_elem is not None else title  # Use title if no content
                
                rating_elem = entry.find('{http://itunes.apple.com/rss}rating')
                rating = int(rating_elem.text) if rating_elem is not None else 3
                
                updated_elem = entry.find('{http://www.w3.org/2005/Atom}updated')
                created_at = updated_elem.text if updated_elem is not None else datetime.now().isoformat()
                
                # Text-based sentiment analysis - ignore star ratings completely
                sentiment = analyze_text_sentiment(content)
                
                processed_review = {
                    'userId': author,
                    'source': 'app_store',
                    'rating': rating,
                    'content': content,
                    'sentiment': sentiment,
                    'createdAt': created_at
                }
                processed_reviews.append(processed_review)
                
            except Exception as entry_error:
                print(f"Error processing App Store review entry: {entry_error}", file=sys.stderr)
                continue
        
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping App Store reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_naver_blog_reviews(service_name='ìµì‹œì˜¤', count=100):
    """
    Scrape reviews from Naver Blog using real API
    
    Args:
        service_name: Service name to search for
        count: Number of reviews to fetch
        
    Returns:
        List of review dictionaries
    """
    try:
        # Get service keywords for better search results
        keywords = get_service_keywords(service_name)
        processed_reviews = []
        
        # Search with multiple keywords to get diverse results
        search_results = []
        for keyword in keywords[:3]:  # Use top 3 keywords to avoid API rate limits
            results = search_naver(keyword, search_type="blog", display=min(count // len(keywords[:3]) + 5, 30))
            search_results.extend(results)
        
        # Process blog search results
        for i, item in enumerate(search_results[:count]):
            try:
                # Filter out non-review content using quality check
                from naver_api import is_likely_user_review
                if not is_likely_user_review(item, keywords):
                    continue
                
                # Extract clean text from description
                description = extract_text_from_html(item.get('description', ''))
                title = extract_text_from_html(item.get('title', ''))
                
                # Combine title and description for content
                content = f"{title}. {description}" if title and description else (title or description)
                
                # Skip if content is too short
                if len(content.strip()) < 10:
                    continue
                
                # Text-based sentiment analysis
                sentiment = analyze_text_sentiment(content)
                
                processed_review = {
                    'userId': f"ë¸”ë¡œê±°{i+1}",
                    'source': 'naver_blog',
                    'rating': 4 if sentiment == 'positive' else 2,
                    'content': content[:500],  # Limit content length
                    'sentiment': sentiment,
                    'createdAt': item.get('postdate', datetime.now().strftime('%Y%m%d')),
                    'url': item.get('link', '')
                }
                processed_reviews.append(processed_review)
                
            except Exception as item_error:
                print(f"Error processing blog item {i}: {str(item_error)}", file=sys.stderr)
                continue
        
        print(f"Collected {len(processed_reviews)} Naver Blog reviews for {service_name} using keywords: {keywords[:3]}", file=sys.stderr)
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping Naver Blog reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_naver_cafe_reviews(service_name='ìµì‹œì˜¤', count=100):
    """
    Scrape reviews from Naver Cafe using real API
    
    Args:
        service_name: Service name to search for
        count: Number of reviews to fetch
        
    Returns:
        List of review dictionaries
    """
    try:
        # Get service keywords for better search results
        keywords = get_service_keywords(service_name)
        processed_reviews = []
        
        # Search with multiple keywords to get diverse results
        search_results = []
        for keyword in keywords[:3]:  # Use top 3 keywords to avoid API rate limits
            results = search_naver(keyword, search_type="cafe", display=min(count // len(keywords[:3]) + 5, 30))
            search_results.extend(results)
        
        # Process cafe search results
        for i, item in enumerate(search_results[:count]):
            try:
                # Filter out non-review content using quality check
                from naver_api import is_likely_user_review
                if not is_likely_user_review(item, keywords):
                    continue
                
                # Extract clean text from description
                description = extract_text_from_html(item.get('description', ''))
                title = extract_text_from_html(item.get('title', ''))
                
                # Combine title and description for content
                content = f"{title}. {description}" if title and description else (title or description)
                
                # Skip if content is too short
                if len(content.strip()) < 10:
                    continue
                
                # Text-based sentiment analysis
                sentiment = analyze_text_sentiment(content)
                
                processed_review = {
                    'userId': f"ì¹´í˜íšŒì›{i+1}",
                    'source': 'naver_cafe',
                    'rating': 4 if sentiment == 'positive' else 2,
                    'content': content[:500],  # Limit content length
                    'sentiment': sentiment,
                    'createdAt': datetime.now().isoformat(),
                    'url': item.get('link', ''),
                    'cafe_name': item.get('cafename', '')
                }
                processed_reviews.append(processed_review)
                
            except Exception as item_error:
                print(f"Error processing cafe item {i}: {str(item_error)}", file=sys.stderr)
                continue
        
        print(f"Collected {len(processed_reviews)} Naver Cafe reviews for {service_name} using keywords: {keywords[:3]}", file=sys.stderr)
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping Naver Cafe reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_reviews(app_id_google='com.lguplus.sohoapp', app_id_apple='1571096278', count=100, sources=['google_play'], service_name='ìµì‹œì˜¤'):
    """
    Scrape reviews from multiple sources
    
    Args:
        app_id_google: Google Play Store app ID
        app_id_apple: Apple App Store app ID
        count: Number of reviews to fetch per source
        sources: List of sources to scrape from
        service_name: Service name for Naver searches
        
    Returns:
        List of review dictionaries
    """
    all_reviews = []
    
    if 'google_play' in sources:
        google_reviews = scrape_google_play_reviews(app_id_google, count)
        all_reviews.extend(google_reviews)
        print(f"Collected {len(google_reviews)} reviews from Google Play", file=sys.stderr)
    
    if 'app_store' in sources:
        apple_reviews = scrape_app_store_reviews(app_id_apple, count)
        all_reviews.extend(apple_reviews)
        print(f"Collected {len(apple_reviews)} reviews from App Store", file=sys.stderr)
    
    if 'naver_blog' in sources:
        blog_reviews = scrape_naver_blog_reviews(service_name, count)
        all_reviews.extend(blog_reviews)
        print(f"Collected {len(blog_reviews)} reviews from Naver Blog", file=sys.stderr)
    
    if 'naver_cafe' in sources:
        cafe_reviews = scrape_naver_cafe_reviews(service_name, count)
        all_reviews.extend(cafe_reviews)
        print(f"Collected {len(cafe_reviews)} reviews from Naver Cafe", file=sys.stderr)
    
    return all_reviews

def analyze_sentiments(reviews):
    """
    Enhanced HEART framework analysis with dynamic insights generation
    
    Args:
        reviews: List of review dictionaries
        
    Returns:
        Dictionary with insights and word frequency data
    """
    if not reviews:
        return {'insights': [], 'wordCloud': {'positive': [], 'negative': []}}
    
    print(f"Starting enhanced HEART analysis on {len(reviews)} reviews...", file=sys.stderr)
    
    # Re-analyze sentiment based on text content only (ignore star ratings)
    for review in reviews:
        # Update sentiment based on text analysis
        review['sentiment'] = analyze_text_sentiment(review['content'])
    
    # Debug: Print text-based sentiment analysis results
    text_based_negative = sum(1 for r in reviews if r['sentiment'] == 'negative')
    text_based_positive = sum(1 for r in reviews if r['sentiment'] == 'positive')
    print(f"Text-based sentiment analysis: {text_based_positive} positive, {text_based_negative} negative", file=sys.stderr)
    
    # HEART framework analysis with detailed issue tracking
    heart_analysis = {
        'task_success': {'issues': [], 'details': []},
        'happiness': {'issues': [], 'details': []},
        'engagement': {'issues': [], 'details': []},
        'adoption': {'issues': [], 'details': []},
        'retention': {'issues': [], 'details': []}
    }
    
    # Pattern matching for specific issues
    for review in reviews:
        content = review['content'].lower()
        rating = review.get('rating', 3)
        user_id = review.get('userId', 'Unknown')
        
        # Analyze ALL reviews regardless of rating to capture nuanced feedback
        # Even high-rated reviews can contain specific complaints and improvement suggestions
        
        # Task Success - Core functionality problems (check regardless of rating)
        if any(keyword in content for keyword in ['ì˜¤ë¥˜', 'ì—ëŸ¬', 'ë²„ê·¸', 'íŠ•', 'êº¼ì§', 'ì‘ë™ì•ˆí•¨', 'ì‹¤í–‰ì•ˆë¨', 'ëŠê¹€', 'ì—°ê²°ì•ˆë¨', 'ì•ˆë“¤ë¦¼', 'ì†Œë¦¬ì•ˆë‚¨', 'ì•ˆë¨', 'ì•ˆë˜', 'í¬ë˜ì‹œ', 'ì¢…ë£Œ', 'ì¬ì‹œì‘', 'ë¬¸ì œ', 'ë¶ˆí¸', 'ì•ˆë°›ì•„ì§€', 'ë°›ì•„ì§€ì§€', 'ì‹¤í–‰ë˜ì§€', 'ì‘ë™í•˜ì§€', 'ëŠì–´ì§€', 'ëŠê¸´ë‹¤', 'ë‹¹í™©ìŠ¤ëŸ¬ìš´', 'ê¸°ë‹¤ë ¤ì•¼', 'ìŠ¬ë¼ì´ë“œ', 'ë°±ê·¸ë¼ìš´ë“œ', 'ìë™ìœ¼ë¡œ', 'ë„˜ì–´ê°€ì§€', 'ê³„ì†', 'ë³¼ë¥¨ë²„íŠ¼', 'ì§„ë™', 'êº¼ì§€ë©´', 'ì¢‹ê² ë„¤ìš”', 'ì°¨ëŸ‰', 'ë¸”íˆ¬', 'í†µí™”ì¢…ë£Œ', 'ìŒì•…ì¬ìƒ', 'ìŠ¤íŒ¸ì •ë³´', 'ë”¸ë ¤ì™€ì„œ', 'ë²ˆí˜¸í™•ì¸', 'ê¸°ë‹¤ë ¤ì•¼']):
            heart_analysis['task_success']['issues'].append(content)
            if 'íŠ•' in content or 'êº¼ì§' in content or 'í¬ë˜ì‹œ' in content:
                heart_analysis['task_success']['details'].append('ì•± í¬ë˜ì‹œ')
            elif 'ì—°ê²°' in content and ('ì•ˆë¨' in content or 'ëŠê¹€' in content):
                heart_analysis['task_success']['details'].append('ë„¤íŠ¸ì›Œí¬ ì—°ê²°')
            elif 'ì†Œë¦¬' in content and 'ì•ˆë‚¨' in content:
                heart_analysis['task_success']['details'].append('ìŒì„± ê¸°ëŠ¥')
            elif 'ë³¼ë¥¨ë²„íŠ¼' in content or 'ì§„ë™' in content:
                heart_analysis['task_success']['details'].append('í•˜ë“œì›¨ì–´ ì œì–´')
            elif 'ë°±ê·¸ë¼ìš´ë“œ' in content or 'ìë™ìœ¼ë¡œ' in content:
                heart_analysis['task_success']['details'].append('ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬')
            elif 'ìŠ¤íŒ¸ì •ë³´' in content or 'ìŠ¬ë¼ì´ë“œ' in content:
                heart_analysis['task_success']['details'].append('UI í‘œì‹œ ë¬¸ì œ')
            elif 'í†µí™”' in content or 'ì „í™”' in content:
                heart_analysis['task_success']['details'].append('í†µí™” ê¸°ëŠ¥')
            else:
                heart_analysis['task_success']['details'].append('ê¸°ëŠ¥ ì˜¤ë¥˜')
        
        # Happiness - User satisfaction issues (check regardless of rating)
        elif any(keyword in content for keyword in ['ì§œì¦', 'ìµœì•…', 'ì‹¤ë§', 'í™”ë‚¨', 'ë¶ˆë§Œ', 'ë³„ë¡œ', 'êµ¬ë¦¼', 'ì‹«ì–´', 'ë‹µë‹µ', 'ìŠ¤íŠ¸ë ˆìŠ¤', 'ë‹¹í™©ìŠ¤ëŸ¬ìš´', 'ë¶ˆí¸', 'ê¸°ë‹¤ë ¤ì•¼', 'ë¬¸ì œ']):
            heart_analysis['happiness']['issues'].append(content)
            if 'ìµœì•…' in content or 'í™”ë‚¨' in content:
                heart_analysis['happiness']['details'].append('ê°•í•œ ë¶ˆë§Œ')
            elif 'ë‹¹í™©ìŠ¤ëŸ¬ìš´' in content or 'ë¶ˆí¸' in content:
                heart_analysis['happiness']['details'].append('ì‚¬ìš©ì ê²½í—˜ ì €í•˜')
            else:
                heart_analysis['happiness']['details'].append('ë§Œì¡±ë„ ì €í•˜')
        
        # Engagement - Usage patterns (check regardless of rating)
        elif any(keyword in content for keyword in ['ì•ˆì¨', 'ì‚¬ìš©ì•ˆí•¨', 'ì¬ë¯¸ì—†', 'ì§€ë£¨', 'í¥ë¯¸ì—†', 'ë³„ë¡œì•ˆì“´', 'ê°€ë”ë§Œ', 'ì¢‹ì§€ë§Œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ë‹¤ë§Œ', 'ì•„ì‰¬ìš´', 'ë”', 'ì¶”ê°€', 'ê°œì„ ', 'í–¥ìƒ', 'ì¢‹ê² ë„¤ìš”']):
            heart_analysis['engagement']['issues'].append(content)
            if 'ì¢‹ì§€ë§Œ' in content or 'í•˜ì§€ë§Œ' in content or 'ì¢‹ê² ë„¤ìš”' in content:
                heart_analysis['engagement']['details'].append('ê°œì„  ì œì•ˆ')
            else:
                heart_analysis['engagement']['details'].append('ì‚¬ìš© ë¹ˆë„ ì €í•˜')
        
        # Retention - Churn indicators (check regardless of rating)
        elif any(keyword in content for keyword in ['ì‚­ì œ', 'í•´ì§€', 'ê·¸ë§Œ', 'ì•ˆì“¸', 'ë‹¤ë¥¸ê±°', 'ë°”ê¿€', 'íƒˆí‡´', 'í¬ê¸°', 'ì¤‘ë‹¨']):
            heart_analysis['retention']['issues'].append(content)
            heart_analysis['retention']['details'].append('ì´íƒˆ ìœ„í—˜')
        
        # Adoption - Onboarding difficulties (check regardless of rating)
        elif any(keyword in content for keyword in ['ì–´ë ¤ì›€', 'ë³µì¡', 'ëª¨ë¥´ê² ', 'í—·ê°ˆ', 'ì–´ë–»ê²Œ', 'ì„¤ëª…ë¶€ì¡±', 'ì‚¬ìš©ë²•', 'ê°€ì´ë“œ', 'ë„ì›€ë§']):
            heart_analysis['adoption']['issues'].append(content)
            heart_analysis['adoption']['details'].append('ì‚¬ìš©ì„± ë¬¸ì œ')
    
    # Generate insights based on actual review content analysis
    insights = []
    insight_id = 1
    
    # Business impact weights
    impact_weights = {
        'task_success': 5,  # Critical - core functionality
        'retention': 4,     # High - user churn
        'happiness': 3,     # Medium - satisfaction
        'engagement': 2,    # Low-Medium - usage
        'adoption': 1       # Low - onboarding
    }
    
    # Debug: Print heart analysis results
    print(f"Heart analysis results:", file=sys.stderr)
    for category, data in heart_analysis.items():
        print(f"  {category}: {len(data['issues'])} issues", file=sys.stderr)
        if data['issues']:
            print(f"    First issue: {data['issues'][0][:50]}...", file=sys.stderr)
    
    # Generate insights based on the analyzed HEART categories
    for category, data in heart_analysis.items():
        if data['issues']:
            count = len(data['issues'])
            impact_score = count * impact_weights[category]
            
            # Priority calculation (more lenient thresholds)
            if impact_score >= 10 or (category == 'task_success' and count >= 2):
                priority = "critical"
                priority_emoji = "ğŸ”´"
            elif impact_score >= 4 or count >= 2:
                priority = "major"
                priority_emoji = "ğŸŸ "
            else:
                priority = "minor"
                priority_emoji = "ğŸŸ¢"
            
            # Analyze actual review content to identify specific problems
            actual_issues = []
            for issue_text in data['issues']:
                # Extract key phrases and issues from actual reviews
                if 'í¬ë˜ì‹œ' in issue_text or 'êº¼ì ¸' in issue_text or 'êº¼ì§€' in issue_text or 'íŠ•ê²¨' in issue_text or 'íŠ•ê¹€' in issue_text or 'ë‚˜ê°€ë²„ë¦¼' in issue_text:
                    actual_issues.append('ì•± í¬ë˜ì‹œ/ê°•ì œ ì¢…ë£Œ')
                elif ('ì „í™”' in issue_text or 'í†µí™”' in issue_text) and ('ëŠì–´' in issue_text or 'ë°›' in issue_text or 'ì•ˆë¨' in issue_text or 'ëŠê¹€' in issue_text):
                    actual_issues.append('í†µí™” ê¸°ëŠ¥ ì˜¤ë¥˜')
                elif 'ì—°ê²°' in issue_text or 'ë„¤íŠ¸ì›Œí¬' in issue_text or 'ì ‘ì†' in issue_text:
                    actual_issues.append('ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ')
                elif 'ë¡œê·¸ì¸' in issue_text or 'ì¸ì¦' in issue_text or 'ë¡œê·¸' in issue_text:
                    actual_issues.append('ë¡œê·¸ì¸/ì¸ì¦ ë¬¸ì œ')
                elif 'ì‚­ì œ' in issue_text or 'í•´ì§€' in issue_text or 'ê·¸ë§Œ' in issue_text:
                    actual_issues.append('ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì˜ë„')
                elif 'ë¶ˆí¸' in issue_text or 'ë³µì¡' in issue_text or 'ì–´ë ¤ì›€' in issue_text:
                    actual_issues.append('ì‚¬ìš©ì„± ë¬¸ì œ')
                else:
                    actual_issues.append('ê¸°íƒ€ ë¬¸ì œ')
            
            # Find most common actual issue
            if actual_issues:
                most_common_issue = max(set(actual_issues), key=actual_issues.count)
                issue_count = actual_issues.count(most_common_issue)
            else:
                most_common_issue = 'ê¸°íƒ€ ë¬¸ì œ'
                issue_count = count
            
            # Generate realistic problem prediction and solution based on HEART category
            predicted_problem = ""
            realistic_solution = ""
            
            if category == 'task_success':
                if most_common_issue == 'í†µí™” ê¸°ëŠ¥ ì˜¤ë¥˜':
                    predicted_problem = "í†µí™” ì—°ê²° ì‹¤íŒ¨ë¡œ ì¸í•œ í•µì‹¬ ê¸°ëŠ¥ ìˆ˜í–‰ ë¶ˆê°€"
                    realistic_solution = "í†µí™” ì—°ê²° ë¡œì§ ì ê²€, VoIP ì„œë²„ ì•ˆì •ì„± ê°•í™”, ë„¤íŠ¸ì›Œí¬ ìƒíƒœë³„ ëŒ€ì‘ ë¡œì§ ê°œë°œ"
                elif most_common_issue == 'ë¡œê·¸ì¸/ì¸ì¦ ë¬¸ì œ':
                    predicted_problem = "ë¡œê·¸ì¸ ì‹¤íŒ¨ë¡œ ì¸í•œ ì„œë¹„ìŠ¤ ì ‘ê·¼ ë¶ˆê°€"
                    realistic_solution = "ì¸ì¦ ì„œë²„ ëª¨ë‹ˆí„°ë§ ê°•í™”, ë‹¤ì¤‘ ì¸ì¦ ë°©ì‹ ì œê³µ, ë¡œê·¸ì¸ ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì•ˆë‚´ ë©”ì‹œì§€"
                elif most_common_issue == 'í†µí™”ì¤‘ëŒ€ê¸° ê¸°ëŠ¥ ë¶€ì¬':
                    predicted_problem = "í†µí™”ì¤‘ëŒ€ê¸° ë¯¸ì§€ì›ìœ¼ë¡œ ì¸í•œ ì—…ë¬´ íš¨ìœ¨ì„± ì €í•˜"
                    realistic_solution = "í†µí™”ì¤‘ëŒ€ê¸° ê¸°ëŠ¥ ê°œë°œ, ë©€í‹°íƒœìŠ¤í‚¹ ì§€ì›, ì½œì„¼í„° ì‹œìŠ¤í…œ ì—°ë™"
                elif most_common_issue == 'ì• í”Œì›Œì¹˜ í˜¸í™˜ì„± ë¬¸ì œ':
                    predicted_problem = "ì›¨ì–´ëŸ¬ë¸” ê¸°ê¸° ë¯¸ì§€ì›ìœ¼ë¡œ ì¸í•œ ì ‘ê·¼ì„± ì œí•œ"
                    realistic_solution = "WatchOS ì—°ë™ ê°œë°œ, ì›Œì¹˜ ì „ìš© UI êµ¬í˜„, í•˜ë“œì›¨ì–´ ë²„íŠ¼ ì§€ì›"
                elif most_common_issue == 'ë¸”ë£¨íˆ¬ìŠ¤/ì—ì–´íŒŸ í˜¸í™˜ì„± ë¬¸ì œ':
                    predicted_problem = "ì˜¤ë””ì˜¤ ê¸°ê¸° í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¸í•œ ì‚¬ìš©ì ê²½í—˜ ì €í•˜"
                    realistic_solution = "ë¸”ë£¨íˆ¬ìŠ¤ í”„ë¡œíŒŒì¼ ì§€ì› í™•ëŒ€, ì˜¤ë””ì˜¤ ì½”ë± ìµœì í™”, ê¸°ê¸°ë³„ í…ŒìŠ¤íŠ¸"
                elif most_common_issue == 'í†µí™”ì—°ê²°ìŒ ê´€ë ¨ ë¬¸ì œ':
                    predicted_problem = "ì—°ê²°ìŒ ë³¼ë¥¨/ì„¤ì • ë¬¸ì œë¡œ ì¸í•œ ì‚¬ìš©ì ë¶ˆí¸"
                    realistic_solution = "ì—°ê²°ìŒ ê°œì¸í™” ê¸°ëŠ¥, ë³¼ë¥¨ ì¡°ì ˆ ì˜µì…˜, ë¬´ìŒ ëª¨ë“œ ì§€ì›"
                elif most_common_issue == 'ì•Œëœ°í° ì§€ì› ë¬¸ì œ':
                    predicted_problem = "MVNO ë¯¸ì§€ì›ìœ¼ë¡œ ì¸í•œ ì‚¬ìš©ì ì ‘ê·¼ì„± ì œí•œ"
                    realistic_solution = "ì•Œëœ°í° í†µì‹ ì‚¬ ì§€ì› í™•ëŒ€, ì¸ì¦ ì‹œìŠ¤í…œ ê°œì„ , í˜¸í™˜ì„± ê²€ì¦"
                elif most_common_issue == 'íŠ¹ì • ë²ˆí˜¸ ê¸°ë¡ ëˆ„ë½ ë¬¸ì œ':
                    predicted_problem = "í†µí™” ê¸°ë¡ ëˆ„ë½ìœ¼ë¡œ ì¸í•œ ì—…ë¬´ ì¶”ì  ì–´ë ¤ì›€"
                    realistic_solution = "í†µí™” ê¸°ë¡ DB ìµœì í™”, ëª¨ë“  ë²ˆí˜¸ í˜•íƒœ ì§€ì›, ì‹¤ì‹œê°„ ê¸°ë¡ ê²€ì¦"
                elif most_common_issue == 'ë³¼ë¥¨ë²„íŠ¼ ì§„ë™ ì œì–´ ë¬¸ì œ':
                    predicted_problem = "í•˜ë“œì›¨ì–´ ë²„íŠ¼ ì œì–´ ë¬¸ì œë¡œ ì¸í•œ ì‚¬ìš©ì ì¡°ì‘ ë¶ˆí¸"
                    realistic_solution = "í•˜ë“œì›¨ì–´ ì´ë²¤íŠ¸ ì²˜ë¦¬ ê°œì„ , ì§„ë™ ì œì–´ ì˜µì…˜ ì¶”ê°€"
                elif most_common_issue == 'ë°±ê·¸ë¼ìš´ë“œ ì•± ì¢…ë£Œ ë¬¸ì œ':
                    predicted_problem = "í†µí™” ì¢…ë£Œ í›„ ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ë¯¸ì •ë¦¬ë¡œ ì¸í•œ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì ìœ "
                    realistic_solution = "í†µí™” ì¢…ë£Œ ì‹œ ë°±ê·¸ë¼ìš´ë“œ ì•± ìë™ ì¢…ë£Œ, ì˜¤ë””ì˜¤ ì„¸ì…˜ ê´€ë¦¬ ê°œì„ "
                elif most_common_issue == 'ìŠ¤íŒ¸ ì •ë³´ í‘œì‹œ ë¬¸ì œ':
                    predicted_problem = "ìŠ¤íŒ¸ ì •ë³´ ìŠ¬ë¼ì´ë“œ í‘œì‹œë¡œ ì¸í•œ ë²ˆí˜¸ í™•ì¸ ì§€ì—°"
                    realistic_solution = "ìŠ¤íŒ¸ ì •ë³´ í‘œì‹œ UI ê°œì„ , ë²ˆí˜¸ ìš°ì„  í‘œì‹œ ì˜µì…˜ ì œê³µ"
                elif most_common_issue == 'UI ìŠ¬ë¼ì´ë“œ í‘œì‹œ ë¬¸ì œ':
                    predicted_problem = "í…ìŠ¤íŠ¸ ìŠ¬ë¼ì´ë“œ ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ ì¸í•œ ì •ë³´ í™•ì¸ ì§€ì—°"
                    realistic_solution = "ìŠ¬ë¼ì´ë“œ ì†ë„ ì¡°ì ˆ, ì •ì  í‘œì‹œ ëª¨ë“œ ì˜µì…˜ ì¶”ê°€"
                elif most_common_issue == 'ì‚¬ìš©ì ê²½í—˜ í˜¼ë€':
                    predicted_problem = "ì˜ˆìƒì¹˜ ëª»í•œ ì•± ë™ì‘ìœ¼ë¡œ ì¸í•œ ì‚¬ìš©ì í˜¼ë€ ë° ìŠ¤íŠ¸ë ˆìŠ¤"
                    realistic_solution = "ì§ê´€ì ì¸ UI/UX ì¬ì„¤ê³„, ì‚¬ìš©ì ê°€ì´ë“œ ê°œì„ "
                elif most_common_issue == 'ì°¨ëŸ‰ ë¸”ë£¨íˆ¬ìŠ¤ ì—°ë™ ë¬¸ì œ':
                    predicted_problem = "ì°¨ëŸ‰ ë¸”ë£¨íˆ¬ìŠ¤ ì—°ë™ ë¶ˆì•ˆì •ìœ¼ë¡œ ì¸í•œ ìŒì„± í†µí™” í›„ ì˜¤ë””ì˜¤ ì„¸ì…˜ ë¬¸ì œ"
                    realistic_solution = "ì°¨ëŸ‰ ë¸”ë£¨íˆ¬ìŠ¤ í˜¸í™˜ì„± ê°œì„ , ì˜¤ë””ì˜¤ ì„¸ì…˜ ì •ë¦¬ ìë™í™”"
                elif most_common_issue == 'ì „í™” ìˆ˜ì‹  ë¶ˆê°€ ë¬¸ì œ':
                    predicted_problem = "ì „í™” ìˆ˜ì‹  ì‹¤íŒ¨ë¡œ ì¸í•œ ì¤‘ìš” í†µí™” ëˆ„ë½ ìœ„í—˜"
                    realistic_solution = "ìˆ˜ì‹  ì•Œê³ ë¦¬ì¦˜ ê°œì„ , ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ì²´í¬ ê°•í™”"
                else:
                    predicted_problem = "í•µì‹¬ ê¸°ëŠ¥ ì˜¤ë¥˜ë¡œ ì¸í•œ ì‘ì—… ì™„ë£Œ ë¶ˆê°€"
                    realistic_solution = "ê¸°ëŠ¥ë³„ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ê°•í™”, ì˜¤ë¥˜ ë°œìƒ ì‹œ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ êµ¬ì¶•"
                    
            elif category == 'happiness':
                if most_common_issue == 'ì‚¬ìš©ì„± ë¬¸ì œ':
                    predicted_problem = "ì§ê´€ì ì´ì§€ ì•Šì€ UI/UXë¡œ ì¸í•œ ì‚¬ìš©ì ìŠ¤íŠ¸ë ˆìŠ¤"
                    realistic_solution = "ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ ì‹¤ì‹œ, ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡° ë‹¨ìˆœí™”, ì£¼ìš” ê¸°ëŠ¥ ì ‘ê·¼ì„± ê°œì„ "
                elif most_common_issue == 'ì„±ëŠ¥ ì €í•˜':
                    predicted_problem = "ì•± ë¡œë”© ì§€ì—°ìœ¼ë¡œ ì¸í•œ ì‚¬ìš©ì ë‹µë‹µí•¨"
                    realistic_solution = "ì½”ë“œ ìµœì í™”, ì´ë¯¸ì§€ ì••ì¶•, ìºì‹± ì „ëµ ê°œì„ , ë¡œë”© ì¸ë””ì¼€ì´í„° ì¶”ê°€"
                else:
                    predicted_problem = "ì‚¬ìš©ì ê¸°ëŒ€ì™€ ì‹¤ì œ ê²½í—˜ ê°„ì˜ ê´´ë¦¬"
                    realistic_solution = "ì‚¬ìš©ì í”¼ë“œë°± ì •ê¸° ìˆ˜ì§‘, í•µì‹¬ ë¶ˆë§Œ ì‚¬í•­ ìš°ì„  í•´ê²°, UX ê°œì„  í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•"
                    
            elif category == 'engagement':
                if most_common_issue == 'ì‚¬ìš©ì„± ë¬¸ì œ':
                    predicted_problem = "ë³µì¡í•œ ê¸°ëŠ¥ êµ¬ì¡°ë¡œ ì¸í•œ ì‚¬ìš©ì ì°¸ì—¬ë„ ê°ì†Œ"
                    realistic_solution = "í•µì‹¬ ê¸°ëŠ¥ ì ‘ê·¼ì„± ê°œì„ , ê°œì¸í™” ì•Œë¦¼ ì„¤ì •, ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ê¸°ë°˜ ê¸°ëŠ¥ ì¶”ì²œ"
                else:
                    predicted_problem = "ì¬ë°©ë¬¸ ë™ê¸° ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì‚¬ìš© ë¹ˆë„ ì €í•˜"
                    realistic_solution = "í‘¸ì‹œ ì•Œë¦¼ ê°œì¸í™”, ì‚¬ìš©ìë³„ ë§ì¶¤ ì½˜í…ì¸  ì œê³µ, ì •ê¸°ì  ì—…ë°ì´íŠ¸ ë° ì´ë²¤íŠ¸ ì§„í–‰"
                    
            elif category == 'retention':
                if most_common_issue == 'ì•± í¬ë˜ì‹œ/ê°•ì œ ì¢…ë£Œ':
                    predicted_problem = "ì•± ì•ˆì •ì„± ë¬¸ì œë¡œ ì¸í•œ ì‚¬ìš©ì ì´íƒˆ"
                    realistic_solution = "í¬ë˜ì‹œ ë¡œê·¸ ë¶„ì„ ë° ë²„ê·¸ ìˆ˜ì •, ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ê°•í™”, ê¸´ê¸‰ íŒ¨ì¹˜ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•"
                elif most_common_issue == 'ë¡œê·¸ì¸/ì¸ì¦ ë¬¸ì œ':
                    predicted_problem = "ë°˜ë³µì ì¸ ë¡œê·¸ì¸ ì‹¤íŒ¨ë¡œ ì¸í•œ ì¬ë°©ë¬¸ìœ¨ ê°ì†Œ"
                    realistic_solution = "ìë™ ë¡œê·¸ì¸ ê¸°ëŠ¥ ê°œì„ , ì†Œì…œ ë¡œê·¸ì¸ ì—°ë™, ë¹„ë°€ë²ˆí˜¸ ì°¾ê¸° í”„ë¡œì„¸ìŠ¤ ê°„ì†Œí™”"
                else:
                    predicted_problem = "ì§€ì†ì ì¸ ê°€ì¹˜ ì œê³µ ì‹¤íŒ¨ë¡œ ì¸í•œ ì‚¬ìš©ì ì´íƒˆ"
                    realistic_solution = "ì‚¬ìš©ì ìƒëª…ì£¼ê¸°ë³„ ë§ì¶¤ ì„œë¹„ìŠ¤ ì œê³µ, ì¬ë°©ë¬¸ ìœ ë„ ì•Œë¦¼ ìµœì í™”"
                    
            elif category == 'adoption':
                if most_common_issue == 'ì‚¬ìš©ì„± ë¬¸ì œ':
                    predicted_problem = "ë³µì¡í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ ì¸í•œ ì‹ ê·œ ì‚¬ìš©ì ì˜¨ë³´ë”© ì´íƒˆ"
                    realistic_solution = "ì˜¨ë³´ë”© í”Œë¡œìš° ë‹¨ìˆœí™”, ë‹¨ê³„ë³„ ê°€ì´ë“œ ì œê³µ, í•„ìˆ˜ ê¸°ëŠ¥ ì¤‘ì‹¬ íŠœí† ë¦¬ì–¼ êµ¬ì„±"
                elif most_common_issue == 'ë¡œê·¸ì¸/ì¸ì¦ ë¬¸ì œ':
                    predicted_problem = "ì¸ì¦ ì˜¤ë¥˜ë¡œ ì¸í•œ ì‹ ê·œ ì‚¬ìš©ì ìœ ì… ì‹¤íŒ¨"
                    realistic_solution = "ê°„í¸ íšŒì›ê°€ì… ì˜µì…˜ ì œê³µ, ì¸ì¦ ê³¼ì • ìµœì†Œí™”, ì„¤ì • ìë™í™” ê¸°ëŠ¥ ê°•í™”"
                else:
                    predicted_problem = "í•µì‹¬ ê°€ì¹˜ ì´í•´ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì´ˆê¸° ì´íƒˆë¥  ì¦ê°€"
                    realistic_solution = "í•µì‹¬ ê¸°ëŠ¥ ìš°ì„  ë…¸ì¶œ, ì‚¬ìš©ì ìœ í˜•ë³„ ë§ì¶¤ ì˜¨ë³´ë”©, ì²« ì„±ê³µ ê²½í—˜ ë³´ì¥"
            
            # Extract actual user quotes from reviews for more authentic problem descriptions
            user_quotes = []
            for issue_text in data['issues'][:3]:  # Get first 3 issues for quotes
                # Extract meaningful quotes (first 50 chars)
                if len(issue_text) > 50:
                    quote = issue_text[:50] + "..."
                else:
                    quote = issue_text
                user_quotes.append(f'"{quote}"')
            
            quotes_text = " / ".join(user_quotes) if user_quotes else "ì‚¬ìš©ì í”¼ë“œë°± ë¶„ì„ ê²°ê³¼"
            
            # Create more detailed, UX-researcher style description
            heart_category_ko = {
                'task_success': 'í•µì‹¬ ê¸°ëŠ¥ ìˆ˜í–‰',
                'happiness': 'ì‚¬ìš©ì ë§Œì¡±ë„', 
                'engagement': 'ì‚¬ìš©ì ì°¸ì—¬ë„',
                'retention': 'ì‚¬ìš©ì ìœ ì§€ìœ¨',
                'adoption': 'ì‹ ê·œ ì‚¬ìš©ì ì ì‘'
            }
            
            # Generate specific UX improvement examples based on the category and issues
            ux_improvement_examples = generate_ux_improvement_points(category, most_common_issue, data['issues'])
            
            # Generate UX-focused improvement suggestions based on actual user review content
            ux_improvement_suggestions = generate_realistic_ux_suggestions(category, most_common_issue, data['issues'], predicted_problem, quotes_text)
            
            description = f"""**HEART í•­ëª©**: {category}
**ë¬¸ì œ ìš”ì•½**: {quotes_text}ì—ì„œ ë“œëŸ¬ë‚˜ëŠ” {predicted_problem}
**UX ê°œì„  ì œì•ˆ**: {ux_improvement_suggestions}
**ìš°ì„ ìˆœìœ„**: {priority.upper()}"""

            insights.append({
                'id': insight_id,
                'title': f"{priority_emoji} {priority.title()} | HEART: {category} | {most_common_issue} ({count}ê±´)",
                'description': description,
                'priority': priority,
                'mentionCount': count,
                'trend': 'stable',
                'category': category
            })
            insight_id += 1
    
    # Sort by priority and impact
    priority_order = {'critical': 3, 'major': 2, 'minor': 1}
    insights.sort(key=lambda x: (priority_order[x['priority']], x['mentionCount']), reverse=True)
    
    # Limit to top 5 insights
    insights = insights[:5]
    
    # Enhanced Korean word frequency analysis using advanced processing
    positive_texts = [r['content'] for r in reviews if r.get('sentiment') == 'positive']
    negative_texts = [r['content'] for r in reviews if r.get('sentiment') == 'negative']
    
    # Use advanced Korean processing to extract meaningful words
    positive_cloud = extract_korean_words_advanced(positive_texts, 'positive', 10)
    negative_cloud = extract_korean_words_advanced(negative_texts, 'negative', 10)
    
    print(f"Generated {len(insights)} HEART insights, {len(positive_cloud)} positive words, {len(negative_cloud)} negative words", file=sys.stderr)
    
    return {
        'insights': insights,
        'wordCloud': {
            'positive': positive_cloud,
            'negative': negative_cloud
        }
    }

def generate_ux_improvement_points(category, issue_type, issues):
    """
    Generate specific UX improvement examples based on HEART category and issue type
    """
    # Sample some issues for context
    sample_issues = issues[:3] if len(issues) > 3 else issues
    
    if category == 'task_success':
        if 'í†µí™”' in issue_type or 'ì „í™”' in issue_type:
            return """ğŸ“± í†µí™” í’ˆì§ˆ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ: ì‹¤ì‹œê°„ í†µí™” í’ˆì§ˆ í‘œì‹œ (ì‹ í˜¸ ê°•ë„, ì§€ì—°ì‹œê°„, ìŒì„± í’ˆì§ˆ)
ğŸ”„ ì›í„°ì¹˜ ì¬ì—°ê²° ë²„íŠ¼: í†µí™” ëŠê¹€ ì‹œ ì¦‰ì‹œ ì¬ì—°ê²° ê°€ëŠ¥í•œ í”Œë¡œíŒ… ë²„íŠ¼ ì¶”ê°€
âš ï¸ í†µí™” ì „ ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ì²´í¬: í†µí™” ì‹œì‘ ì „ ì—°ê²° í’ˆì§ˆ ë¯¸ë¦¬ ì•Œë¦¼ (ë¹¨ê°„/ë…¸ë€/ì´ˆë¡ ì•„ì´ì½˜)
ğŸšï¸ ë³¼ë¥¨ ì»¨íŠ¸ë¡¤ ê°œì„ : í†µí™” ì¤‘ ë³¼ë¥¨ ì¡°ì ˆ ì‹œ ì§„ë™/ë²¨ì†Œë¦¬ ìë™ ì •ì§€ í† ê¸€"""
        elif 'CCTV' in issue_type or 'í™”ë©´' in issue_type:
            return """ğŸ“± í•€ì¹˜ ì¤Œ ì œìŠ¤ì²˜ í™œì„±í™”: ë‘ ì†ê°€ë½ìœ¼ë¡œ í™•ëŒ€/ì¶•ì†Œ ê°€ëŠ¥í•œ ì§ê´€ì  ì¸í„°í˜ì´ìŠ¤
ğŸ–¥ï¸ ë©€í‹° ë·° ëª¨ë“œ: 4ë¶„í• /9ë¶„í•  í™”ë©´ìœ¼ë¡œ ì—¬ëŸ¬ ì¹´ë©”ë¼ ë™ì‹œ ëª¨ë‹ˆí„°ë§
ğŸ“‹ ì¦ê²¨ì°¾ê¸° ì¹´ë©”ë¼: ìì£¼ í™•ì¸í•˜ëŠ” ì¹´ë©”ë¼ë¥¼ ìƒë‹¨ì— ê³ ì • í‘œì‹œ
ğŸ”„ ìë™ ìƒˆë¡œê³ ì¹¨ ê°„ê²© ì„¤ì •: 1ì´ˆ/3ì´ˆ/5ì´ˆ ìë™ ê°±ì‹  ì˜µì…˜"""
        elif 'ì•±' in issue_type and 'íŠ•ê¹€' in issue_type:
            return """ğŸ›¡ï¸ ì•± ì•ˆì •ì„± ëª¨ë‹ˆí„°ë§: í¬ë˜ì‹œ ë°œìƒ ì‹œ ìë™ ì¬ì‹œì‘ ë° ì´ì „ ìƒíƒœ ë³µì›
ğŸ’¾ ì„¸ì…˜ ìë™ ì €ì¥: 5ì´ˆë§ˆë‹¤ ì‚¬ìš©ì ìƒíƒœ ìë™ ì €ì¥ìœ¼ë¡œ ë°ì´í„° ì†ì‹¤ ë°©ì§€
ğŸ”„ ì˜¤í”„ë¼ì¸ ëª¨ë“œ: ë„¤íŠ¸ì›Œí¬ ë¶ˆì•ˆì • ì‹œ ìºì‹œëœ ë°ì´í„°ë¡œ ê¸°ë³¸ ê¸°ëŠ¥ ìœ ì§€
âš¡ ê²½ëŸ‰ ëª¨ë“œ: ì €ì‚¬ì–‘ ê¸°ê¸°ìš© ë‹¨ìˆœí™”ëœ UI ë° ê¸°ëŠ¥ ì œê³µ"""
        else:
            return """ğŸ¯ ì‘ì—… ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸: ì‚¬ìš©ìê°€ ìˆ˜í–‰í•´ì•¼ í•  ë‹¨ê³„ë³„ ê°€ì´ë“œ ì œê³µ
ğŸ“Š ì§„í–‰ë¥  í‘œì‹œ: ì‘ì—… ì™„ë£Œë„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” í”„ë¡œê·¸ë ˆìŠ¤ ë°”
ğŸ” ì‹¤ì‹œê°„ ì˜¤ë¥˜ ê°ì§€: ë¬¸ì œ ë°œìƒ ì‹œ ì¦‰ì‹œ ì•Œë¦¼ ë° í•´ê²° ë°©ì•ˆ ì œì‹œ
âš¡ ë¹ ë¥¸ ì•¡ì„¸ìŠ¤ ë©”ë‰´: ìì£¼ ì‚¬ìš©í•˜ëŠ” ê¸°ëŠ¥ì„ í™ˆ í™”ë©´ì— ë°”ë¡œê°€ê¸° ì œê³µ"""
    
    elif category == 'happiness':
        return """ğŸ˜Š ì‚¬ìš©ì í”¼ë“œë°± ì‹¤ì‹œê°„ ë°˜ì˜: ì•± ë‚´ ê°„ë‹¨í•œ ë§Œì¡±ë„ í‰ê°€ (ğŸ‘/ğŸ‘) ë²„íŠ¼
ğŸ¨ ê°œì¸í™” í…Œë§ˆ: ì‚¬ìš©ì ì„ í˜¸ì— ë”°ë¥¸ ìƒ‰ìƒ/í°íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì˜µì…˜  
ğŸ† ì„±ì·¨ê° ì œê³µ: ê¸°ëŠ¥ ì‚¬ìš© ì‹œ ì‘ì€ ì• ë‹ˆë©”ì´ì…˜ê³¼ ì„±ê³µ ë©”ì‹œì§€ í‘œì‹œ
ğŸ“± ì‚¬ìš© ê°€ì´ë“œ íˆ´íŒ: ìƒˆë¡œìš´ ê¸°ëŠ¥ ì‚¬ìš© ì‹œ ì¹œê·¼í•œ ì•ˆë‚´ ë§í’ì„  ì œê³µ
ğŸ”” ê¸ì •ì  ì•Œë¦¼: 'ì˜¤ëŠ˜ í†µí™” í’ˆì§ˆì´ ì¢‹ì•˜ìŠµë‹ˆë‹¤' ê°™ì€ ê²©ë ¤ ë©”ì‹œì§€"""
    
    elif category == 'engagement':
        return """ğŸ“ˆ ì‚¬ìš© í†µê³„ ì‹œê°í™”: ì£¼ê°„/ì›”ê°„ ì‚¬ìš© íŒ¨í„´ì„ ì˜ˆìœ ì°¨íŠ¸ë¡œ í‘œì‹œ
ğŸ¯ ê°œì¸ ëª©í‘œ ì„¤ì •: í†µí™” ì‹œê°„, ì•± ì‚¬ìš© ë¹ˆë„ ë“± ê°œì¸ ëª©í‘œ ì„¤ì • ê¸°ëŠ¥
ğŸ”” ìŠ¤ë§ˆíŠ¸ ì•Œë¦¼: ì‚¬ìš©ì íŒ¨í„´ ê¸°ë°˜ ë§ì¶¤í˜• ì•Œë¦¼ (ì ì‹¬ì‹œê°„, í‡´ê·¼ì‹œê°„ ë“±)
ğŸ ì‚¬ìš© ë³´ìƒ: ì—°ì† ì‚¬ìš©ì¼ìˆ˜ì— ë”°ë¥¸ ì‘ì€ í˜œíƒ ì œê³µ
ğŸ“± ìœ„ì ¯ ì œê³µ: í™ˆ í™”ë©´ì—ì„œ ë°”ë¡œ í™•ì¸ ê°€ëŠ¥í•œ ê°„ë‹¨í•œ ì •ë³´ í‘œì‹œ"""
    
    elif category == 'retention':
        return """ğŸ”„ ì‚¬ìš© ì´ë ¥ ë°±ì—…: í´ë¼ìš°ë“œ ë™ê¸°í™”ë¡œ ê¸°ê¸° ë³€ê²½ ì‹œì—ë„ ì„¤ì • ìœ ì§€
ğŸ“Š ê°œì¸í™” ëŒ€ì‹œë³´ë“œ: ì‚¬ìš©ìë³„ ë§ì¶¤ ì •ë³´ ë°°ì¹˜ ë° ìì£¼ ì“°ëŠ” ê¸°ëŠ¥ ìš°ì„  í‘œì‹œ
ğŸ¯ ë‹¨ê³„ë³„ ì˜¨ë³´ë”©: ì‹ ê·œ ì‚¬ìš©ìë¥¼ ìœ„í•œ ì¹œê·¼í•œ 3ë‹¨ê³„ ê°€ì´ë“œ íˆ¬ì–´
âš¡ ë¹ ë¥¸ ë³µêµ¬: ì•± ì‚­ì œ í›„ ì¬ì„¤ì¹˜ ì‹œ ê¸°ì¡´ ì„¤ì • 1ì´ˆ ë³µì› ê¸°ëŠ¥
ğŸ”” ì¬ë°©ë¬¸ ìœ ë„: ë©°ì¹  ì‚¬ìš©í•˜ì§€ ì•Šì„ ì‹œ ìœ ìš©í•œ ê¸°ëŠ¥ ì†Œê°œ ì•Œë¦¼"""
    
    elif category == 'adoption':
        return """ğŸš€ 3ë¶„ í€µ ìŠ¤íƒ€íŠ¸: í•µì‹¬ ê¸°ëŠ¥ 3ê°œë§Œ ì²´í—˜í•´ë³´ëŠ” ê°„ë‹¨í•œ íŠœí† ë¦¬ì–¼
ğŸ“± ë¬´ë£Œ ì²´í—˜: í”„ë¦¬ë¯¸ì—„ ê¸°ëŠ¥ 7ì¼ ë¬´ë£Œ ì²´í—˜ í›„ í•„ìš”ì‹œ ì—…ê·¸ë ˆì´ë“œ
ğŸ¯ ëª©ì ë³„ ì„¤ì •: 'í†µí™”ìš©', 'CCTVìš©', 'ì¢…í•©ê´€ë¦¬ìš©' ë“± ì‚¬ìš© ëª©ì ì— ë”°ë¥¸ ì´ˆê¸° ì„¤ì •
ğŸ“ ì‹¤ì‹œê°„ í—¬í”„: ë§‰íˆëŠ” ë¶€ë¶„ ìˆì„ ë•Œ ì±„íŒ…ìœ¼ë¡œ ì¦‰ì‹œ ë„ì›€ ë°›ê¸°
ğŸƒ ì›í´ë¦­ ì‹œì‘: ë³µì¡í•œ ì„¤ì • ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ 'ë¹ ë¥¸ ì‹œì‘' ëª¨ë“œ"""
    
    else:
        return """ğŸ¯ ì‚¬ìš©ì ì¤‘ì‹¬ ê°œì„ : ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ê¸°ë°˜ UI/UX ìµœì í™”
ğŸ“± ì ‘ê·¼ì„± í–¥ìƒ: í° ë²„íŠ¼, ëª…í™•í•œ ë¼ë²¨, ì§ê´€ì ì¸ ì•„ì´ì½˜ ì‚¬ìš©
ğŸ”„ í”¼ë“œë°± ë£¨í”„: ì‚¬ìš©ì ì˜ê²¬ ìˆ˜ì§‘ â†’ ê°œì„  â†’ ê²°ê³¼ ê³µìœ  ìˆœí™˜ êµ¬ì¡°
âš¡ ì„±ëŠ¥ ìµœì í™”: ë¡œë”© ì‹œê°„ ë‹¨ì¶• ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°œì„ """

def generate_technical_implementation(category, issue_type, issues, problem_description):
    """
    Generate specific technical implementation based on actual user issues
    """
    # Sample issues for context
    sample_issues = issues[:5] if len(issues) > 5 else issues
    
    if category == 'task_success':
        if 'í†µí™”' in issue_type or 'ì „í™”' in issue_type:
            return """ğŸ”§ í†µí™” ì—°ê²° ì‹¤íŒ¨ ì¬í˜„: ë„¤íŠ¸ì›Œí¬ ìƒíƒœë³„ í†µí™” ì‹œë„ ì¼€ì´ìŠ¤ 100ê°œ í…ŒìŠ¤íŠ¸
ğŸ“Š VoIP ì„œë²„ ëª¨ë‹ˆí„°ë§: ì—°ê²° ì„±ê³µë¥ , ì‘ë‹µ ì‹œê°„, íŒ¨í‚· ì†ì‹¤ë¥  ì‹¤ì‹œê°„ íŠ¸ë˜í‚¹
ğŸ” í†µí™” í’ˆì§ˆ ë¡œê¹…: ìŒì„± ì½”ë±, ì§€ì—°ì‹œê°„, ì—ì½” ì œê±° ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
âš¡ ìë™ ì¬ì—°ê²° ì•Œê³ ë¦¬ì¦˜: ì—°ê²° ì‹¤íŒ¨ ì‹œ 3ì´ˆ/10ì´ˆ/30ì´ˆ ê°„ê²©ìœ¼ë¡œ ì¬ì‹œë„ ë¡œì§ êµ¬í˜„"""
        elif 'CCTV' in issue_type or 'í™”ë©´' in issue_type:
            return """ğŸ”§ í™”ë©´ í™•ëŒ€ ì œìŠ¤ì²˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì ìš©: PinchGestureRecognizer êµ¬í˜„
ğŸ“Š ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”: í•´ìƒë„ë³„ ì••ì¶•ë¥  ì¡°ì • ë° ë²„í¼ë§ ê°œì„ 
ğŸ” ë©€í‹° ì¹´ë©”ë¼ ë©”ëª¨ë¦¬ ê´€ë¦¬: ë¹„í™œì„± ë·° ë¦¬ì†ŒìŠ¤ í•´ì œ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”
âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ìºì‹±: 3ì´ˆ ë°± ë²„í¼ë§ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ëŠê¹€ ì‹œ ë¬´ì¤‘ë‹¨ ì¬ìƒ"""
        elif 'ì•±' in issue_type and ('íŠ•ê¹€' in issue_type or 'ë‚˜ê°€ë²„ë¦¼' in issue_type):
            return """ğŸ”§ í¬ë˜ì‹œ ì¬í˜„ ì‹œë‚˜ë¦¬ì˜¤ ì‘ì„±: ì‚¬ìš©ì ì„ íƒ ë‹¨ê³„ë³„ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì¶”ì 
ğŸ“Š ì‹¤ì‹œê°„ í¬ë˜ì‹œ ë¦¬í¬íŒ…: Firebase Crashlytics ë„ì…ìœ¼ë¡œ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ìë™ ìˆ˜ì§‘
ğŸ” ì•± ìƒíƒœ ë³µì› ì‹œìŠ¤í…œ: SharedPreferences/UserDefaults í™œìš© ì„¸ì…˜ ìë™ ì €ì¥
âš¡ ë©”ëª¨ë¦¬ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€: ì´ë¯¸ì§€ ë¡œë”© ì‹œ ë©”ëª¨ë¦¬ í’€ ê´€ë¦¬ ë° ë¹„ë™ê¸° ì²˜ë¦¬"""
        elif 'ë¡œê·¸ì¸' in issue_type or 'ì¸ì¦' in issue_type:
            return """ğŸ”§ ì¸ì¦ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„: ê¸°ê¸°ë³„, OSë³„ ë¡œê·¸ì¸ ì‹œë„ ë¡œê·¸ ìˆ˜ì§‘
ğŸ“Š í† í° ê°±ì‹  ë¡œì§ ê°œì„ : JWT ë§Œë£Œ 10ë¶„ ì „ ìë™ ê°±ì‹  êµ¬í˜„
ğŸ” OAuth ì—°ë™ ë””ë²„ê¹…: ì œ3ì ì¸ì¦ ì‘ë‹µ ì‹œê°„ ë° ì—ëŸ¬ ì½”ë“œ ë¶„ì„
âš¡ ì˜¤í”„ë¼ì¸ ì¸ì¦ ìºì‹±: ë§ˆì§€ë§‰ ì„±ê³µ ì¸ì¦ ì •ë³´ ì•”í˜¸í™” ì €ì¥"""
        else:
            return """ğŸ”§ í•µì‹¬ ê¸°ëŠ¥ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: ì£¼ìš” ì›Œí¬í”Œë¡œìš° ìë™í™” í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 100ê°œ ì‘ì„±
ğŸ“Š ì„±ëŠ¥ ì§€í‘œ ëª¨ë‹ˆí„°ë§: ì‘ë‹µ ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, CPU ì ìœ ìœ¨ ì‹¤ì‹œê°„ ì¶”ì 
ğŸ” ì—ëŸ¬ íŠ¸ë˜í‚¹ ì‹œìŠ¤í…œ: Sentry ë„ì…ìœ¼ë¡œ ì‹¤ì‹œê°„ ë²„ê·¸ ë¦¬í¬íŒ… ë° ì•Œë¦¼
âš¡ ê¸°ëŠ¥ë³„ ë¡¤ë°± ì‹œìŠ¤í…œ: ë¬¸ì œ ë°œìƒ ì‹œ ì´ì „ ì•ˆì • ë²„ì „ìœ¼ë¡œ ì¦‰ì‹œ ë³µêµ¬"""
    
    elif category == 'happiness':
        return """ğŸ”§ ì‚¬ìš©ì ë§Œì¡±ë„ ì¸¡ì •: ì•± ë‚´ NPS ì ìˆ˜ ìˆ˜ì§‘ ë° í”¼ë“œë°± ë¶„ì„ ì‹œìŠ¤í…œ
ğŸ“Š ê°ì • ë¶„ì„ API: ë¦¬ë·° í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ìœ¼ë¡œ ë¶ˆë§Œ í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ
ğŸ” ì‚¬ìš©ì í–‰ë™ ë¶„ì„: íˆíŠ¸ë§µ íˆ´ ë„ì…ìœ¼ë¡œ UI ì‚¬ìš© íŒ¨í„´ ì‹œê°í™”
âš¡ ê°œì¸í™” ì•Œê³ ë¦¬ì¦˜: ì‚¬ìš© íŒ¨í„´ ê¸°ë°˜ ë§ì¶¤í˜• UI ë°°ì¹˜ ë° ê¸°ëŠ¥ ì¶”ì²œ"""
    
    elif category == 'engagement':
        return """ğŸ”§ ì‚¬ìš©ì í™œë™ ë°ì´í„° ìˆ˜ì§‘: ì„¸ì…˜ ê¸¸ì´, ê¸°ëŠ¥ ì‚¬ìš© ë¹ˆë„, ì´íƒˆ ì§€ì  ë¶„ì„
ğŸ“Š í‘¸ì‹œ ì•Œë¦¼ ìµœì í™”: A/B í…ŒìŠ¤íŠ¸ë¡œ ìµœì  ë°œì†¡ ì‹œê°„ ë° ë©”ì‹œì§€ ê°œì„ 
ğŸ” ì‚¬ìš©ì ì—¬ì • ë§¤í•‘: ì£¼ìš” ê¸°ëŠ¥ë³„ ì‚¬ìš©ì í”Œë¡œìš° ë¶„ì„ ë° ë³‘ëª© ì§€ì  ì‹ë³„
âš¡ ê²Œì„í™” ìš”ì†Œ êµ¬í˜„: ì‚¬ìš© ëª©í‘œ ë‹¬ì„± ì‹œ í¬ì¸íŠ¸ ì§€ê¸‰ ë° ë°°ì§€ ì‹œìŠ¤í…œ"""
    
    elif category == 'retention':
        return """ğŸ”§ ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸: ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ ê¸°ë°˜ ì´íƒˆ ê°€ëŠ¥ì„± ìŠ¤ì½”ì–´ë§
ğŸ“Š ì¬ë°©ë¬¸ ìœ ë„ ì‹œìŠ¤í…œ: ë¹„í™œì„± ì‚¬ìš©ì ëŒ€ìƒ ë§ì¶¤í˜• ì´ë©”ì¼/SMS ìº í˜ì¸
ğŸ” ì½”í˜¸íŠ¸ ë¶„ì„ êµ¬ì¶•: ê°€ì… ì‹œì ë³„ ì‚¬ìš©ì ê·¸ë£¹ ìƒì¡´ ë¶„ì„ ë° ê°œì„ ì  ë„ì¶œ
âš¡ ê³„ì • ì—°ë™ ê°•í™”: ì†Œì…œ ë¡œê·¸ì¸, í´ë¼ìš°ë“œ ë°±ì—…ìœ¼ë¡œ ê¸°ê¸° ë³€ê²½ ì‹œ ë°ì´í„° ìœ ì§€"""
    
    elif category == 'adoption':
        return """ğŸ”§ ì˜¨ë³´ë”© í”Œë¡œìš° ê°œì„ : ì‹ ê·œ ì‚¬ìš©ì ì²« 7ì¼ê°„ ì‚¬ìš© íŒ¨í„´ ë¶„ì„
ğŸ“Š í¼ë„ ë¶„ì„ ì‹œìŠ¤í…œ: ê°€ì…ë¶€í„° ì²« ì„±ê³µ ê²½í—˜ê¹Œì§€ ë‹¨ê³„ë³„ ì´íƒˆë¥  ì¸¡ì •
ğŸ” ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„: ì‚¬ìš© ëª©ì ë³„ ë§ì¶¤í˜• ì´ˆê¸° ì„¤ì • ì›Œí¬í”Œë¡œìš° ê°œë°œ
âš¡ í”„ë¡œê·¸ë ˆì‹œë¸Œ ë””ìŠ¤í´ë¡œì €: ë³µì¡í•œ ê¸°ëŠ¥ì„ ë‹¨ê³„ë³„ë¡œ ì ì§„ì  ë…¸ì¶œ"""
    
    else:
        return """ğŸ”§ ì¢…í•© í’ˆì§ˆ ê´€ë¦¬: ìë™í™”ëœ íšŒê·€ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ
ğŸ“Š ì‚¬ìš©ì í”¼ë“œë°± ë¶„ì„: ë¦¬ë·° í‚¤ì›Œë“œ ë¶„ì„ ë° ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ê°œë°œ ë¡œë“œë§µ ìˆ˜ë¦½
ğŸ” í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„±: iOS/Android/Web ì¼ê´€ëœ ì‚¬ìš©ì ê²½í—˜ ë³´ì¥
âš¡ ì§€ì†ì  ê°œì„  í”„ë¡œì„¸ìŠ¤: ì£¼ê°„ ì‚¬ìš©ì ë°ì´í„° ë¦¬ë·° ë° ë¹ ë¥¸ ê°œì„  ì‚¬ì´í´ êµ¬ì¶•"""

def generate_ux_improvement_suggestions(category, issue_type, issues, problem_description, quotes_text):
    """
    Generate UX-focused improvement suggestions based on actual user review quotes and issues
    """
    if category == 'task_success':
        if 'í†µí™”' in issue_type or 'ì „í™”' in issue_type:
            return """- í†µí™” ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ 'ë‹¤ì‹œ ì‹œë„í•˜ê¸°', 'ë‹¤ë¥¸ ë²ˆí˜¸ë¡œ ê±¸ê¸°', 'ë¬¸ì ë³´ë‚´ê¸°' ë²„íŠ¼ì´ í¬í•¨ëœ ì˜µì…˜ í™”ë©´ ì œê³µ
- í†µí™” í’ˆì§ˆì´ ì¢‹ì§€ ì•Šì„ ë•Œ í™”ë©´ í•˜ë‹¨ì— 'ìŒì§ˆ ê°œì„ ' í† ê¸€ ë²„íŠ¼ ë°°ì¹˜í•˜ì—¬ ì‚¬ìš©ìê°€ ì§ì ‘ ì¡°ì • ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„
- í†µí™” ì—°ê²° ì¤‘ ë¡œë”© í™”ë©´ì— 'ì—°ê²° ì¤‘ì…ë‹ˆë‹¤' ë©”ì‹œì§€ì™€ í•¨ê»˜ ì˜ˆìƒ ëŒ€ê¸° ì‹œê°„ í‘œì‹œ
- í†µí™” ì‹¤íŒ¨ ë°˜ë³µ ì‹œ 'ë„¤íŠ¸ì›Œí¬ ìƒíƒœ í™•ì¸' ê°€ì´ë“œ íŒì—…ê³¼ í•¨ê»˜ ê³ ê°ì„¼í„° ì§ì ‘ ì—°ê²° ë²„íŠ¼ ì œê³µ"""
        
        elif 'CCTV' in issue_type or 'í™”ë©´' in issue_type:
            return """- CCTV í™”ë©´ í™•ëŒ€ ì•ˆ ë  ë•Œ í™”ë©´ ìƒë‹¨ì— 'í™•ëŒ€/ì¶•ì†Œ ë„ì›€ë§' ì•„ì´ì½˜ ìƒì‹œ í‘œì‹œí•˜ì—¬ ì œìŠ¤ì²˜ ê°€ì´ë“œ ì œê³µ
- ì˜ìƒ ëŠê¹€ ë°œìƒ ì‹œ í™”ë©´ ì¤‘ì•™ì— 'ì¬ì—°ê²° ì¤‘' ìƒíƒœë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•˜ê³ , ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ ë°°ì¹˜
- ì—¬ëŸ¬ ì¹´ë©”ë¼ ë™ì‹œ ë³´ê¸° ì‹œ ê° í™”ë©´ì— 'ì „ì²´í™”ë©´' ë²„íŠ¼ì„ ê°œë³„ ë°°ì¹˜í•˜ì—¬ ì›í•˜ëŠ” í™”ë©´ë§Œ í¬ê²Œ ë³´ê¸° ê°€ëŠ¥
- ì˜ìƒ ë¡œë”© ì§€ì—° ì‹œ 'ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”' ë©”ì‹œì§€ì™€ í•¨ê»˜ ì˜ˆìƒ ë¡œë”© ì‹œê°„ í”„ë¡œê·¸ë ˆìŠ¤ ë°” í‘œì‹œ"""
        
        elif 'ì•±' in issue_type and ('íŠ•ê¹€' in issue_type or 'ë‚˜ê°€ë²„ë¦¼' in issue_type):
            return """- ì•± ì§„ì… ì‹œ ì‚¬ìš©ì ì„¤ì •/ì„ íƒ í™”ë©´ì„ ë‹¨ê³„ì ìœ¼ë¡œ ë¡œë”©í•˜ë„ë¡ ê°œì„ í•˜ì—¬ ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„±ì„ ì¤„ì´ê³ , ì˜¤ë¥˜ ë°œìƒ ì‹œì—ëŠ” 'ë‹¤ì‹œ ì‹œë„í•˜ê¸°' ë˜ëŠ” 'ê³ ê°ì„¼í„° ì—°ê²°' ì˜µì…˜ì´ ìˆëŠ” Fallback í™”ë©´ ì œê³µ
- ì²« ì‹¤í–‰ ì‹œ ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ê³¼ ì§„í–‰ ìƒíƒœë¥¼ ì‹œê°ì ìœ¼ë¡œ ì•ˆë‚´í•´, ì•±ì´ ë©ˆì¶˜ ë“¯í•œ ì¸ìƒì„ ì£¼ì§€ ì•Šë„ë¡ ì„¤ê³„
- ë™ì¼í•œ ë¬¸ì œ ë°œìƒ ì‹œ ì‚¬ìš©ìì—ê²Œ ë¹„ì¹¨í•´ì  íŒì—…ì„ í†µí•´ ëŒ€ì‘ ë°©ë²• ì•ˆë‚´ (ì˜ˆ: "ì¼ì‹œì ì¸ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ê³ ê°ì„¼í„° ë¬¸ì˜ ë˜ëŠ” ì¬ì‹œë„ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤")
- í¬ë˜ì‹œ ë°œìƒ ì „ ë§ˆì§€ë§‰ í™”ë©´ì„ ì„ì‹œ ì €ì¥í•˜ì—¬ ì¬ì‹¤í–‰ ì‹œ ì´ì „ ìƒíƒœë¡œ ë³µì› ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„"""
        
        elif 'ë¡œê·¸ì¸' in issue_type or 'ì¸ì¦' in issue_type:
            return """- ë¡œê·¸ì¸ ì‹¤íŒ¨ ì‹œ 'ì•„ì´ë”” ì°¾ê¸°', 'ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •', 'ê³ ê°ì„¼í„° ë¬¸ì˜' ë²„íŠ¼ì„ í•œ í™”ë©´ì— ëª…í™•íˆ ë°°ì¹˜
- ì¸ì¦ ì˜¤ë¥˜ ë°œìƒ ì‹œ êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ì›ì¸ê³¼ í•´ê²° ë°©ë²•ì„ ì¹œê·¼í•œ ë§íˆ¬ë¡œ ì•ˆë‚´ (ì˜ˆ: "íœ´ëŒ€í° ë²ˆí˜¸ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”")
- ë¡œê·¸ì¸ í™”ë©´ì— 'ê°„í¸ ë¡œê·¸ì¸' ì˜µì…˜ ì¶”ê°€í•˜ì—¬ ìƒì²´ ì¸ì¦, íŒ¨í„´ ì¸ì¦ ë“± ëŒ€ì•ˆ ì œê³µ
- ë°˜ë³µ ë¡œê·¸ì¸ ì‹¤íŒ¨ ì‹œ 'ë¡œê·¸ì¸ ë„ì›€ë§' í™”ë©´ìœ¼ë¡œ ìë™ ì´ë™í•˜ì—¬ ë‹¨ê³„ë³„ í•´ê²° ê°€ì´ë“œ ì œê³µ"""
        
        else:
            return """- í•µì‹¬ ê¸°ëŠ¥ ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¦‰ì‹œ 'ë¬¸ì œ ì‹ ê³ í•˜ê¸°' ë²„íŠ¼ê³¼ í•¨ê»˜ ì„ì‹œ í•´ê²° ë°©ë²• ì•ˆë‚´
- ê¸°ëŠ¥ ì‹¤í–‰ ì „ ë¡œë”© ì‹œê°„ì´ ì˜ˆìƒë  ë•Œ ì§„í–‰ ìƒí™©ì„ %ë¡œ í‘œì‹œí•˜ê³  'ì·¨ì†Œ' ë²„íŠ¼ ì œê³µ
- ìì£¼ ì‚¬ìš©í•˜ëŠ” ê¸°ëŠ¥ì„ í™ˆ í™”ë©´ ìƒë‹¨ì— ë°”ë¡œê°€ê¸°ë¡œ ë°°ì¹˜í•˜ì—¬ ì ‘ê·¼ì„± í–¥ìƒ
- ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‚¬ìš©ì ì¹œí™”ì ì¸ ë©”ì‹œì§€ë¡œ ìƒí™© ì„¤ëª… ë° ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´"""
    
    elif category == 'happiness':
        return """- ì‚¬ìš©ì ë¶ˆë§Œ í‘œí˜„ ì‹œ ì•± ë‚´ 'ì˜ê²¬ ë³´ë‚´ê¸°' ê¸°ëŠ¥ì„ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë©”ë‰´ ìƒë‹¨ì— ë°°ì¹˜
- ê¸ì •ì  í”¼ë“œë°± ì‹œ 'ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ ë³„ì  ë‚¨ê¸°ê¸°' ë“±ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ìœ ë„ ë©”ì‹œì§€ í‘œì‹œ
- ì‚¬ìš©ì ë§Œì¡±ë„ ì¡°ì‚¬ë¥¼ íŒì—…ì´ ì•„ë‹Œ ì•± ì‚¬ìš© í”Œë¡œìš°ì— ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©
- ë¬¸ì œ í•´ê²° í›„ 'í•´ê²°ë˜ì—ˆë‚˜ìš”?' í™•ì¸ ë©”ì‹œì§€ë¡œ ì‚¬ìš©ì ë§Œì¡±ë„ í™•ì¸"""
    
    elif category == 'engagement':
        return """- ì‚¬ìš©ìê°€ íŠ¹ì • ê¸°ëŠ¥ì„ ìì£¼ ì‚¬ìš©í•  ë•Œ ê´€ë ¨ ê¸°ëŠ¥ ì¶”ì²œ ë©”ì‹œì§€ë¥¼ ì ì ˆí•œ íƒ€ì´ë°ì— í‘œì‹œ
- ì•± ì‚¬ìš© íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìë³„ ë§ì¶¤í˜• í™ˆ í™”ë©´ êµ¬ì„± ì œì•ˆ
- ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶œì‹œ ì‹œ ê¸°ì¡´ ì‚¬ìš© íŒ¨í„´ê³¼ ì—°ê²°í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì†Œê°œ
- ì‚¬ìš©ì í™œë™ì´ ì¤„ì–´ë“¤ ë•Œ 'ë†“ì¹œ ê¸°ëŠ¥' ì•Œë¦¼ìœ¼ë¡œ ì¬ì°¸ì—¬ ìœ ë„"""
    
    elif category == 'retention':
        return """- ì‚¬ìš©ìê°€ ì•±ì„ ì‚­ì œí•˜ë ¤ í•  ë•Œ 'ì ê¹, ë¬¸ì œê°€ ìˆìœ¼ì‹ ê°€ìš”?' íŒì—…ìœ¼ë¡œ ì´íƒˆ ì‚¬ìœ  íŒŒì•… ë° ì¦‰ì‹œ í•´ê²° ì‹œë„
- ì¥ê¸°ê°„ ë¯¸ì‚¬ìš© ì‹œ 'ìƒˆë¡œìš´ ê¸°ëŠ¥ ì—…ë°ì´íŠ¸' ì•Œë¦¼ë³´ë‹¤ëŠ” 'ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚¬ìš©í•˜ì…¨ë˜ ê¸°ëŠ¥' ì¤‘ì‹¬ìœ¼ë¡œ ë³µê·€ ìœ ë„
- ê³„ì • ì‚­ì œ ì „ 'ë°ì´í„° ë°±ì—…' ì˜µì…˜ ì œê³µí•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥ì„± ì—´ì–´ë‘ê¸°
- ì‚¬ìš©ìë³„ ì´ìš© íŒ¨í„´ ê¸°ë°˜ ë§ì¶¤í˜• 'ë‹¤ì‹œ ì‹œì‘í•˜ê¸°' ê°€ì´ë“œ ì œê³µ"""
    
    elif category == 'adoption':
        return """- ì‹ ê·œ ì‚¬ìš©ì ì˜¨ë³´ë”© ì‹œ '3ë¶„ ë§Œì— ì‹œì‘í•˜ê¸°' ë“± ëª…í™•í•œ ì‹œê°„ ì˜ˆìƒì¹˜ ì œì‹œ
- ë³µì¡í•œ ì´ˆê¸° ì„¤ì •ì„ 'ë‚˜ì¤‘ì— í•˜ê¸°' ì˜µì…˜ê³¼ í•¨ê»˜ ì œê³µí•˜ì—¬ ì§„ì… ì¥ë²½ ì™„í™”
- ì²« ì„±ê³µ ê²½í—˜ í›„ 'ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´' ë©”ì‹œì§€ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ê¸°ëŠ¥ í™•ì¥ ìœ ë„
- ì‚¬ìš© ëª©ì ë³„ 'ë¹ ë¥¸ ì‹œì‘' í…œí”Œë¦¿ ì œê³µ (ì˜ˆ: 'CCTVë§Œ ì‚¬ìš©', 'í†µí™” ê¸°ëŠ¥ ì¤‘ì‹¬' ë“±)"""
    
    else:
        return """- ì‚¬ìš©ì ë¦¬ë·°ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì  ë¬¸ì œì ì„ í•´ê²°í•˜ëŠ” ë‹¨ê³„ë³„ ê°€ì´ë“œ ì œê³µ
- ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì— ëŒ€í•œ 'ìì£¼ ë¬»ëŠ” ì§ˆë¬¸' ì„¹ì…˜ì„ ì•± ë‚´ ì‰½ê²Œ ì ‘ê·¼ ê°€ëŠ¥í•œ ìœ„ì¹˜ì— ë°°ì¹˜
- ì‚¬ìš©ì í”¼ë“œë°±ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ë¹ ë¥¸ ê°œì„  ì‚¬í•­ì„ ì•± ë‚´ ê³µì§€ë¡œ íˆ¬ëª…í•˜ê²Œ ê³µìœ 
- ê° ê¸°ëŠ¥ë³„ 'ë„ì›€ë§' ë²„íŠ¼ì„ ìƒí™©ì— ë§ê²Œ ë°°ì¹˜í•˜ì—¬ ì¦‰ì‹œ ë„ì›€ ë°›ì„ ìˆ˜ ìˆë„ë¡ ì„¤ê³„"""

def generate_realistic_ux_suggestions(category, issue_type, issues, problem_description, quotes_text):
    """
    Generate UX improvement suggestions based on actual user review content and specific problems
    Focus on interface experience improvements, user flow optimization, and concrete UX solutions
    """
    # Analyze specific user expressions and problems from quotes
    specific_suggestions = []
    
    # Analyze actual user quotes to identify specific UX problems
    if 'í†µí™”ì¤‘ ëŒ€ê¸°ê°€ ë˜ì§€ ì•Šì•„ì„œ ë¶ˆí¸í•˜ë„¤ìš”' in quotes_text:
        specific_suggestions.extend([
            "í†µí™” ì¤‘ í™”ë©´ í•˜ë‹¨ì— 'ëŒ€ê¸°' ë²„íŠ¼ì„ ì¶”ê°€í•˜ì—¬ í˜„ì¬ í†µí™”ë¥¼ ì¼ì‹œì •ì§€í•˜ê³  ë‹¤ë¥¸ ì „í™”ë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ ì œê³µ",
            "ëŒ€ê¸° ìƒíƒœ ì§„ì… ì‹œ 'í†µí™” ëŒ€ê¸° ì¤‘' í‘œì‹œì™€ í•¨ê»˜ 'ëŒ€ê¸° í•´ì œ' ë²„íŠ¼ì„ í™”ë©´ ì¤‘ì•™ì— ë°°ì¹˜í•˜ì—¬ ì§ê´€ì  ì¡°ì‘ ê°€ëŠ¥"
        ])
    
    if 'ë³¼ë¥¨ë²„íŠ¼ ëˆ„ë¥´ë©´ ì§„ë™ì´ êº¼ì§€ë©´ ì¢‹ê² ë„¤ìš”' in quotes_text and 'ë‹¹í™©ìŠ¤ëŸ¬ìš´ ê²½í—˜' in quotes_text:
        specific_suggestions.extend([
            "í†µí™” ìˆ˜ì‹  ì‹œ ë³¼ë¥¨ë²„íŠ¼ í„°ì¹˜ ì˜ì—­ì„ í™”ë©´ì— ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•˜ì—¬ 'ë³¼ë¥¨ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ë¬´ìŒ ëª¨ë“œ'ì„ì„ ë¯¸ë¦¬ ì•ˆë‚´",
            "ë³¼ë¥¨ ë²„íŠ¼ í„°ì¹˜ ì‹œ ì¦‰ì‹œ ì§„ë™ ì¤‘ë‹¨ê³¼ í•¨ê»˜ 'ë¬´ìŒ ëª¨ë“œë¡œ ì „í™˜ë¨' í”¼ë“œë°± ë©”ì‹œì§€ë¥¼ í™”ë©´ ìƒë‹¨ì— ì§§ê²Œ í‘œì‹œ"
        ])
    
    if 'í†µí™”ì—°ê²°ìŒì¢€ ë°”ê¿‰ì‹œë‹¤ ì‹œë„ëŸ¬ì›Œì£½ê² ìŠµë‹ˆë‹¤' in quotes_text:
        specific_suggestions.extend([
            "ì„¤ì • ë©”ë‰´ ì²« ë²ˆì§¸ í•­ëª©ì— 'í†µí™”ìŒ ì„¤ì •' ë°°ì¹˜í•˜ê³  ë³¼ë¥¨ ì¡°ì ˆ ìŠ¬ë¼ì´ë”ì™€ í•¨ê»˜ 'ë¬´ìŒ', 'ì§„ë™', 'ë²¨ì†Œë¦¬' ì˜µì…˜ì„ í•œ í™”ë©´ì— í‘œì‹œ",
            "í†µí™” ì—°ê²°ìŒ ë³€ê²½ ì‹œ ì¦‰ì‹œ ë¯¸ë¦¬ë“£ê¸° ê¸°ëŠ¥ê³¼ í•¨ê»˜ 'ì´ ì†Œë¦¬ë¡œ ì„¤ì •í•˜ì‹œê² ì–´ìš”?' í™•ì¸ íŒì—… ì œê³µ"
        ])
    
    if 'í™”ë©´ í™•ëŒ€ ì•ˆë˜ëŠ” ê²ƒ ì¢€ ì–´ë–»ê²Œ í•´ì£¼ì„¸ìš” ë‹µë‹µí•˜ë„¤ìš”' in quotes_text:
        specific_suggestions.extend([
            "CCTV í™”ë©´ ìš°ì¸¡ í•˜ë‹¨ì— ë‹ë³´ê¸° ì•„ì´ì½˜(+/-) ë²„íŠ¼ì„ ê³ ì • ë°°ì¹˜í•˜ì—¬ í•€ì¹˜ ì œìŠ¤ì²˜ê°€ ì–´ë ¤ìš´ ì‚¬ìš©ìë„ ì‰½ê²Œ í™•ëŒ€/ì¶•ì†Œ ê°€ëŠ¥",
            "í™”ë©´ í™•ëŒ€ ì‹¤íŒ¨ ì‹œ 'í™•ëŒ€ê°€ ì•ˆ ë˜ì‹œë‚˜ìš”? ì•„ë˜ + ë²„íŠ¼ì„ ëˆŒëŸ¬ë³´ì„¸ìš”' ë§í’ì„  ì•ˆë‚´ë¥¼ í™”ë©´ ì¤‘ì•™ì— 3ì´ˆê°„ í‘œì‹œ"
        ])
    
    if 'ì¸ì¦ì—ëŸ¬ë¡œ ì‚¬ìš©ë¶ˆê°€í•©ë‹ˆë‹¤' in quotes_text:
        specific_suggestions.extend([
            "íŠ¹ì • ê¸°ì¢… ì¸ì¦ ì˜¤ë¥˜ ë°œìƒ ì‹œ 'ì´ ê¸°ì¢…ì—ì„œ ì¼ì‹œì  ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤' ì•ˆë‚´ì™€ í•¨ê»˜ 'ì„ì‹œ ì ‘ì† ë°©ë²•' ê°€ì´ë“œë¥¼ ë‹¨ê³„ë³„ë¡œ ì œê³µ",
            "ì¸ì¦ ì¬ì‹œë„ ì‹œ 'ë‹¤ì‹œ ì¸ì¦ ì¤‘ì…ë‹ˆë‹¤...' ì§„í–‰ë¥  ë°”ì™€ í•¨ê»˜ ì˜ˆìƒ ì†Œìš”ì‹œê°„ 'ì•½ 30ì´ˆ' í‘œì‹œë¡œ ëŒ€ê¸° ë¶ˆì•ˆê° í•´ì†Œ"
        ])
    
    if 'ì•± ì—´ë©´ ê·¸ëƒ¥ ë‚˜ê°€ë²„ë¦¼' in quotes_text or 'ë‚˜ê°€ë²„ë¦¼' in quotes_text:
        specific_suggestions.extend([
            "ì•± ì²« ì‹¤í–‰ ì‹œ ë¡œë”© í™”ë©´ì— 'ì•±ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤' ë©”ì‹œì§€ì™€ í•¨ê»˜ ê°„ë‹¨í•œ ì§„í–‰ë¥  í‘œì‹œë¡œ ì•±ì´ ë©ˆì¶˜ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ ì•Šë„ë¡ ì„¤ê³„",
            "ì•± í¬ë˜ì‹œ í›„ ì¬ì‹¤í–‰ ì‹œ 'ì´ì „ í™”ë©´ì—ì„œ ë‹¤ì‹œ ì‹œì‘í•˜ì‹œê² ì–´ìš”?' ì˜µì…˜ìœ¼ë¡œ ë§ˆì§€ë§‰ ì‚¬ìš© ìœ„ì¹˜ë¡œ ë°”ë¡œ ì´ë™ ê°€ëŠ¥"
        ])
    
    if 'í•´ì§€í•˜ê³ ì‹¶ë„¤ìš”' in quotes_text or 'ì‚­ì œ' in quotes_text:
        specific_suggestions.extend([
            "ì„¤ì • ë©”ë‰´ì—ì„œ 'ì„œë¹„ìŠ¤ í•´ì§€' ì„ íƒ ì‹œ ì¦‰ì‹œ í•´ì§€ í™”ë©´ìœ¼ë¡œ ì´ë™í•˜ì§€ ì•Šê³  'ë¬¸ì œê°€ ìˆìœ¼ì‹ ê°€ìš”?' ì¤‘ê°„ ë‹¨ê³„ë¥¼ ê±°ì³ í•´ê²° ì‹œë„",
            "í•´ì§€ ì˜ì‚¬ í‘œí˜„ ì‹œ '30ì¼ ë¬´ë£Œ ì—°ì¥' ë˜ëŠ” '1:1 ë§ì¶¤ ìƒë‹´' ê°™ì€ ëŒ€ì•ˆì„ ì¹´ë“œ í˜•íƒœë¡œ ì œì‹œí•˜ì—¬ ì´íƒˆ ë°©ì§€"
        ])
    
    if 'ë¡œê·¸ì•„ì›ƒë˜ì„œ' in quotes_text and 'ì§„í–‰ì´ ì•ˆë©ë‹ˆë‹¤' in quotes_text:
        specific_suggestions.extend([
            "ì˜ˆê¸°ì¹˜ ì•Šì€ ë¡œê·¸ì•„ì›ƒ ë°œìƒ ì‹œ ìë™ ë¡œê·¸ì¸ ì‹œë„ ì¤‘ì„ì„ ì•Œë¦¬ëŠ” 'ìë™ìœ¼ë¡œ ë‹¤ì‹œ ë¡œê·¸ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤' ë©”ì‹œì§€ì™€ í•¨ê»˜ ìˆ˜ë™ ë¡œê·¸ì¸ ë²„íŠ¼ ë³‘í–‰ ì œê³µ",
            "ë¡œê·¸ì¸ í™”ë©´ì—ì„œ 'ì´ì „ ê³„ì •ìœ¼ë¡œ ë¹ ë¥¸ ë¡œê·¸ì¸' ë²„íŠ¼ì„ ID ì…ë ¥ì°½ ìœ„ì— ë°°ì¹˜í•˜ì—¬ ì¬ì…ë ¥ ë¶€ë‹´ ê°ì†Œ"
        ])
    
    # If no specific quotes matched, provide category-based generic suggestions
    if not specific_suggestions:
        if category == 'task_success':
            if 'í†µí™”' in issue_type or 'ì „í™”' in issue_type:
                specific_suggestions = [
                    "í†µí™” ì—°ê²° ì‹¤íŒ¨ ì‹œ 'ë‹¤ì‹œ ì—°ê²°' ë²„íŠ¼ì„ í™”ë©´ ì¤‘ì•™ì— í¬ê²Œ ë°°ì¹˜í•˜ê³  ì¼ë°˜ ì „í™”ì•±ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” 'ì¼ë°˜ í†µí™”' ì˜µì…˜ì„ í•¨ê»˜ ì œê³µ",
                    "í†µí™” í’ˆì§ˆ ë¬¸ì œ ë°œìƒ ì‹œ í™”ë©´ ìƒë‹¨ì— ì‹ í˜¸ ê°•ë„ í‘œì‹œê¸°ë¥¼ ì¶”ê°€í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥"
                ]
            elif 'ë„¤íŠ¸ì›Œí¬' in issue_type or 'ì—°ê²°' in issue_type:
                specific_suggestions = [
                    "ì—°ê²° ëŠê¹€ ë°œìƒ ì‹œ 'ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤' ë©”ì‹œì§€ì™€ í•¨ê»˜ ìë™ ì¬ì—°ê²° ì§„í–‰ë¥  í‘œì‹œ",
                    "ì—°ê²° ì‹¤íŒ¨ ë°˜ë³µ ì‹œ 'ë„¤íŠ¸ì›Œí¬ ì„¤ì • í™•ì¸' ê°€ì´ë“œë¥¼ ë‹¨ê³„ë³„ë¡œ ì œê³µí•˜ëŠ” ë„ì›€ë§ í˜ì´ì§€ë¡œ ì—°ê²°"
                ]
            else:
                specific_suggestions = [
                    "í•µì‹¬ ê¸°ëŠ¥ ì˜¤ë¥˜ ì‹œ 'ë¬¸ì œ ì‹ ê³ í•˜ê¸°' ë²„íŠ¼ì„ ì—ëŸ¬ ë©”ì‹œì§€ì™€ í•¨ê»˜ í‘œì‹œí•˜ì—¬ ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘",
                    "ê¸°ëŠ¥ ì‚¬ìš© ì¤‘ ë¬¸ì œ ë°œìƒ ì‹œ 'ì´ì „ ë‹¨ê³„ë¡œ ëŒì•„ê°€ê¸°' ì˜µì…˜ ì œê³µí•˜ì—¬ ì‘ì—… ì—°ì†ì„± ë³´ì¥"
                ]
        
        elif category == 'happiness':
            specific_suggestions = [
                "ì‚¬ìš©ì ë¶ˆë§Œ ê°ì§€ ì‹œ ì•± ìƒë‹¨ì— 'ì˜ê²¬ ë³´ë‚´ê¸°' ì•Œë¦¼ ë°°ë„ˆë¥¼ ì¼ì‹œì ìœ¼ë¡œ í‘œì‹œí•˜ì—¬ í”¼ë“œë°± ê²½ë¡œ ì œê³µ",
                "ê¸ì •ì  ê²½í—˜ í›„ ìì—°ìŠ¤ëŸ½ê²Œ 'ì´ ê¸°ëŠ¥ì´ ë„ì›€ë˜ì…¨ë‚˜ìš”?' ì—„ì§€ì²™ ë²„íŠ¼ìœ¼ë¡œ ë§Œì¡±ë„ ìˆ˜ì§‘"
            ]
        
        elif category == 'engagement':
            specific_suggestions = [
                "ìì£¼ ì‚¬ìš©í•˜ëŠ” ê¸°ëŠ¥ì„ í™ˆ í™”ë©´ 'ì¦ê²¨ì°¾ê¸°' ì˜ì—­ì— ìë™ ë°°ì¹˜í•˜ê³  ì‚¬ìš©ìê°€ ì§ì ‘ í¸ì§‘ ê°€ëŠ¥í•œ ì˜µì…˜ ì œê³µ",
                "ìƒˆ ê¸°ëŠ¥ ì¶”ê°€ ì‹œ ê¸°ì¡´ ì‚¬ìš© íŒ¨í„´ê³¼ ì—°ê´€ëœ ê¸°ëŠ¥ë§Œ 'ìƒˆë¡œìš´ ê¸°ëŠ¥' ë°°ì§€ì™€ í•¨ê»˜ ì¶”ì²œ"
            ]
        
        elif category == 'retention':
            specific_suggestions = [
                "ì¥ê¸°ê°„ ë¯¸ì‚¬ìš© ì‹œ 'ë§ˆì§€ë§‰ ì‚¬ìš© ê¸°ëŠ¥'ì„ ë©”ì¸ í™”ë©´ì— ìš°ì„  í‘œì‹œí•˜ì—¬ ìµìˆ™í•œ ê¸°ëŠ¥ë¶€í„° ì¬ì‹œì‘ ìœ ë„",
                "ê³„ì • ì‚­ì œ ì‹œë„ ì‹œ 'ë°ì´í„° ë°±ì—… í›„ ì‚­ì œ' ì˜µì…˜ìœ¼ë¡œ í–¥í›„ ë³µêµ¬ ê°€ëŠ¥ì„± ì œê³µ"
            ]
        
        elif category == 'adoption':
            specific_suggestions = [
                "ì‹ ê·œ ì‚¬ìš©ì ì˜¨ë³´ë”©ì—ì„œ '3ë¶„ ë¹ ë¥¸ ì‹œì‘' ê²½ë¡œì™€ 'ìì„¸í•œ ì„¤ì •' ê²½ë¡œë¥¼ ì„ íƒì§€ë¡œ ì œê³µ",
                "ì²« ì‚¬ìš© ì‹œ 'ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ê¸°ëŠ¥ 3ê°€ì§€'ë¥¼ ì¹´ë“œ í˜•íƒœë¡œ ì œì‹œí•˜ì—¬ ì„ íƒì  í•™ìŠµ ê°€ëŠ¥"
            ]
    
    return "\n".join([f"- {suggestion}" for suggestion in specific_suggestions])

def generate_specific_problem_from_quotes(quotes_text, category):
    """
    Generate specific problem description based on actual user quotes
    """
    # Extract emotional expressions and specific issues
    if "ë‹µë‹µí•˜ë„¤ìš”" in quotes_text and "í™”ë©´ í™•ëŒ€" in quotes_text:
        return "í™”ë©´ í™•ëŒ€ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì•„ ì‚¬ìš©ìê°€ ë‹µë‹µí•¨ì„ ëŠë¼ëŠ” ìƒí™©ìœ¼ë¡œ ì¸í•œ í•µì‹¬ ê¸°ëŠ¥ ìˆ˜í–‰ ë¶ˆê°€"
    elif "íŠ•ê²¨ì„œ" in quotes_text and "ì¸ì¦ë‹¤ì‹œí•´ì•¼ë˜ê³ " in quotes_text:
        return "ì•±ì´ ë¹ˆë²ˆí•˜ê²Œ íŠ•ê¸°ë©° ë§¤ë²ˆ ì¬ì¸ì¦ì„ ìš”êµ¬í•˜ëŠ” ìƒí™©ìœ¼ë¡œ ì¸í•œ í•µì‹¬ ê¸°ëŠ¥ ìˆ˜í–‰ ë¶ˆê°€"
    elif "í•´ì§€í•˜ê³ ì‹¶ë„¤ìš”" in quotes_text:
        return "ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì „ë°˜ì ì¸ ì‹ ë¢°ë„ ê¸‰ê²© ì €í•˜ë¡œ ì¸í•œ ê³„ì•½ í•´ì§€ ê³ ë ¤"
    elif "ë¡œê·¸ì•„ì›ƒë˜ì„œ" in quotes_text and "ì§„í–‰ì´ ì•ˆë©ë‹ˆë‹¤" in quotes_text:
        return "ë¡œê·¸ì¸ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•œ ì„œë¹„ìŠ¤ ì ‘ê·¼ ë¶ˆê°€"
    elif "ëŠì–´ì§" in quotes_text and "ëŠì–´ì§€ê³ " in quotes_text:
        return "í†µí™” ì—°ê²° ì‹¤íŒ¨ì™€ ì¤‘ë„ ëŠê¹€ í˜„ìƒ ë°˜ë³µìœ¼ë¡œ ì¸í•œ í•µì‹¬ ê¸°ëŠ¥ ìˆ˜í–‰ ë¶ˆê°€"
    elif "ì‚­ì œ" in quotes_text:
        return "ì§€ì†ì ì¸ ê¸°ëŠ¥ ì˜¤ë¥˜ë¡œ ì¸í•œ ì‚¬ìš©ì ì´íƒˆê³¼ ì•± ì™„ì „ ì‚­ì œ"
    else:
        return f"{category} ê´€ë ¨ ì‚¬ìš©ì ë¶ˆë§Œìœ¼ë¡œ ì¸í•œ ì„œë¹„ìŠ¤ ì´ìš© ì €í•˜"
    if "ì—°ê²°" in quotes_text and "ëŠ" in quotes_text:
        user_expressions.append("ì—°ê²°_ëŠê¹€")
    if "cctv" in quotes_text.lower() or "CCTV" in quotes_text:
        user_expressions.append("CCTV_ë¬¸ì œ")
    if "ì‚¬ìš©ë¶ˆê°€" in quotes_text or "ì•ˆë©ë‹ˆë‹¤" in quotes_text:
        user_expressions.append("ê¸°ëŠ¥_ì‚¬ìš©ë¶ˆê°€")
    if "ì˜¤ë¥˜" in quotes_text or "ì—ëŸ¬" in quotes_text:
        user_expressions.append("ì˜¤ë¥˜_ë°œìƒ")
    if "ë‹µë‹µ" in quotes_text or "ì§œì¦" in quotes_text:
        user_expressions.append("ì‚¬ìš©ì_ë¶ˆë§Œ")
    
    if category == 'task_success':
        if "ì•±_í¬ë˜ì‹œ" in user_expressions:
            return """- ì•± ì²« ì‹¤í–‰ ì‹œ, ì‚¬ìš©ì ì„ íƒ ë‹¨ê³„ë¥¼ ê±°ì¹˜ê¸° ì „ì— 'ì•± ì´ˆê¸° ë¡œë”© ì•ˆë‚´ í™”ë©´'ì„ ë³„ë„ë¡œ êµ¬ì„±í•´ ì•±ì´ ë©ˆì¶˜ ë“¯í•œ ì¸ìƒì„ ì¤„ì´ì§€ ì•Šë„ë¡ ì„¤ê³„
- ì‚¬ìš©ì ì„ íƒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì¦‰ì‹œ "ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”" ê°™ì€ ë¹„ì¹¨í•´ì„± ì•ˆë‚´ í™”ë©´ìœ¼ë¡œ ìœ ë„
- ë°˜ë³µ ê°•ì œ ì¢…ë£Œ ì‹œ, ì‚¬ìš©ìì—ê²Œ "ì•±ì´ ì •ìƒ ì‘ë™í•˜ì§€ ì•Šë‚˜ìš”?"ë¼ëŠ” í”¼ë“œë°± ì˜µì…˜ì„ ì œê³µí•´ ì‚¬ìš©ìë„ ë¬¸ì œ ì¸ì‹ì— ê¸°ì—¬í•˜ë„ë¡ ìœ ë„
- í¬ë˜ì‹œ ë°œìƒ ì „ ë§ˆì§€ë§‰ í™”ë©´ì„ ì„ì‹œ ì €ì¥í•˜ì—¬ ì¬ì‹¤í–‰ ì‹œ í•´ë‹¹ ìœ„ì¹˜ì—ì„œ ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„"""
        
        elif "í™”ë©´_í™•ëŒ€_ë¶ˆê°€" in user_expressions:
            return """- CCTV í™”ë©´ ìš°ì¸¡ í•˜ë‹¨ì— 'í™•ëŒ€/ì¶•ì†Œ ë„ì›€ë§' ì•„ì´ì½˜ì„ ìƒì‹œ í‘œì‹œí•˜ì—¬ ì‚¬ìš©ìê°€ ì œìŠ¤ì²˜ ë°©ë²•ì„ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„
- í™”ë©´ í™•ëŒ€ê°€ ì•ˆ ë  ë•Œ "í™”ë©´ì„ ë‘ ì†ê°€ë½ìœ¼ë¡œ ë²Œë ¤ì„œ í™•ëŒ€í•´ë³´ì„¸ìš”" ê°™ì€ ì§ê´€ì ì¸ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ í™”ë©´ ì¤‘ì•™ì— í‘œì‹œ
- í™•ëŒ€ ê¸°ëŠ¥ ì‹¤íŒ¨ ì‹œ "í™•ëŒ€ê°€ ì•ˆ ë˜ì‹œë‚˜ìš”? ìƒˆë¡œê³ ì¹¨ì„ ì‹œë„í•´ë³´ì„¸ìš”" ë²„íŠ¼ê³¼ í•¨ê»˜ ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ ì˜µì…˜ ì œê³µ
- êµ¬í˜• ê¸°ê¸°ì—ì„œ í™•ëŒ€ ê¸°ëŠ¥ì´ ì œí•œì ì¼ ë•Œ "ì´ ê¸°ê¸°ì—ì„œëŠ” í™•ëŒ€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤" ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ"""
        
        elif "ë¡œê·¸ì¸_ë¬¸ì œ" in user_expressions:
            return """- ë¡œê·¸ì¸ ì‹¤íŒ¨ ì‹œ "ì•„ì´ë””ë‚˜ ë¹„ë°€ë²ˆí˜¸ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”" ë©”ì‹œì§€ì™€ í•¨ê»˜ 'ì•„ì´ë”” ì°¾ê¸°', 'ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •' ë²„íŠ¼ì„ ëª…í™•íˆ ë°°ì¹˜
- ì¸ì¦ ì˜¤ë¥˜ ë°œìƒ ì‹œ "ì¸ì¦ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”" ê°™ì€ ì¹œê·¼í•œ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ
- ë°˜ë³µ ë¡œê·¸ì¸ ì‹¤íŒ¨ ì‹œ "ë¡œê·¸ì¸ì— ê³„ì† ë¬¸ì œê°€ ìˆìœ¼ì‹œë©´ ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”" ì•ˆë‚´ì™€ í•¨ê»˜ ê³ ê°ì„¼í„° ì—°ê²° ë²„íŠ¼ ì œê³µ
- ë¡œê·¸ì¸ í™”ë©´ì—ì„œ "ê°„í¸ ë¡œê·¸ì¸" ì˜µì…˜ì„ ì¶”ê°€í•˜ì—¬ ìƒì²´ ì¸ì¦ ë“± ëŒ€ì•ˆ ì œê³µ"""
        
        elif "ì—°ê²°_ëŠê¹€" in user_expressions:
            return """- ì—°ê²°ì´ ëŠê²¼ì„ ë•Œ "ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤" ë©”ì‹œì§€ì™€ í•¨ê»˜ 'ì¬ì—°ê²°' ë²„íŠ¼ì„ í™”ë©´ ì¤‘ì•™ì— í¬ê²Œ ë°°ì¹˜
- ìì£¼ ì—°ê²°ì´ ëŠê¸°ëŠ” ê²½ìš° "ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”" ì•ˆë‚´ ë©”ì‹œì§€ì™€ í•¨ê»˜ ë„¤íŠ¸ì›Œí¬ ì§„ë‹¨ ê°€ì´ë“œ ì œê³µ
- ì—°ê²° ì¤‘ í™”ë©´ì— "ì—°ê²° ì¤‘ì…ë‹ˆë‹¤..." ë©”ì‹œì§€ì™€ í•¨ê»˜ ì§„í–‰ë¥  í‘œì‹œí•˜ì—¬ ì‚¬ìš©ìê°€ ê¸°ë‹¤ë¦¼ì˜ ì´ìœ ë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ ì„¤ê³„
- ì—°ê²° ì‹¤íŒ¨ ë°˜ë³µ ì‹œ "ê³„ì† ì—°ê²°ì— ë¬¸ì œê°€ ìˆìœ¼ì‹œë©´ ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”" ì˜µì…˜ ì œê³µ"""
        
        elif "CCTV_ë¬¸ì œ" in user_expressions:
            return """- CCTV í™”ë©´ì´ ì•ˆ ë³´ì¼ ë•Œ "CCTV ì—°ê²°ì„ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤" ë©”ì‹œì§€ì™€ í•¨ê»˜ ë¡œë”© ìƒíƒœ í‘œì‹œ
- ì˜ìƒì´ ëŠê¸¸ ë•Œ "ì˜ìƒ ì—°ê²°ì´ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. ìƒˆë¡œê³ ì¹¨ì„ ì‹œë„í•´ì£¼ì„¸ìš”" ë²„íŠ¼ê³¼ í•¨ê»˜ ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ ì˜µì…˜ ì œê³µ
- ì—¬ëŸ¬ ì¹´ë©”ë¼ í™”ë©´ì—ì„œ ê°ê° "ì „ì²´í™”ë©´" ë²„íŠ¼ì„ ê°œë³„ ë°°ì¹˜í•˜ì—¬ ì›í•˜ëŠ” ì¹´ë©”ë¼ë§Œ í¬ê²Œ ë³¼ ìˆ˜ ìˆë„ë¡ ì„¤ê³„
- CCTV ê¸°ëŠ¥ ì‚¬ìš© ì¤‘ ë¬¸ì œ ë°œìƒ ì‹œ "CCTVì— ë¬¸ì œê°€ ìˆë‚˜ìš”?" í”¼ë“œë°± ë²„íŠ¼ìœ¼ë¡œ ì‚¬ìš©ì ì˜ê²¬ ìˆ˜ì§‘"""
        
        else:
            return """- í•µì‹¬ ê¸°ëŠ¥ ì‚¬ìš© ì¤‘ ë¬¸ì œ ë°œìƒ ì‹œ "ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”" ë©”ì‹œì§€ì™€ í•¨ê»˜ 'ì¬ì‹œë„' ë²„íŠ¼ ì œê³µ
- ê¸°ëŠ¥ ì‹¤í–‰ì´ ì˜¤ë˜ ê±¸ë¦´ ë•Œ "ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”" ë©”ì‹œì§€ì™€ í•¨ê»˜ ì§„í–‰ë¥  í‘œì‹œ
- ìì£¼ ì‚¬ìš©í•˜ëŠ” ê¸°ëŠ¥ì„ í™ˆ í™”ë©´ ìƒë‹¨ì— í° ë²„íŠ¼ìœ¼ë¡œ ë°°ì¹˜í•˜ì—¬ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„
- ì˜¤ë¥˜ ë°œìƒ ì‹œ "ë¬¸ì œê°€ ê³„ì†ë˜ë©´ ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”" ì•ˆë‚´ì™€ í•¨ê»˜ ê³ ê°ì„¼í„° ì—°ê²° ë²„íŠ¼ ì œê³µ"""
    
    elif category == 'happiness':
        if "ì‚¬ìš©ì_ë¶ˆë§Œ" in user_expressions:
            return """- ì‚¬ìš©ì ë¶ˆë§Œ í‘œí˜„ ì‹œ ì•± ë‚´ "ì˜ê²¬ ë³´ë‚´ê¸°" ê¸°ëŠ¥ì„ ë©”ë‰´ ìƒë‹¨ì— ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë°°ì¹˜
- ë¬¸ì œ í•´ê²° í›„ "ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆë‚˜ìš”?" í™•ì¸ ë©”ì‹œì§€ë¡œ ì‚¬ìš©ì ë§Œì¡±ë„ í™•ì¸
- ë‹µë‹µí•¨ì„ í‘œí˜„í•˜ëŠ” ì‚¬ìš©ìë¥¼ ìœ„í•´ "ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ì£¼ì„¸ìš”" ê°™ì€ ë”°ëœ»í•œ ë©”ì‹œì§€ ì œê³µ
- ê¸ì •ì  í”¼ë“œë°± ì‹œ "ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ ë³„ì  ë‚¨ê¸°ê¸°" ë“±ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ìœ ë„ ë©”ì‹œì§€ í‘œì‹œ"""
        else:
            return """- ì‚¬ìš©ì ë§Œì¡±ë„ ì¡°ì‚¬ë¥¼ íŒì—…ì´ ì•„ë‹Œ ì•± ì‚¬ìš© í”Œë¡œìš°ì— ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©
- ë¬¸ì œ í•´ê²° í›„ "í•´ê²°ë˜ì—ˆë‚˜ìš”?" í™•ì¸ ë©”ì‹œì§€ë¡œ ì‚¬ìš©ì ë§Œì¡±ë„ í™•ì¸
- ì‚¬ìš©ì ë¶ˆë§Œ í‘œí˜„ ì‹œ ì•± ë‚´ "ì˜ê²¬ ë³´ë‚´ê¸°" ê¸°ëŠ¥ì„ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë©”ë‰´ ìƒë‹¨ì— ë°°ì¹˜
- ê¸ì •ì  í”¼ë“œë°± ì‹œ "ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ ë³„ì  ë‚¨ê¸°ê¸°" ë“±ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ìœ ë„ ë©”ì‹œì§€ í‘œì‹œ"""
    
    elif category == 'engagement':
        return """- ì‚¬ìš©ìê°€ íŠ¹ì • ê¸°ëŠ¥ì„ ìì£¼ ì‚¬ìš©í•  ë•Œ "ì´ ê¸°ëŠ¥ë„ ìœ ìš©í•  ê²ƒ ê°™ì•„ìš”" ê°™ì€ ê´€ë ¨ ê¸°ëŠ¥ ì¶”ì²œ ë©”ì‹œì§€ë¥¼ ì ì ˆí•œ íƒ€ì´ë°ì— í‘œì‹œ
- ì•± ì‚¬ìš© íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ "ìì£¼ ì‚¬ìš©í•˜ëŠ” ê¸°ëŠ¥" ì„¹ì…˜ì„ í™ˆ í™”ë©´ì— ë°°ì¹˜
- ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶œì‹œ ì‹œ "ìƒˆë¡œìš´ ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤" ì•Œë¦¼ì„ ê¸°ì¡´ ì‚¬ìš© íŒ¨í„´ê³¼ ì—°ê²°í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì†Œê°œ
- ì‚¬ìš©ì í™œë™ì´ ì¤„ì–´ë“¤ ë•Œ "ë†“ì¹˜ì‹  ê¸°ëŠ¥ì´ ìˆì–´ìš”" ì•Œë¦¼ìœ¼ë¡œ ì¬ì°¸ì—¬ ìœ ë„"""
    
    elif category == 'retention':
        return """- ì‚¬ìš©ìê°€ ì•±ì„ ì‚­ì œí•˜ë ¤ í•  ë•Œ "ì ê¹, ë¬¸ì œê°€ ìˆìœ¼ì‹ ê°€ìš”?" íŒì—…ìœ¼ë¡œ ì´íƒˆ ì‚¬ìœ  íŒŒì•… ë° ì¦‰ì‹œ í•´ê²° ì‹œë„
- ì¥ê¸°ê°„ ë¯¸ì‚¬ìš© ì‹œ "ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚¬ìš©í•˜ì…¨ë˜ ê¸°ëŠ¥" ì¤‘ì‹¬ìœ¼ë¡œ "ë‹¤ì‹œ ì‹œì‘í•´ë³´ì„¸ìš”" ë©”ì‹œì§€ë¡œ ë³µê·€ ìœ ë„
- ê³„ì • ì‚­ì œ ì „ "ë°ì´í„°ë¥¼ ë°±ì—…í•´ë‘ì‹œê² ì–´ìš”?" ì˜µì…˜ ì œê³µí•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥ì„± ì—´ì–´ë‘ê¸°
- ì‚¬ìš©ìë³„ ì´ìš© íŒ¨í„´ ê¸°ë°˜ "ì´ëŸ° ê¸°ëŠ¥ë„ ìˆì–´ìš”" ë§ì¶¤í˜• ì•ˆë‚´ë¡œ ì§€ì† ì‚¬ìš© ìœ ë„"""
    
    elif category == 'adoption':
        return """- ì‹ ê·œ ì‚¬ìš©ì ì˜¨ë³´ë”© ì‹œ "3ë¶„ ë§Œì— ì‹œì‘í•˜ê¸°" ë“± ëª…í™•í•œ ì‹œê°„ ì˜ˆìƒì¹˜ ì œì‹œ
- ë³µì¡í•œ ì´ˆê¸° ì„¤ì •ì„ "ë‚˜ì¤‘ì— ì„¤ì •í•˜ê¸°" ì˜µì…˜ê³¼ í•¨ê»˜ ì œê³µí•˜ì—¬ ì§„ì… ì¥ë²½ ì™„í™”
- ì²« ì„±ê³µ ê²½í—˜ í›„ "ì˜í•˜ì…¨ì–´ìš”! ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”" ë©”ì‹œì§€ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ê¸°ëŠ¥ í™•ì¥ ìœ ë„
- ì‚¬ìš© ëª©ì ë³„ "ë¹ ë¥¸ ì‹œì‘" í…œí”Œë¦¿ ì œê³µ (ì˜ˆ: "CCTVë§Œ ì‚¬ìš©í•˜ê¸°", "í†µí™” ê¸°ëŠ¥ ì¤‘ì‹¬ìœ¼ë¡œ ì‹œì‘í•˜ê¸°" ë“±)"""
    
    else:
        return """- ì‚¬ìš©ì ë¦¬ë·°ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì  ë¬¸ì œì ì„ í•´ê²°í•˜ëŠ” "ë‹¨ê³„ë³„ ê°€ì´ë“œ" ì œê³µ
- ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì— ëŒ€í•œ "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸" ì„¹ì…˜ì„ ì•± ë‚´ ì‰½ê²Œ ì ‘ê·¼ ê°€ëŠ¥í•œ ìœ„ì¹˜ì— ë°°ì¹˜
- ì‚¬ìš©ì í”¼ë“œë°±ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ë¹ ë¥¸ ê°œì„  ì‚¬í•­ì„ "ì—…ë°ì´íŠ¸ ì†Œì‹"ìœ¼ë¡œ íˆ¬ëª…í•˜ê²Œ ê³µìœ 
- ê° ê¸°ëŠ¥ë³„ "ë„ì›€ë§" ë²„íŠ¼ì„ ìƒí™©ì— ë§ê²Œ ë°°ì¹˜í•˜ì—¬ ì¦‰ì‹œ ë„ì›€ ë°›ì„ ìˆ˜ ìˆë„ë¡ ì„¤ê³„"""

def main():
    """Main function to run the scraper"""
    try:
        # Parse command line arguments
        analyze_mode = '--analyze' in sys.argv
        
        if analyze_mode:
            # Remove --analyze flag from arguments
            args = [arg for arg in sys.argv[1:] if arg != '--analyze']
            
            if len(args) >= 3:
                # Format: python scraper.py --analyze google_app_id apple_app_id count sources
                app_id_google = args[0]
                app_id_apple = args[1]
                count = int(args[2])
                sources = args[3].split(',') if len(args) > 3 else ['google_play']
            else:
                # Legacy format: python scraper.py --analyze app_id count
                app_id_google = args[0] if len(args) > 0 else 'com.lguplus.sohoapp'
                app_id_apple = '1571096278'
                count = int(args[1]) if len(args) > 1 else 100
                sources = ['google_play']
        else:
            if len(sys.argv) > 3:
                # Format: python scraper.py google_app_id apple_app_id count sources
                app_id_google = sys.argv[1]
                app_id_apple = sys.argv[2]
                count = int(sys.argv[3])
                sources = sys.argv[4].split(',') if len(sys.argv) > 4 else ['google_play']
            else:
                # Legacy format: python scraper.py app_id count
                app_id_google = sys.argv[1] if len(sys.argv) > 1 else 'com.lguplus.sohoapp'
                app_id_apple = '1571096278'
                count = int(sys.argv[2]) if len(sys.argv) > 2 else 100
                sources = ['google_play']
        
        # Get reviews from specified sources
        service_name = 'ìµì‹œì˜¤'  # Default service name
        reviews_data = scrape_reviews(app_id_google, app_id_apple, count, sources, service_name)
        
        if analyze_mode:
            # Perform analysis
            analysis_result = analyze_sentiments(reviews_data)
            result = {
                'success': True,
                'message': f'{len(reviews_data)}ê°œì˜ ë¦¬ë·°ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.',
                'reviewCount': len(reviews_data),
                'insights': analysis_result['insights'],
                'wordCloud': analysis_result['wordCloud']
            }
        else:
            # Always include analysis for collection
            analysis_result = analyze_sentiments(reviews_data)
            result = {
                'success': True,
                'reviews': reviews_data,
                'reviewCount': len(reviews_data),
                'message': f'{len(reviews_data)}ê°œì˜ ë¦¬ë·°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.',
                'analysis': analysis_result,
                'sources': sources,
                'counts': {
                    'google_play': len([r for r in reviews_data if r['source'] == 'google_play']),
                    'app_store': len([r for r in reviews_data if r['source'] == 'app_store']),
                    'naver_blog': len([r for r in reviews_data if r['source'] == 'naver_blog']),
                    'naver_cafe': len([r for r in reviews_data if r['source'] == 'naver_cafe'])
                }
            }
        
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
    except Exception as e:
        error_result = {
            'success': False,
            'error': str(e),
            'message': 'ë¦¬ë·° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
        }
        print(json.dumps(error_result, ensure_ascii=False, indent=2))
        sys.exit(1)

if __name__ == "__main__":
    main()