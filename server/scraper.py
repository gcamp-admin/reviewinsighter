#!/usr/bin/env python3
"""
Google Play Store Review Scraper for Ïö∞Î¶¨Í∞ÄÍ≤å Ìå®ÌÇ§ÏßÄ
"""

import json
import sys
import requests
from datetime import datetime
from google_play_scraper import Sort, reviews
import pandas as pd
import xml.etree.ElementTree as ET
import re
from service_data import get_service_keywords, get_service_info
from naver_api import search_naver, extract_text_from_html

# Enhanced Korean text processing
try:
    from konlpy.tag import Okt, Kkma
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    ADVANCED_PROCESSING = True
except ImportError:
    ADVANCED_PROCESSING = False
    print("Advanced Korean processing libraries not available, using basic text processing", file=sys.stderr)

def extract_korean_words_advanced(text_list, sentiment='positive', max_words=10):
    """
    Enhanced Korean word extraction using KoNLPy for morphological analysis
    
    Args:
        text_list: List of text strings to analyze
        sentiment: Target sentiment ('positive' or 'negative')
        max_words: Maximum number of words to return
        
    Returns:
        List of word frequency dictionaries
    """
    if not ADVANCED_PROCESSING or not text_list:
        return extract_korean_words_basic(text_list, sentiment, max_words)
    
    try:
        # Initialize Korean morphological analyzer
        okt = Okt()
        word_freq = {}
        
        for text in text_list:
            if not text or not isinstance(text, str):
                continue
                
            # Extract nouns and adjectives (most meaningful for sentiment analysis)
            morphs = okt.pos(text, stem=True)
            
            # Filter for meaningful Korean words
            for word, pos in morphs:
                # Include nouns, adjectives, and verbs
                if pos in ['Noun', 'Adjective', 'Verb'] and len(word) >= 2:
                    # Remove common stop words
                    if word not in ['ÏûàÎã§', 'ÏóÜÎã§', 'ÎêòÎã§', 'ÌïòÎã§', 'Ïù¥Îã§', 'Í∑∏Î†áÎã§', 'Í∞ôÎã§', 'Îã§Î•¥Îã§', 'ÎßéÎã§', 'Ï†ÅÎã§', 'ÌÅ¨Îã§', 'ÏûëÎã§', 'Ï¢ãÎã§', 'ÎÇòÏÅòÎã§', 'ÏÉàÎ°≠Îã§', 'Ïò§ÎûòÎêòÎã§']:
                        word_freq[word] = word_freq.get(word, 0) + 1
        
        # Sort by frequency and return top words
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        
        result = []
        for word, freq in sorted_words[:max_words]:
            result.append({
                'word': word,
                'frequency': freq,
                'sentiment': sentiment
            })
        
        print(f"Advanced Korean processing extracted {len(result)} {sentiment} words", file=sys.stderr)
        return result
        
    except Exception as e:
        print(f"Error in advanced Korean processing: {e}, falling back to basic processing", file=sys.stderr)
        return extract_korean_words_basic(text_list, sentiment, max_words)

def extract_korean_words_basic(text_list, sentiment='positive', max_words=10):
    """
    Basic Korean word extraction using regex and frequency analysis
    """
    word_freq = {}
    
    for text in text_list:
        if not text or not isinstance(text, str):
            continue
            
        # Remove punctuation and split into words
        import re
        korean_words = re.findall(r'[Í∞Ä-Ìû£]+', text)
        
        for word in korean_words:
            if len(word) >= 2:  # Filter out single characters
                # Skip common words and particles
                skip_words = ['Ïù¥Í≤É', 'Í∑∏Í≤É', 'Ï†ÄÍ≤É', 'Ïó¨Í∏∞', 'Í±∞Í∏∞', 'Ï†ÄÍ∏∞', 'Ïù¥Î†áÍ≤å', 'Í∑∏Î†áÍ≤å', 'Ï†ÄÎ†áÍ≤å', 'ÎïåÎ¨∏', 'ÏúÑÌï¥', 'ÌÜµÌï¥', 'ÎåÄÌï¥', 'ÏóêÏÑú', 'ÏúºÎ°ú', 'ÏóêÍ≤å', 'ÌïúÌÖå', 'ÏóêÎèÑ', 'ÎèÑ', 'Îäî', 'ÏùÄ', 'Ïù¥', 'Í∞Ä', 'ÏùÑ', 'Î•º', 'Ïùò', 'Í≥º', 'ÏôÄ', 'Ïóê', 'ÏóêÏÑú', 'ÏúºÎ°ú', 'Î°ú', 'Îßå', 'Î∂ÄÌÑ∞', 'ÍπåÏßÄ', 'Î≥¥Îã§', 'Ï≤òÎüº', 'Í∞ôÏù¥', 'ÎßàÎã§', 'ÎßàÏ†Ä', 'Ï°∞Ï∞®', 'Î∞ñÏóê', 'Ïô∏Ïóê', 'ÎåÄÏã†', 'ÎßêÍ≥†']
                
                if word not in skip_words:
                    word_freq[word] = word_freq.get(word, 0) + 1
    
    # Sort by frequency and return top words
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    
    result = []
    for word, freq in sorted_words[:max_words]:
        result.append({
            'word': word,
            'frequency': freq,
            'sentiment': sentiment
        })
    
    return result

def is_negative_review_by_sections(text: str, negative_keywords: list) -> bool:
    """
    Check if review is negative based on section analysis
    
    Args:
        text: Review text content
        negative_keywords: List of negative keywords to check
        
    Returns:
        Boolean indicating if review is negative
    """
    lowered = text.lower()

    # Í∏∞Ï§Ä: "Îã®Ï†ê" Ïù¥ÌõÑ Î∂ÄÏ†ï ÌÇ§ÏõåÎìú Ìè¨Ìï® Ïó¨Î∂Ä
    if "Îã®Ï†ê" in lowered:
        parts = lowered.split("Îã®Ï†ê", 1)
        after = parts[1]  # Îã®Ï†ê Ïù¥ÌõÑ Î¨∏Ïû•
        if any(kw in after for kw in negative_keywords):
            return True

    # Î≥¥Ï°∞: Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ÏóêÎèÑ Î∂ÄÏ†ï ÌÇ§ÏõåÎìúÍ∞Ä ÎßéÏùÄ Í≤ΩÏö∞ Î∂ÄÏ†ï Ï≤òÎ¶¨
    neg_count = sum(lowered.count(kw) for kw in negative_keywords)
    return neg_count >= 2  # Î∂ÄÏ†ï ÌÇ§ÏõåÎìú 2Í∞ú Ïù¥ÏÉÅÏù¥Î©¥ Î∂ÄÏ†ï

def analyze_text_sentiment(text):
    """
    Korean text-based sentiment analysis
    Analyzes review content to determine sentiment regardless of star ratings
    
    Args:
        text: Review text content
        
    Returns:
        String: 'positive' or 'negative'
    """
    if not text or not isinstance(text, str):
        return "positive"  # Default to positive for empty/invalid text
    
    # Convert to lowercase for analysis
    content = text.lower()
    
    # Priority rule: Any review containing 'Î∂àÌé∏' is automatically negative
    if 'Î∂àÌé∏' in content:
        return "negative"
    
    # Refined negative keywords focusing on specific app issues
    strong_negative_keywords = [
        "Îú®Í±∞ÏõÄ", "Î∂àÌé∏", "Î∞©Ìï¥", "ÏóÜÏùå", "Ïò§Î•ò", "ÏïàÎê®", "ÏïàÎèº", 
        "Ïä§Ìå∏", "Ï∞®Îã® Ïïà", "Î¨∏Ï†ú", "ÎÅäÍπÄ", "Í≥ºÏó¥", "Í±∞Ïä¨Î¶º",
        
        # Additional critical issues
        "Í∑ÄÏ∞Æ", "ÏßúÏ¶ù", "ÌôîÎÇ®", "Ïä§Ìä∏Î†àÏä§", "ÌûòÎì§", "Ïñ¥Î†µ", 
        "Î≥ÑÎ°ú", "ÏµúÏïÖ", "ÌòïÌé∏ÏóÜ", "Íµ¨Î¶¨", "Ïã§Îßù", "ÎÇòÏÅò", 
        "ÏóêÎü¨", "Î®πÌÜµ", "Î©àÏ∂§", "ÌäïÍπÄ", "ÎäêÎ¶º", "Î†â", "Î≥µÏû°",
        
        # Technical problems
        'Î≤ÑÍ∑∏', 'ÌäïÍ∏¥Îã§', 'ÎÇòÍ∞ÄÎ≤ÑÎ¶º', 'Í∫ºÏßê', 'ÌÅ¨ÎûòÏãú', 'Ï¢ÖÎ£å', 'Ïû¨ÏãúÏûë',
        'ÏûëÎèôÏïàÌï®', 'Ïã§ÌñâÏïàÎê®', 'ÏïàÎ∞õÏïÑÏ†∏', 'Î∞õÏïÑÏßÄÏßÄ', 'Ïã§ÌñâÎêòÏßÄ', 'ÏûëÎèôÌïòÏßÄ',
        'ÎÅäÏñ¥ÏßÄ', 'ÎÅäÍ∏¥Îã§', 'Ïó∞Í≤∞ÏïàÎê®', 'ÏïàÎì§Î¶º', 'ÏÜåÎ¶¨ÏïàÎÇ®',
        
        # User dissatisfaction
        'Ïì∞Î†àÍ∏∞', 'Îπ°Ïπ®', 'Ïó¥Î∞õ', 'Î∂àÎßå', 'Ïã´Ïñ¥', 'ÎãµÎãµ', 'ÎãπÌô©Ïä§Îü¨Ïö¥',
        'Í≥†Ïû•', 'ÎßùÌï®', 'ÏóâÎßù',
        
        # Usage abandonment
        'ÏÇ≠Ï†ú', 'ÏßÄÏõÄ', 'Ìï¥ÏßÄ', 'Í∑∏Îßå', 'ÏïàÏì∏', 'Îã§Î•∏Í±∞', 'Î∞îÍøÄ', 'ÌÉàÌá¥', 'Ìè¨Í∏∞', 'Ï§ëÎã®',
        'ÏïàÏç®', 'ÏÇ¨Ïö©ÏïàÌï®', 'Î™ªÏì∞Í≤†', 'Ïì∏Î™®ÏóÜ',
        
        # App-specific issues
        'ÌÜµÌôîÏ§ë ÎåÄÍ∏∞', 'ÏïàÏßÄÏõê', 'Î≥ºÎ•®Î≤ÑÌäº', 'ÏßÑÎèô', 'Î∞±Í∑∏ÎùºÏö¥Îìú', 'ÏûêÎèôÏúºÎ°ú', 'Ïä¨ÎùºÏù¥Îìú',
        'Ïä§Ìå∏Ï†ïÎ≥¥', 'Îî∏Î†§ÏôÄÏÑú', 'Î≤àÌò∏ÌôïÏù∏', 'Í∏∞Îã§Î†§Ïïº', 'Ï∞®Îüâ', 'Î∏îÌà¨', 'ÌÜµÌôîÏ¢ÖÎ£å',
        
        # User experience problems
        'ÌôîÎ©¥ ÌôïÎåÄ ÏïàÎê®', 'Î™ªÏïåÏïÑ', 'ÏßÄÎÇòÏπòÎäî', 'Ïï†ÌîåÏù¥Îì†', 'ÏÇºÏÑ±Ïù¥Îì†',
        'Ï†ÄÍ≤©ÌïòÎ†§Í≥†', 'ÏïåÎú∞Ìè∞ ÏïàÎêúÎã§', 'ÏßúÏ¶ùÎÇòÏ£†', 'ÏïàÎê†Í±∞', 'Ïôú ÏïàÎê©ÎãàÍπå', 'ÎÇúÎ¶¨ÎÇ¨Ïùå',
        'ÏÉÅÎåÄÎ∞©Í≥º ÎÇòÏùò Î™©ÏÜåÎ¶¨Ïùò Ïã±ÌÅ¨Í∞Ä ÎßûÏßÄ ÏïäÍ≥†', 'Ïö∏Î¶¨ÏßÄÏïäÍ±∞ÎÇò', 'Î∂ÄÏû¨Ï§ë', 'Î∞îÎ°ú ÎÅäÍ∏∞Í≥†',
        'ÏïàÍ±∏Î¶¨Îäî', 'ÎπàÎ≤àÌï®', 'ÏãúÎÅÑÎü¨ÏõåÏ£ΩÍ≤†ÏäµÎãàÎã§', 'ÎãπÌô©Ïä§Îü¨Ïö¥', 'Î∂àÌé∏ÌïòÎÑ§Ïöî'
    ]
    
    # Apply section-based analysis first
    if is_negative_review_by_sections(text, strong_negative_keywords):
        return "negative"
    
    # Positive indicators (Korean expressions)
    positive_keywords = [
        # Direct praise
        'Ï¢ãÏïÑ', 'Ï¢ãÎã§', 'Ï¢ãÎÑ§', 'Ï¢ãÏùå', 'ÌõåÎ•≠', 'Ïö∞Ïàò', 'ÏµúÍ≥†', 'ÎåÄÎ∞ï', 'ÏôÑÎ≤Ω', 'ÎßåÏ°±',
        'Ïûò', 'Ìé∏Î¶¨', 'Ïú†Ïö©', 'ÎèÑÏõÄ', 'Í∞êÏÇ¨', 'Í≥†ÎßàÏõå', 'Ï∂îÏ≤ú', 'Í¥úÏ∞Æ', 'ÎÇòÏÅòÏßÄÏïä',
        
        # Functional satisfaction
        'ÏûòÏÇ¨Ïö©', 'ÏûòÏì∞', 'ÏûòÎê®', 'ÏûòÎêò', 'ÏûòÏûëÎèô', 'Ï†ïÏÉÅ', 'ÏõêÌôú', 'Î∂ÄÎìúÎüΩ', 'Îπ†Î•¥',
        'Í∞ÑÌé∏', 'ÏâΩ', 'Ìé∏Ìï¥', 'ÍπîÎÅî', 'ÏïàÏ†ï', 'Ïã†Î¢∞',
        
        # Appreciation
        'Ïú†Ïö©ÌïòÍ≥†', 'Ï¢ãÏïÑÏöî', 'ÎßâÏïÑÏ§òÏÑú', 'ÏöîÏïΩÎêòÍ≥†', 'ÌÖçÏä§Ìä∏Î°ú', 'Ïç®Ï†∏ÏÑú',
        'Î≥¥Ïù¥Ïä§ÌîºÏã±', 'ÎßâÏïÑÏ§òÏÑú', 'Ï¢ãÏïÑÏöî', 'aiÍ≥†', 'ÌÜµÌôîÎÇ¥Ïö©',
        
        # Mild complaints that are still generally positive
        'Ï¢ãÏßÄÎßå', 'ÎßåÏ°±Ìï©ÎãàÎã§Îßå', 'Ï†ÑÎ∞òÏ†ÅÏù∏ Í∏∞Îä•ÏùÄ ÎßåÏ°±', 'Ïûò ÏÇ¨Ïö©ÌïòÍ≥† ÏûàÏäµÎãàÎã§',
        'Îî± ÌïúÍ∞ÄÏßÄ ÏïÑÏâ¨Ïö¥Í≤å', 'Ïù¥Í≤ÉÎßå ÎêúÎã§Î©¥', 'Ï†ïÎßê ÏôÑÎ≤ΩÌï†Í±∞'
    ]
    
    # Count negative and positive indicators
    negative_count = 0
    positive_count = 0
    
    for keyword in strong_negative_keywords:
        if keyword in content:
            negative_count += 1
    
    for keyword in positive_keywords:
        if keyword in content:
            positive_count += 1
    
    # Special handling for mixed sentiment reviews
    # If review contains both positive and negative elements, analyze overall tone
    if negative_count > 0 and positive_count > 0:
        # Check for constructive feedback patterns
        constructive_patterns = [
            'Ï¢ãÏßÄÎßå', 'ÎßåÏ°±Ìï©ÎãàÎã§Îßå', 'Ï¢ãÍ≤†ÎÑ§Ïöî', 'ÎêúÎã§Î©¥', 'ÏßÄÏõêÌï¥Ï§ÑÏàò', 'Í∞úÏÑ†',
            'Ï∂îÍ∞Ä', 'Ìñ•ÏÉÅ', 'ÏóÖÎç∞Ïù¥Ìä∏', 'Î∞îÍøâÏãúÎã§', 'ÌïòÎ©¥ Ï¢ãÍ≤†'
        ]
        
        is_constructive = any(pattern in content for pattern in constructive_patterns)
        
        # If it's constructive feedback, weight it based on severity
        if is_constructive:
            # Count severity of negative issues
            critical_issues = ['Ïò§Î•ò', 'ÏóêÎü¨', 'Î≤ÑÍ∑∏', 'ÌäïÍπÄ', 'ÌÅ¨ÎûòÏãú', 'ÏµúÏïÖ', 'Ïì∞Î†àÍ∏∞', 'ÏÇ≠Ï†ú']
            has_critical = any(issue in content for issue in critical_issues)
            
            if has_critical:
                return "negative"
            else:
                # Constructive feedback without critical issues = positive
                return "positive"
        else:
            # Mixed sentiment without constructive tone - go with majority
            return "negative" if negative_count > positive_count else "positive"
    
    # Pure sentiment analysis
    if negative_count > 0:
        return "negative"
    elif positive_count > 0:
        return "positive"
    else:
        # No clear sentiment indicators - analyze length and structure
        # Very short reviews or question-only reviews default to positive
        if len(content.strip()) < 10:
            return "positive"
        
        # Check for question marks (often constructive)
        if '?' in content or 'Ïñ∏Ï†ú' in content or 'Ïñ¥ÎñªÍ≤å' in content:
            return "positive"
        
        # Default to positive for neutral content
        return "positive"

def scrape_google_play_reviews(app_id='com.lguplus.sohoapp', count=100, lang='ko', country='kr'):
    """
    Scrape reviews from Google Play Store
    
    Args:
        app_id: Google Play Store app ID
        count: Number of reviews to fetch
        lang: Language code (default: 'ko')
        country: Country code (default: 'kr')
        
    Returns:
        List of review dictionaries
    """
    try:
        # Fetch reviews from Google Play Store
        result, _ = reviews(
            app_id,
            lang=lang,
            country=country,
            sort=Sort.NEWEST,
            count=count
        )
        
        # Process and clean the data
        processed_reviews = []
        for review in result:
            # Text-based sentiment analysis - ignore star ratings completely
            sentiment = analyze_text_sentiment(review['content'])
            
            processed_review = {
                'userId': review['userName'] if review['userName'] else 'ÏùµÎ™Ö',
                'source': 'google_play',
                'rating': review['score'],
                'content': review['content'],
                'sentiment': sentiment,
                'createdAt': review['at'].isoformat() if review['at'] else datetime.now().isoformat()
            }
            processed_reviews.append(processed_review)
        
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping Google Play reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_app_store_reviews(app_id='1571096278', count=100):
    """
    Scrape reviews from Apple App Store
    
    Args:
        app_id: Apple App Store app ID
        count: Number of reviews to fetch (limited by RSS feed)
        
    Returns:
        List of review dictionaries
    """
    try:
        # Construct RSS URL for App Store reviews
        rss_url = f'https://itunes.apple.com/kr/rss/customerreviews/id={app_id}/sortBy=mostRecent/xml'
        
        response = requests.get(rss_url, timeout=10)
        if response.status_code != 200:
            print(f"Failed to fetch App Store reviews: HTTP {response.status_code}", file=sys.stderr)
            return []
        
        root = ET.fromstring(response.content)
        
        # Process reviews (skip first entry which is just metadata)
        processed_reviews = []
        entries = root.findall('.//{http://www.w3.org/2005/Atom}entry')[1:]
        
        for entry in entries[:count]:  # Limit to requested count
            try:
                # Extract review data
                author_elem = entry.find('{http://www.w3.org/2005/Atom}author')
                author = author_elem[0].text if author_elem is not None and len(author_elem) > 0 else 'ÏùµÎ™Ö'
                
                title_elem = entry.find('{http://www.w3.org/2005/Atom}title')
                title = title_elem.text if title_elem is not None else ''
                
                content_elem = entry.find('{http://www.w3.org/2005/Atom}content')
                content = content_elem.text if content_elem is not None else title  # Use title if no content
                
                rating_elem = entry.find('{http://itunes.apple.com/rss}rating')
                rating = int(rating_elem.text) if rating_elem is not None else 3
                
                updated_elem = entry.find('{http://www.w3.org/2005/Atom}updated')
                created_at = updated_elem.text if updated_elem is not None else datetime.now().isoformat()
                
                # Text-based sentiment analysis - ignore star ratings completely
                sentiment = analyze_text_sentiment(content)
                
                processed_review = {
                    'userId': author,
                    'source': 'app_store',
                    'rating': rating,
                    'content': content,
                    'sentiment': sentiment,
                    'createdAt': created_at
                }
                processed_reviews.append(processed_review)
                
            except Exception as entry_error:
                print(f"Error processing App Store review entry: {entry_error}", file=sys.stderr)
                continue
        
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping App Store reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_naver_blog_reviews(service_name='ÏùµÏãúÏò§', count=100):
    """
    Scrape reviews from Naver Blog using real API
    
    Args:
        service_name: Service name to search for
        count: Number of reviews to fetch
        
    Returns:
        List of review dictionaries
    """
    try:
        # Get service keywords for better search results
        keywords = get_service_keywords(service_name)
        processed_reviews = []
        
        # Search with multiple keywords to get diverse results
        search_results = []
        for keyword in keywords[:3]:  # Use top 3 keywords to avoid API rate limits
            results = search_naver(keyword, search_type="blog", display=min(count // len(keywords[:3]) + 5, 30))
            search_results.extend(results)
        
        # Process blog search results
        for i, item in enumerate(search_results[:count]):
            try:
                # Filter out non-review content using quality check
                from naver_api import is_likely_user_review
                if not is_likely_user_review(item, keywords):
                    continue
                
                # Extract clean text from description
                description = extract_text_from_html(item.get('description', ''))
                title = extract_text_from_html(item.get('title', ''))
                
                # Combine title and description for content
                content = f"{title}. {description}" if title and description else (title or description)
                
                # Skip if content is too short
                if len(content.strip()) < 10:
                    continue
                
                # Text-based sentiment analysis
                sentiment = analyze_text_sentiment(content)
                
                # Convert date from YYYYMMDD to ISO format
                postdate = item.get('postdate', datetime.now().strftime('%Y%m%d'))
                try:
                    # Parse YYYYMMDD format and convert to ISO
                    if len(postdate) == 8 and postdate.isdigit():
                        year = int(postdate[:4])
                        month = int(postdate[4:6])
                        day = int(postdate[6:8])
                        created_at = datetime(year, month, day).isoformat()
                    else:
                        created_at = datetime.now().isoformat()
                except:
                    created_at = datetime.now().isoformat()
                
                processed_review = {
                    'userId': f"Î∏îÎ°úÍ±∞{i+1}",
                    'source': 'naver_blog',
                    'rating': 4 if sentiment == 'positive' else 2,
                    'content': content[:500],  # Limit content length
                    'sentiment': sentiment,
                    'createdAt': created_at,
                    'url': item.get('link', '')
                }
                processed_reviews.append(processed_review)
                
            except Exception as item_error:
                print(f"Error processing blog item {i}: {str(item_error)}", file=sys.stderr)
                continue
        
        print(f"Collected {len(processed_reviews)} Naver Blog reviews for {service_name} using keywords: {keywords[:3]}", file=sys.stderr)
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping Naver Blog reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_naver_cafe_reviews(service_name='ÏùµÏãúÏò§', count=100):
    """
    Scrape reviews from Naver Cafe using real API
    
    Args:
        service_name: Service name to search for
        count: Number of reviews to fetch
        
    Returns:
        List of review dictionaries
    """
    try:
        # Get service keywords for better search results
        keywords = get_service_keywords(service_name)
        processed_reviews = []
        
        # Search with multiple keywords to get diverse results
        search_results = []
        for keyword in keywords[:3]:  # Use top 3 keywords to avoid API rate limits
            results = search_naver(keyword, search_type="cafe", display=min(count // len(keywords[:3]) + 5, 30))
            search_results.extend(results)
        
        # Process cafe search results
        for i, item in enumerate(search_results[:count]):
            try:
                # Filter out non-review content using quality check
                from naver_api import is_likely_user_review
                if not is_likely_user_review(item, keywords):
                    continue
                
                # Extract clean text from description
                description = extract_text_from_html(item.get('description', ''))
                title = extract_text_from_html(item.get('title', ''))
                
                # Combine title and description for content
                content = f"{title}. {description}" if title and description else (title or description)
                
                # Skip if content is too short
                if len(content.strip()) < 10:
                    continue
                
                # Text-based sentiment analysis
                sentiment = analyze_text_sentiment(content)
                
                # Convert date from YYYYMMDD to ISO format
                postdate = item.get('postdate', datetime.now().strftime('%Y%m%d'))
                try:
                    # Parse YYYYMMDD format and convert to ISO
                    if len(postdate) == 8 and postdate.isdigit():
                        year = int(postdate[:4])
                        month = int(postdate[4:6])
                        day = int(postdate[6:8])
                        created_at = datetime(year, month, day).isoformat()
                    else:
                        created_at = datetime.now().isoformat()
                except:
                    created_at = datetime.now().isoformat()
                
                processed_review = {
                    'userId': f"Ïπ¥ÌéòÌöåÏõê{i+1}",
                    'source': 'naver_cafe',
                    'rating': 4 if sentiment == 'positive' else 2,
                    'content': content[:500],  # Limit content length
                    'sentiment': sentiment,
                    'createdAt': created_at,
                    'url': item.get('link', ''),
                    'cafe_name': item.get('cafename', '')
                }
                processed_reviews.append(processed_review)
                
            except Exception as item_error:
                print(f"Error processing cafe item {i}: {str(item_error)}", file=sys.stderr)
                continue
        
        print(f"Collected {len(processed_reviews)} Naver Cafe reviews for {service_name} using keywords: {keywords[:3]}", file=sys.stderr)
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping Naver Cafe reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_reviews(app_id_google='com.lguplus.sohoapp', app_id_apple='1571096278', count=100, sources=['google_play'], service_name='ÏùµÏãúÏò§'):
    """
    Scrape reviews from multiple sources
    
    Args:
        app_id_google: Google Play Store app ID
        app_id_apple: Apple App Store app ID
        count: Number of reviews to fetch per source
        sources: List of sources to scrape from
        service_name: Service name for Naver searches
        
    Returns:
        List of review dictionaries
    """
    all_reviews = []
    
    if 'google_play' in sources:
        google_reviews = scrape_google_play_reviews(app_id_google, count)
        all_reviews.extend(google_reviews)
        print(f"Collected {len(google_reviews)} reviews from Google Play", file=sys.stderr)
    
    if 'app_store' in sources:
        apple_reviews = scrape_app_store_reviews(app_id_apple, count)
        all_reviews.extend(apple_reviews)
        print(f"Collected {len(apple_reviews)} reviews from App Store", file=sys.stderr)
    
    if 'naver_blog' in sources:
        blog_reviews = scrape_naver_blog_reviews(service_name, count)
        all_reviews.extend(blog_reviews)
        print(f"Collected {len(blog_reviews)} reviews from Naver Blog", file=sys.stderr)
    
    if 'naver_cafe' in sources:
        cafe_reviews = scrape_naver_cafe_reviews(service_name, count)
        all_reviews.extend(cafe_reviews)
        print(f"Collected {len(cafe_reviews)} reviews from Naver Cafe", file=sys.stderr)
    
    return all_reviews

def analyze_sentiments(reviews):
    """
    Enhanced HEART framework analysis with dynamic insights generation
    
    Args:
        reviews: List of review dictionaries
        
    Returns:
        Dictionary with insights and word frequency data
    """
    if not reviews:
        return {'insights': [], 'wordCloud': {'positive': [], 'negative': []}}
    
    print(f"Starting enhanced HEART analysis on {len(reviews)} reviews...", file=sys.stderr)
    
    # Re-analyze sentiment based on text content only (ignore star ratings)
    for review in reviews:
        # Update sentiment based on text analysis
        review['sentiment'] = analyze_text_sentiment(review['content'])
    
    # Debug: Print text-based sentiment analysis results
    text_based_negative = sum(1 for r in reviews if r['sentiment'] == 'negative')
    text_based_positive = sum(1 for r in reviews if r['sentiment'] == 'positive')
    print(f"Text-based sentiment analysis: {text_based_positive} positive, {text_based_negative} negative", file=sys.stderr)
    
    # HEART framework analysis with detailed issue tracking
    heart_analysis = {
        'task_success': {'issues': [], 'details': []},
        'happiness': {'issues': [], 'details': []},
        'engagement': {'issues': [], 'details': []},
        'adoption': {'issues': [], 'details': []},
        'retention': {'issues': [], 'details': []}
    }
    
    # Pattern matching for specific issues
    for review in reviews:
        content = review['content'].lower()
        rating = review.get('rating', 3)
        user_id = review.get('userId', 'Unknown')
        
        # Analyze ALL reviews regardless of rating to capture nuanced feedback
        # Even high-rated reviews can contain specific complaints and improvement suggestions
        
        # Task Success - Core functionality problems (check regardless of rating)
        if any(keyword in content for keyword in ['Ïò§Î•ò', 'ÏóêÎü¨', 'Î≤ÑÍ∑∏', 'Ìäï', 'Í∫ºÏßê', 'ÏûëÎèôÏïàÌï®', 'Ïã§ÌñâÏïàÎê®', 'ÎÅäÍπÄ', 'Ïó∞Í≤∞ÏïàÎê®', 'ÏïàÎì§Î¶º', 'ÏÜåÎ¶¨ÏïàÎÇ®', 'ÏïàÎê®', 'ÏïàÎêò', 'ÌÅ¨ÎûòÏãú', 'Ï¢ÖÎ£å', 'Ïû¨ÏãúÏûë', 'Î¨∏Ï†ú', 'Î∂àÌé∏', 'ÏïàÎ∞õÏïÑÏßÄ', 'Î∞õÏïÑÏßÄÏßÄ', 'Ïã§ÌñâÎêòÏßÄ', 'ÏûëÎèôÌïòÏßÄ', 'ÎÅäÏñ¥ÏßÄ', 'ÎÅäÍ∏¥Îã§', 'ÎãπÌô©Ïä§Îü¨Ïö¥', 'Í∏∞Îã§Î†§Ïïº', 'Ïä¨ÎùºÏù¥Îìú', 'Î∞±Í∑∏ÎùºÏö¥Îìú', 'ÏûêÎèôÏúºÎ°ú', 'ÎÑòÏñ¥Í∞ÄÏßÄ', 'Í≥ÑÏÜç', 'Î≥ºÎ•®Î≤ÑÌäº', 'ÏßÑÎèô', 'Í∫ºÏßÄÎ©¥', 'Ï¢ãÍ≤†ÎÑ§Ïöî', 'Ï∞®Îüâ', 'Î∏îÌà¨', 'ÌÜµÌôîÏ¢ÖÎ£å', 'ÏùåÏïÖÏû¨ÏÉù', 'Ïä§Ìå∏Ï†ïÎ≥¥', 'Îî∏Î†§ÏôÄÏÑú', 'Î≤àÌò∏ÌôïÏù∏', 'Í∏∞Îã§Î†§Ïïº']):
            heart_analysis['task_success']['issues'].append(content)
            if 'Ìäï' in content or 'Í∫ºÏßê' in content or 'ÌÅ¨ÎûòÏãú' in content:
                heart_analysis['task_success']['details'].append('Ïï± ÌÅ¨ÎûòÏãú')
            elif 'Ïó∞Í≤∞' in content and ('ÏïàÎê®' in content or 'ÎÅäÍπÄ' in content):
                heart_analysis['task_success']['details'].append('ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞')
            elif 'ÏÜåÎ¶¨' in content and 'ÏïàÎÇ®' in content:
                heart_analysis['task_success']['details'].append('ÏùåÏÑ± Í∏∞Îä•')
            elif 'Î≥ºÎ•®Î≤ÑÌäº' in content or 'ÏßÑÎèô' in content:
                heart_analysis['task_success']['details'].append('ÌïòÎìúÏõ®Ïñ¥ Ï†úÏñ¥')
            elif 'Î∞±Í∑∏ÎùºÏö¥Îìú' in content or 'ÏûêÎèôÏúºÎ°ú' in content:
                heart_analysis['task_success']['details'].append('Î∞±Í∑∏ÎùºÏö¥Îìú Ï≤òÎ¶¨')
            elif 'Ïä§Ìå∏Ï†ïÎ≥¥' in content or 'Ïä¨ÎùºÏù¥Îìú' in content:
                heart_analysis['task_success']['details'].append('UI ÌëúÏãú Î¨∏Ï†ú')
            elif 'ÌÜµÌôî' in content or 'Ï†ÑÌôî' in content:
                heart_analysis['task_success']['details'].append('ÌÜµÌôî Í∏∞Îä•')
            else:
                heart_analysis['task_success']['details'].append('Í∏∞Îä• Ïò§Î•ò')
        
        # Happiness - User satisfaction issues (check regardless of rating)
        elif any(keyword in content for keyword in ['ÏßúÏ¶ù', 'ÏµúÏïÖ', 'Ïã§Îßù', 'ÌôîÎÇ®', 'Î∂àÎßå', 'Î≥ÑÎ°ú', 'Íµ¨Î¶º', 'Ïã´Ïñ¥', 'ÎãµÎãµ', 'Ïä§Ìä∏Î†àÏä§', 'ÎãπÌô©Ïä§Îü¨Ïö¥', 'Î∂àÌé∏', 'Í∏∞Îã§Î†§Ïïº', 'Î¨∏Ï†ú']):
            heart_analysis['happiness']['issues'].append(content)
            if 'ÏµúÏïÖ' in content or 'ÌôîÎÇ®' in content:
                heart_analysis['happiness']['details'].append('Í∞ïÌïú Î∂àÎßå')
            elif 'ÎãπÌô©Ïä§Îü¨Ïö¥' in content or 'Î∂àÌé∏' in content:
                heart_analysis['happiness']['details'].append('ÏÇ¨Ïö©Ïûê Í≤ΩÌóò Ï†ÄÌïò')
            else:
                heart_analysis['happiness']['details'].append('ÎßåÏ°±ÎèÑ Ï†ÄÌïò')
        
        # Engagement - Usage patterns (check regardless of rating)
        elif any(keyword in content for keyword in ['ÏïàÏç®', 'ÏÇ¨Ïö©ÏïàÌï®', 'Ïû¨ÎØ∏ÏóÜ', 'ÏßÄÎ£®', 'Ìù•ÎØ∏ÏóÜ', 'Î≥ÑÎ°úÏïàÏì¥', 'Í∞ÄÎÅîÎßå', 'Ï¢ãÏßÄÎßå', 'ÌïòÏßÄÎßå', 'Í∑∏Îü∞Îç∞', 'Îã§Îßå', 'ÏïÑÏâ¨Ïö¥', 'Îçî', 'Ï∂îÍ∞Ä', 'Í∞úÏÑ†', 'Ìñ•ÏÉÅ', 'Ï¢ãÍ≤†ÎÑ§Ïöî']):
            heart_analysis['engagement']['issues'].append(content)
            if 'Ï¢ãÏßÄÎßå' in content or 'ÌïòÏßÄÎßå' in content or 'Ï¢ãÍ≤†ÎÑ§Ïöî' in content:
                heart_analysis['engagement']['details'].append('Í∞úÏÑ† Ï†úÏïà')
            else:
                heart_analysis['engagement']['details'].append('ÏÇ¨Ïö© ÎπàÎèÑ Ï†ÄÌïò')
        
        # Retention - Churn indicators (check regardless of rating)
        elif any(keyword in content for keyword in ['ÏÇ≠Ï†ú', 'Ìï¥ÏßÄ', 'Í∑∏Îßå', 'ÏïàÏì∏', 'Îã§Î•∏Í±∞', 'Î∞îÍøÄ', 'ÌÉàÌá¥', 'Ìè¨Í∏∞', 'Ï§ëÎã®']):
            heart_analysis['retention']['issues'].append(content)
            heart_analysis['retention']['details'].append('Ïù¥ÌÉà ÏúÑÌóò')
        
        # Adoption - Onboarding difficulties (check regardless of rating)
        elif any(keyword in content for keyword in ['Ïñ¥Î†§ÏõÄ', 'Î≥µÏû°', 'Î™®Î•¥Í≤†', 'Ìó∑Í∞à', 'Ïñ¥ÎñªÍ≤å', 'ÏÑ§Î™ÖÎ∂ÄÏ°±', 'ÏÇ¨Ïö©Î≤ï', 'Í∞ÄÏù¥Îìú', 'ÎèÑÏõÄÎßê']):
            heart_analysis['adoption']['issues'].append(content)
            heart_analysis['adoption']['details'].append('ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú')
    
    # Generate insights based on actual review content analysis
    insights = []
    insight_id = 1
    
    # Business impact weights
    impact_weights = {
        'task_success': 5,  # Critical - core functionality
        'retention': 4,     # High - user churn
        'happiness': 3,     # Medium - satisfaction
        'engagement': 2,    # Low-Medium - usage
        'adoption': 1       # Low - onboarding
    }
    
    # Debug: Print heart analysis results
    print(f"Heart analysis results:", file=sys.stderr)
    for category, data in heart_analysis.items():
        print(f"  {category}: {len(data['issues'])} issues", file=sys.stderr)
        if data['issues']:
            print(f"    First issue: {data['issues'][0][:50]}...", file=sys.stderr)
    
    # Generate insights based on the analyzed HEART categories
    for category, data in heart_analysis.items():
        if data['issues']:
            count = len(data['issues'])
            impact_score = count * impact_weights[category]
            
            # Priority calculation (more lenient thresholds)
            if impact_score >= 10 or (category == 'task_success' and count >= 2):
                priority = "critical"
                priority_emoji = "üî¥"
            elif impact_score >= 4 or count >= 2:
                priority = "major"
                priority_emoji = "üü†"
            else:
                priority = "minor"
                priority_emoji = "üü¢"
            
            # Analyze actual review content to identify specific problems
            actual_issues = []
            for issue_text in data['issues']:
                # Extract key phrases and issues from actual reviews
                if 'ÌÅ¨ÎûòÏãú' in issue_text or 'Í∫ºÏ†∏' in issue_text or 'Í∫ºÏßÄ' in issue_text or 'ÌäïÍ≤®' in issue_text or 'ÌäïÍπÄ' in issue_text or 'ÎÇòÍ∞ÄÎ≤ÑÎ¶º' in issue_text:
                    actual_issues.append('Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å')
                elif ('Ï†ÑÌôî' in issue_text or 'ÌÜµÌôî' in issue_text) and ('ÎÅäÏñ¥' in issue_text or 'Î∞õ' in issue_text or 'ÏïàÎê®' in issue_text or 'ÎÅäÍπÄ' in issue_text):
                    actual_issues.append('ÌÜµÌôî Í∏∞Îä• Ïò§Î•ò')
                elif 'Ïó∞Í≤∞' in issue_text or 'ÎÑ§Ìä∏ÏõåÌÅ¨' in issue_text or 'Ï†ëÏÜç' in issue_text:
                    actual_issues.append('ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ Î¨∏Ï†ú')
                elif 'Î°úÍ∑∏Ïù∏' in issue_text or 'Ïù∏Ï¶ù' in issue_text or 'Î°úÍ∑∏' in issue_text:
                    actual_issues.append('Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú')
                elif 'ÏÇ≠Ï†ú' in issue_text or 'Ìï¥ÏßÄ' in issue_text or 'Í∑∏Îßå' in issue_text:
                    actual_issues.append('ÏÑúÎπÑÏä§ Ï§ëÎã® ÏùòÎèÑ')
                elif 'Î∂àÌé∏' in issue_text or 'Î≥µÏû°' in issue_text or 'Ïñ¥Î†§ÏõÄ' in issue_text:
                    actual_issues.append('ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú')
                else:
                    actual_issues.append('Í∏∞ÌÉÄ Î¨∏Ï†ú')
            
            # Find most common actual issue
            if actual_issues:
                most_common_issue = max(set(actual_issues), key=actual_issues.count)
                issue_count = actual_issues.count(most_common_issue)
            else:
                most_common_issue = 'Í∏∞ÌÉÄ Î¨∏Ï†ú'
                issue_count = count
            
            # Generate realistic problem prediction and solution based on HEART category
            predicted_problem = ""
            realistic_solution = ""
            
            if category == 'task_success':
                if most_common_issue == 'ÌÜµÌôî Í∏∞Îä• Ïò§Î•ò':
                    predicted_problem = "ÌÜµÌôî Ïó∞Í≤∞ Ïã§Ìå®Î°ú Ïù∏Ìïú ÌïµÏã¨ Í∏∞Îä• ÏàòÌñâ Î∂àÍ∞Ä"
                    realistic_solution = "ÌÜµÌôî Ïó∞Í≤∞ Î°úÏßÅ Ï†êÍ≤Ä, VoIP ÏÑúÎ≤Ñ ÏïàÏ†ïÏÑ± Í∞ïÌôî, ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉÅÌÉúÎ≥Ñ ÎåÄÏùë Î°úÏßÅ Í∞úÎ∞ú"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    predicted_problem = "Î°úÍ∑∏Ïù∏ Ïã§Ìå®Î°ú Ïù∏Ìïú ÏÑúÎπÑÏä§ Ï†ëÍ∑º Î∂àÍ∞Ä"
                    realistic_solution = "Ïù∏Ï¶ù ÏÑúÎ≤Ñ Î™®ÎãàÌÑ∞ÎßÅ Í∞ïÌôî, Îã§Ï§ë Ïù∏Ï¶ù Î∞©Ïãù Ï†úÍ≥µ, Î°úÍ∑∏Ïù∏ Ïã§Ìå® Ïãú Î™ÖÌôïÌïú ÏïàÎÇ¥ Î©îÏãúÏßÄ"
                elif most_common_issue == 'ÌÜµÌôîÏ§ëÎåÄÍ∏∞ Í∏∞Îä• Î∂ÄÏû¨':
                    predicted_problem = "ÌÜµÌôîÏ§ëÎåÄÍ∏∞ ÎØ∏ÏßÄÏõêÏúºÎ°ú Ïù∏Ìïú ÏóÖÎ¨¥ Ìö®Ïú®ÏÑ± Ï†ÄÌïò"
                    realistic_solution = "ÌÜµÌôîÏ§ëÎåÄÍ∏∞ Í∏∞Îä• Í∞úÎ∞ú, Î©ÄÌã∞ÌÉúÏä§ÌÇπ ÏßÄÏõê, ÏΩúÏÑºÌÑ∞ ÏãúÏä§ÌÖú Ïó∞Îèô"
                elif most_common_issue == 'Ïï†ÌîåÏõåÏπò Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    predicted_problem = "Ïõ®Ïñ¥Îü¨Î∏î Í∏∞Í∏∞ ÎØ∏ÏßÄÏõêÏúºÎ°ú Ïù∏Ìïú Ï†ëÍ∑ºÏÑ± Ï†úÌïú"
                    realistic_solution = "WatchOS Ïó∞Îèô Í∞úÎ∞ú, ÏõåÏπò Ï†ÑÏö© UI Íµ¨ÌòÑ, ÌïòÎìúÏõ®Ïñ¥ Î≤ÑÌäº ÏßÄÏõê"
                elif most_common_issue == 'Î∏îÎ£®Ìà¨Ïä§/ÏóêÏñ¥Ìåü Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    predicted_problem = "Ïò§ÎîîÏò§ Í∏∞Í∏∞ Ìò∏ÌôòÏÑ± Î¨∏Ï†úÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Í≤ΩÌóò Ï†ÄÌïò"
                    realistic_solution = "Î∏îÎ£®Ìà¨Ïä§ ÌîÑÎ°úÌååÏùº ÏßÄÏõê ÌôïÎåÄ, Ïò§ÎîîÏò§ ÏΩîÎç± ÏµúÏ†ÅÌôî, Í∏∞Í∏∞Î≥Ñ ÌÖåÏä§Ìä∏"
                elif most_common_issue == 'ÌÜµÌôîÏó∞Í≤∞Ïùå Í¥ÄÎ†® Î¨∏Ï†ú':
                    predicted_problem = "Ïó∞Í≤∞Ïùå Î≥ºÎ•®/ÏÑ§Ï†ï Î¨∏Ï†úÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Î∂àÌé∏"
                    realistic_solution = "Ïó∞Í≤∞Ïùå Í∞úÏù∏Ìôî Í∏∞Îä•, Î≥ºÎ•® Ï°∞Ï†à ÏòµÏÖò, Î¨¥Ïùå Î™®Îìú ÏßÄÏõê"
                elif most_common_issue == 'ÏïåÎú∞Ìè∞ ÏßÄÏõê Î¨∏Ï†ú':
                    predicted_problem = "MVNO ÎØ∏ÏßÄÏõêÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ï†ëÍ∑ºÏÑ± Ï†úÌïú"
                    realistic_solution = "ÏïåÎú∞Ìè∞ ÌÜµÏã†ÏÇ¨ ÏßÄÏõê ÌôïÎåÄ, Ïù∏Ï¶ù ÏãúÏä§ÌÖú Í∞úÏÑ†, Ìò∏ÌôòÏÑ± Í≤ÄÏ¶ù"
                elif most_common_issue == 'ÌäπÏ†ï Î≤àÌò∏ Í∏∞Î°ù ÎàÑÎùΩ Î¨∏Ï†ú':
                    predicted_problem = "ÌÜµÌôî Í∏∞Î°ù ÎàÑÎùΩÏúºÎ°ú Ïù∏Ìïú ÏóÖÎ¨¥ Ï∂îÏ†Å Ïñ¥Î†§ÏõÄ"
                    realistic_solution = "ÌÜµÌôî Í∏∞Î°ù DB ÏµúÏ†ÅÌôî, Î™®Îì† Î≤àÌò∏ ÌòïÌÉú ÏßÄÏõê, Ïã§ÏãúÍ∞Ñ Í∏∞Î°ù Í≤ÄÏ¶ù"
                elif most_common_issue == 'Î≥ºÎ•®Î≤ÑÌäº ÏßÑÎèô Ï†úÏñ¥ Î¨∏Ï†ú':
                    predicted_problem = "ÌïòÎìúÏõ®Ïñ¥ Î≤ÑÌäº Ï†úÏñ¥ Î¨∏Ï†úÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ï°∞Ïûë Î∂àÌé∏"
                    realistic_solution = "ÌïòÎìúÏõ®Ïñ¥ Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨ Í∞úÏÑ†, ÏßÑÎèô Ï†úÏñ¥ ÏòµÏÖò Ï∂îÍ∞Ä"
                elif most_common_issue == 'Î∞±Í∑∏ÎùºÏö¥Îìú Ïï± Ï¢ÖÎ£å Î¨∏Ï†ú':
                    predicted_problem = "ÌÜµÌôî Ï¢ÖÎ£å ÌõÑ Î∞±Í∑∏ÎùºÏö¥Îìú ÌîÑÎ°úÏÑ∏Ïä§ ÎØ∏Ï†ïÎ¶¨Î°ú Ïù∏Ìïú ÏãúÏä§ÌÖú Î¶¨ÏÜåÏä§ Ï†êÏú†"
                    realistic_solution = "ÌÜµÌôî Ï¢ÖÎ£å Ïãú Î∞±Í∑∏ÎùºÏö¥Îìú Ïï± ÏûêÎèô Ï¢ÖÎ£å, Ïò§ÎîîÏò§ ÏÑ∏ÏÖò Í¥ÄÎ¶¨ Í∞úÏÑ†"
                elif most_common_issue == 'Ïä§Ìå∏ Ï†ïÎ≥¥ ÌëúÏãú Î¨∏Ï†ú':
                    predicted_problem = "Ïä§Ìå∏ Ï†ïÎ≥¥ Ïä¨ÎùºÏù¥Îìú ÌëúÏãúÎ°ú Ïù∏Ìïú Î≤àÌò∏ ÌôïÏù∏ ÏßÄÏó∞"
                    realistic_solution = "Ïä§Ìå∏ Ï†ïÎ≥¥ ÌëúÏãú UI Í∞úÏÑ†, Î≤àÌò∏ Ïö∞ÏÑ† ÌëúÏãú ÏòµÏÖò Ï†úÍ≥µ"
                elif most_common_issue == 'UI Ïä¨ÎùºÏù¥Îìú ÌëúÏãú Î¨∏Ï†ú':
                    predicted_problem = "ÌÖçÏä§Ìä∏ Ïä¨ÎùºÏù¥Îìú Ïï†ÎãàÎ©îÏù¥ÏÖòÏúºÎ°ú Ïù∏Ìïú Ï†ïÎ≥¥ ÌôïÏù∏ ÏßÄÏó∞"
                    realistic_solution = "Ïä¨ÎùºÏù¥Îìú ÏÜçÎèÑ Ï°∞Ï†à, Ï†ïÏ†Å ÌëúÏãú Î™®Îìú ÏòµÏÖò Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÏÇ¨Ïö©Ïûê Í≤ΩÌóò ÌòºÎûÄ':
                    predicted_problem = "ÏòàÏÉÅÏπò Î™ªÌïú Ïï± ÎèôÏûëÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê ÌòºÎûÄ Î∞è Ïä§Ìä∏Î†àÏä§"
                    realistic_solution = "ÏßÅÍ¥ÄÏ†ÅÏù∏ UI/UX Ïû¨ÏÑ§Í≥Ñ, ÏÇ¨Ïö©Ïûê Í∞ÄÏù¥Îìú Í∞úÏÑ†"
                elif most_common_issue == 'Ï∞®Îüâ Î∏îÎ£®Ìà¨Ïä§ Ïó∞Îèô Î¨∏Ï†ú':
                    predicted_problem = "Ï∞®Îüâ Î∏îÎ£®Ìà¨Ïä§ Ïó∞Îèô Î∂àÏïàÏ†ïÏúºÎ°ú Ïù∏Ìïú ÏùåÏÑ± ÌÜµÌôî ÌõÑ Ïò§ÎîîÏò§ ÏÑ∏ÏÖò Î¨∏Ï†ú"
                    realistic_solution = "Ï∞®Îüâ Î∏îÎ£®Ìà¨Ïä§ Ìò∏ÌôòÏÑ± Í∞úÏÑ†, Ïò§ÎîîÏò§ ÏÑ∏ÏÖò Ï†ïÎ¶¨ ÏûêÎèôÌôî"
                elif most_common_issue == 'Ï†ÑÌôî ÏàòÏã† Î∂àÍ∞Ä Î¨∏Ï†ú':
                    predicted_problem = "Ï†ÑÌôî ÏàòÏã† Ïã§Ìå®Î°ú Ïù∏Ìïú Ï§ëÏöî ÌÜµÌôî ÎàÑÎùΩ ÏúÑÌóò"
                    realistic_solution = "ÏàòÏã† ÏïåÍ≥†Î¶¨Ï¶ò Í∞úÏÑ†, ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉÅÌÉú Ï≤¥ÌÅ¨ Í∞ïÌôî"
                else:
                    predicted_problem = "ÌïµÏã¨ Í∏∞Îä• Ïò§Î•òÎ°ú Ïù∏Ìïú ÏûëÏóÖ ÏôÑÎ£å Î∂àÍ∞Ä"
                    realistic_solution = "Í∏∞Îä•Î≥Ñ ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî, Ïò§Î•ò Î∞úÏÉù Ïãú Î≥µÍµ¨ Î©îÏª§ÎãàÏ¶ò Íµ¨Ï∂ï"
                    
            elif category == 'happiness':
                if most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    predicted_problem = "ÏßÅÍ¥ÄÏ†ÅÏù¥ÏßÄ ÏïäÏùÄ UI/UXÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ïä§Ìä∏Î†àÏä§"
                    realistic_solution = "ÏÇ¨Ïö©Ïûê ÌÖåÏä§Ìä∏ Ïã§Ïãú, ÎÑ§ÎπÑÍ≤åÏù¥ÏÖò Íµ¨Ï°∞ Îã®ÏàúÌôî, Ï£ºÏöî Í∏∞Îä• Ï†ëÍ∑ºÏÑ± Í∞úÏÑ†"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò':
                    predicted_problem = "Ïï± Î°úÎî© ÏßÄÏó∞ÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê ÎãµÎãµÌï®"
                    realistic_solution = "ÏΩîÎìú ÏµúÏ†ÅÌôî, Ïù¥ÎØ∏ÏßÄ ÏïïÏ∂ï, Ï∫êÏã± Ï†ÑÎûµ Í∞úÏÑ†, Î°úÎî© Ïù∏ÎîîÏºÄÏù¥ÌÑ∞ Ï∂îÍ∞Ä"
                else:
                    predicted_problem = "ÏÇ¨Ïö©Ïûê Í∏∞ÎåÄÏôÄ Ïã§Ï†ú Í≤ΩÌóò Í∞ÑÏùò Í¥¥Î¶¨"
                    realistic_solution = "ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± Ï†ïÍ∏∞ ÏàòÏßë, ÌïµÏã¨ Î∂àÎßå ÏÇ¨Ìï≠ Ïö∞ÏÑ† Ìï¥Í≤∞, UX Í∞úÏÑ† ÌîÑÎ°úÏÑ∏Ïä§ Íµ¨Ï∂ï"
                    
            elif category == 'engagement':
                if most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    predicted_problem = "Î≥µÏû°Ìïú Í∏∞Îä• Íµ¨Ï°∞Î°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ï∞∏Ïó¨ÎèÑ Í∞êÏÜå"
                    realistic_solution = "ÌïµÏã¨ Í∏∞Îä• Ï†ëÍ∑ºÏÑ± Í∞úÏÑ†, Í∞úÏù∏Ìôî ÏïåÎ¶º ÏÑ§Ï†ï, ÏÇ¨Ïö© Ìå®ÌÑ¥ Î∂ÑÏÑù Í∏∞Î∞ò Í∏∞Îä• Ï∂îÏ≤ú"
                else:
                    predicted_problem = "Ïû¨Î∞©Î¨∏ ÎèôÍ∏∞ Î∂ÄÏ°±ÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö© ÎπàÎèÑ Ï†ÄÌïò"
                    realistic_solution = "Ìë∏Ïãú ÏïåÎ¶º Í∞úÏù∏Ìôî, ÏÇ¨Ïö©ÏûêÎ≥Ñ ÎßûÏ∂§ ÏΩòÌÖêÏ∏† Ï†úÍ≥µ, Ï†ïÍ∏∞Ï†Å ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Ïù¥Î≤§Ìä∏ ÏßÑÌñâ"
                    
            elif category == 'retention':
                if most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å':
                    predicted_problem = "Ïï± ÏïàÏ†ïÏÑ± Î¨∏Ï†úÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ïù¥ÌÉà"
                    realistic_solution = "ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î≤ÑÍ∑∏ ÏàòÏ†ï, ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî, Í∏¥Í∏â Ìå®Ïπò ÌîÑÎ°úÏÑ∏Ïä§ Íµ¨Ï∂ï"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    predicted_problem = "Î∞òÎ≥µÏ†ÅÏù∏ Î°úÍ∑∏Ïù∏ Ïã§Ìå®Î°ú Ïù∏Ìïú Ïû¨Î∞©Î¨∏Ïú® Í∞êÏÜå"
                    realistic_solution = "ÏûêÎèô Î°úÍ∑∏Ïù∏ Í∏∞Îä• Í∞úÏÑ†, ÏÜåÏÖú Î°úÍ∑∏Ïù∏ Ïó∞Îèô, ÎπÑÎ∞ÄÎ≤àÌò∏ Ï∞æÍ∏∞ ÌîÑÎ°úÏÑ∏Ïä§ Í∞ÑÏÜåÌôî"
                else:
                    predicted_problem = "ÏßÄÏÜçÏ†ÅÏù∏ Í∞ÄÏπò Ï†úÍ≥µ Ïã§Ìå®Î°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ïù¥ÌÉà"
                    realistic_solution = "ÏÇ¨Ïö©Ïûê ÏÉùÎ™ÖÏ£ºÍ∏∞Î≥Ñ ÎßûÏ∂§ ÏÑúÎπÑÏä§ Ï†úÍ≥µ, Ïû¨Î∞©Î¨∏ Ïú†ÎèÑ ÏïåÎ¶º ÏµúÏ†ÅÌôî"
                    
            elif category == 'adoption':
                if most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    predicted_problem = "Î≥µÏû°Ìïú Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î°ú Ïù∏Ìïú Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ïò®Î≥¥Îî© Ïù¥ÌÉà"
                    realistic_solution = "Ïò®Î≥¥Îî© ÌîåÎ°úÏö∞ Îã®ÏàúÌôî, Îã®Í≥ÑÎ≥Ñ Í∞ÄÏù¥Îìú Ï†úÍ≥µ, ÌïÑÏàò Í∏∞Îä• Ï§ëÏã¨ ÌäúÌÜ†Î¶¨Ïñº Íµ¨ÏÑ±"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    predicted_problem = "Ïù∏Ï¶ù Ïò§Î•òÎ°ú Ïù∏Ìïú Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ïú†ÏûÖ Ïã§Ìå®"
                    realistic_solution = "Í∞ÑÌé∏ ÌöåÏõêÍ∞ÄÏûÖ ÏòµÏÖò Ï†úÍ≥µ, Ïù∏Ï¶ù Í≥ºÏ†ï ÏµúÏÜåÌôî, ÏÑ§Ï†ï ÏûêÎèôÌôî Í∏∞Îä• Í∞ïÌôî"
                else:
                    predicted_problem = "ÌïµÏã¨ Í∞ÄÏπò Ïù¥Ìï¥ Î∂ÄÏ°±ÏúºÎ°ú Ïù∏Ìïú Ï¥àÍ∏∞ Ïù¥ÌÉàÎ•† Ï¶ùÍ∞Ä"
                    realistic_solution = "ÌïµÏã¨ Í∏∞Îä• Ïö∞ÏÑ† ÎÖ∏Ï∂ú, ÏÇ¨Ïö©Ïûê Ïú†ÌòïÎ≥Ñ ÎßûÏ∂§ Ïò®Î≥¥Îî©, Ï≤´ ÏÑ±Í≥µ Í≤ΩÌóò Î≥¥Ïû•"
            
            # Extract actual user quotes from reviews for more authentic problem descriptions
            user_quotes = []
            for issue_text in data['issues'][:3]:  # Get first 3 issues for quotes
                # Extract meaningful quotes (first 50 chars)
                if len(issue_text) > 50:
                    quote = issue_text[:50] + "..."
                else:
                    quote = issue_text
                user_quotes.append(f'"{quote}"')
            
            quotes_text = " / ".join(user_quotes) if user_quotes else "ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± Î∂ÑÏÑù Í≤∞Í≥º"
            
            # Create more detailed, UX-researcher style description
            heart_category_ko = {
                'task_success': 'ÌïµÏã¨ Í∏∞Îä• ÏàòÌñâ',
                'happiness': 'ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ', 
                'engagement': 'ÏÇ¨Ïö©Ïûê Ï∞∏Ïó¨ÎèÑ',
                'retention': 'ÏÇ¨Ïö©Ïûê Ïú†ÏßÄÏú®',
                'adoption': 'Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ï†ÅÏùë'
            }
            
            # Generate specific UX improvement examples based on the category and issues
            ux_improvement_examples = generate_ux_improvement_points(category, most_common_issue, data['issues'])
            
            # Generate UX-focused improvement suggestions based on actual user review content
            ux_improvement_suggestions = generate_realistic_ux_suggestions(category, most_common_issue, data['issues'], predicted_problem, quotes_text)
            
            description = f"""**HEART Ìï≠Î™©**: {category}
**Î¨∏Ï†ú ÏöîÏïΩ**: {quotes_text}ÏóêÏÑú ÎìúÎü¨ÎÇòÎäî {predicted_problem}
**UX Í∞úÏÑ† Ï†úÏïà**: {ux_improvement_suggestions}
**Ïö∞ÏÑ†ÏàúÏúÑ**: {priority.upper()}"""

            insights.append({
                'id': insight_id,
                'title': f"{priority_emoji} {priority.title()} | HEART: {category} | {most_common_issue} ({count}Í±¥)",
                'description': description,
                'priority': priority,
                'mentionCount': count,
                'trend': 'stable',
                'category': category
            })
            insight_id += 1
    
    # Sort by priority and impact
    priority_order = {'critical': 3, 'major': 2, 'minor': 1}
    insights.sort(key=lambda x: (priority_order[x['priority']], x['mentionCount']), reverse=True)
    
    # Limit to top 5 insights
    insights = insights[:5]
    
    # Enhanced Korean word frequency analysis using advanced processing
    positive_texts = [r['content'] for r in reviews if r.get('sentiment') == 'positive']
    negative_texts = [r['content'] for r in reviews if r.get('sentiment') == 'negative']
    
    # Use advanced Korean processing to extract meaningful words
    positive_cloud = extract_korean_words_advanced(positive_texts, 'positive', 10)
    negative_cloud = extract_korean_words_advanced(negative_texts, 'negative', 10)
    
    print(f"Generated {len(insights)} HEART insights, {len(positive_cloud)} positive words, {len(negative_cloud)} negative words", file=sys.stderr)
    
    return {
        'insights': insights,
        'wordCloud': {
            'positive': positive_cloud,
            'negative': negative_cloud
        }
    }

def generate_ux_improvement_points(category, issue_type, issues):
    """
    Generate specific UX improvement examples based on HEART category and issue type
    """
    # Sample some issues for context
    sample_issues = issues[:3] if len(issues) > 3 else issues
    
    if category == 'task_success':
        if 'ÌÜµÌôî' in issue_type or 'Ï†ÑÌôî' in issue_type:
            return """üì± ÌÜµÌôî ÌíàÏßà ÏãúÍ∞ÅÌôî ÎåÄÏãúÎ≥¥Îìú: Ïã§ÏãúÍ∞Ñ ÌÜµÌôî ÌíàÏßà ÌëúÏãú (Ïã†Ìò∏ Í∞ïÎèÑ, ÏßÄÏó∞ÏãúÍ∞Ñ, ÏùåÏÑ± ÌíàÏßà)
üîÑ ÏõêÌÑ∞Ïπò Ïû¨Ïó∞Í≤∞ Î≤ÑÌäº: ÌÜµÌôî ÎÅäÍπÄ Ïãú Ï¶âÏãú Ïû¨Ïó∞Í≤∞ Í∞ÄÎä•Ìïú ÌîåÎ°úÌåÖ Î≤ÑÌäº Ï∂îÍ∞Ä
‚ö†Ô∏è ÌÜµÌôî Ï†Ñ ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉÅÌÉú Ï≤¥ÌÅ¨: ÌÜµÌôî ÏãúÏûë Ï†Ñ Ïó∞Í≤∞ ÌíàÏßà ÎØ∏Î¶¨ ÏïåÎ¶º (Îπ®Í∞Ñ/ÎÖ∏ÎûÄ/Ï¥àÎ°ù ÏïÑÏù¥ÏΩò)
üéöÔ∏è Î≥ºÎ•® Ïª®Ìä∏Î°§ Í∞úÏÑ†: ÌÜµÌôî Ï§ë Î≥ºÎ•® Ï°∞Ï†à Ïãú ÏßÑÎèô/Î≤®ÏÜåÎ¶¨ ÏûêÎèô Ï†ïÏßÄ ÌÜ†Í∏Ä"""
        elif 'CCTV' in issue_type or 'ÌôîÎ©¥' in issue_type:
            return """üì± ÌïÄÏπò Ï§å Ï†úÏä§Ï≤ò ÌôúÏÑ±Ìôî: Îëê ÏÜêÍ∞ÄÎùΩÏúºÎ°ú ÌôïÎåÄ/Ï∂ïÏÜå Í∞ÄÎä•Ìïú ÏßÅÍ¥ÄÏ†Å Ïù∏ÌÑ∞ÌéòÏù¥Ïä§
üñ•Ô∏è Î©ÄÌã∞ Î∑∞ Î™®Îìú: 4Î∂ÑÌï†/9Î∂ÑÌï† ÌôîÎ©¥ÏúºÎ°ú Ïó¨Îü¨ Ïπ¥Î©îÎùº ÎèôÏãú Î™®ÎãàÌÑ∞ÎßÅ
üìã Ï¶êÍ≤®Ï∞æÍ∏∞ Ïπ¥Î©îÎùº: ÏûêÏ£º ÌôïÏù∏ÌïòÎäî Ïπ¥Î©îÎùºÎ•º ÏÉÅÎã®Ïóê Í≥†Ï†ï ÌëúÏãú
üîÑ ÏûêÎèô ÏÉàÎ°úÍ≥†Ïπ® Í∞ÑÍ≤© ÏÑ§Ï†ï: 1Ï¥à/3Ï¥à/5Ï¥à ÏûêÎèô Í∞±Ïã† ÏòµÏÖò"""
        elif 'Ïï±' in issue_type and 'ÌäïÍπÄ' in issue_type:
            return """üõ°Ô∏è Ïï± ÏïàÏ†ïÏÑ± Î™®ÎãàÌÑ∞ÎßÅ: ÌÅ¨ÎûòÏãú Î∞úÏÉù Ïãú ÏûêÎèô Ïû¨ÏãúÏûë Î∞è Ïù¥Ï†Ñ ÏÉÅÌÉú Î≥µÏõê
üíæ ÏÑ∏ÏÖò ÏûêÎèô Ï†ÄÏû•: 5Ï¥àÎßàÎã§ ÏÇ¨Ïö©Ïûê ÏÉÅÌÉú ÏûêÎèô Ï†ÄÏû•ÏúºÎ°ú Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ Î∞©ÏßÄ
üîÑ Ïò§ÌîÑÎùºÏù∏ Î™®Îìú: ÎÑ§Ìä∏ÏõåÌÅ¨ Î∂àÏïàÏ†ï Ïãú Ï∫êÏãúÎêú Îç∞Ïù¥ÌÑ∞Î°ú Í∏∞Î≥∏ Í∏∞Îä• Ïú†ÏßÄ
‚ö° Í≤ΩÎüâ Î™®Îìú: Ï†ÄÏÇ¨Ïñë Í∏∞Í∏∞Ïö© Îã®ÏàúÌôîÎêú UI Î∞è Í∏∞Îä• Ï†úÍ≥µ"""
        else:
            return """üéØ ÏûëÏóÖ ÏôÑÎ£å Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏: ÏÇ¨Ïö©ÏûêÍ∞Ä ÏàòÌñâÌï¥Ïïº Ìï† Îã®Í≥ÑÎ≥Ñ Í∞ÄÏù¥Îìú Ï†úÍ≥µ
üìä ÏßÑÌñâÎ•† ÌëúÏãú: ÏûëÏóÖ ÏôÑÎ£åÎèÑÎ•º ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú ÌëúÏãúÌïòÎäî ÌîÑÎ°úÍ∑∏Î†àÏä§ Î∞î
üîç Ïã§ÏãúÍ∞Ñ Ïò§Î•ò Í∞êÏßÄ: Î¨∏Ï†ú Î∞úÏÉù Ïãú Ï¶âÏãú ÏïåÎ¶º Î∞è Ìï¥Í≤∞ Î∞©Ïïà Ï†úÏãú
‚ö° Îπ†Î•∏ Ïï°ÏÑ∏Ïä§ Î©îÎâ¥: ÏûêÏ£º ÏÇ¨Ïö©ÌïòÎäî Í∏∞Îä•ÏùÑ Ìôà ÌôîÎ©¥Ïóê Î∞îÎ°úÍ∞ÄÍ∏∞ Ï†úÍ≥µ"""
    
    elif category == 'happiness':
        return """üòä ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± Ïã§ÏãúÍ∞Ñ Î∞òÏòÅ: Ïï± ÎÇ¥ Í∞ÑÎã®Ìïú ÎßåÏ°±ÎèÑ ÌèâÍ∞Ä (üëç/üëé) Î≤ÑÌäº
üé® Í∞úÏù∏Ìôî ÌÖåÎßà: ÏÇ¨Ïö©Ïûê ÏÑ†Ìò∏Ïóê Îî∞Î•∏ ÏÉâÏÉÅ/Ìè∞Ìä∏ Ïª§Ïä§ÌÑ∞ÎßàÏù¥Ïßï ÏòµÏÖò  
üèÜ ÏÑ±Ï∑®Í∞ê Ï†úÍ≥µ: Í∏∞Îä• ÏÇ¨Ïö© Ïãú ÏûëÏùÄ Ïï†ÎãàÎ©îÏù¥ÏÖòÍ≥º ÏÑ±Í≥µ Î©îÏãúÏßÄ ÌëúÏãú
üì± ÏÇ¨Ïö© Í∞ÄÏù¥Îìú Ìà¥ÌåÅ: ÏÉàÎ°úÏö¥ Í∏∞Îä• ÏÇ¨Ïö© Ïãú ÏπúÍ∑ºÌïú ÏïàÎÇ¥ ÎßêÌíçÏÑ† Ï†úÍ≥µ
üîî Í∏çÏ†ïÏ†Å ÏïåÎ¶º: 'Ïò§Îäò ÌÜµÌôî ÌíàÏßàÏù¥ Ï¢ãÏïòÏäµÎãàÎã§' Í∞ôÏùÄ Í≤©Î†§ Î©îÏãúÏßÄ"""
    
    elif category == 'engagement':
        return """üìà ÏÇ¨Ïö© ÌÜµÍ≥Ñ ÏãúÍ∞ÅÌôî: Ï£ºÍ∞Ñ/ÏõîÍ∞Ñ ÏÇ¨Ïö© Ìå®ÌÑ¥ÏùÑ ÏòàÏÅú Ï∞®Ìä∏Î°ú ÌëúÏãú
üéØ Í∞úÏù∏ Î™©Ìëú ÏÑ§Ï†ï: ÌÜµÌôî ÏãúÍ∞Ñ, Ïï± ÏÇ¨Ïö© ÎπàÎèÑ Îì± Í∞úÏù∏ Î™©Ìëú ÏÑ§Ï†ï Í∏∞Îä•
üîî Ïä§ÎßàÌä∏ ÏïåÎ¶º: ÏÇ¨Ïö©Ïûê Ìå®ÌÑ¥ Í∏∞Î∞ò ÎßûÏ∂§Ìòï ÏïåÎ¶º (Ï†êÏã¨ÏãúÍ∞Ñ, Ìá¥Í∑ºÏãúÍ∞Ñ Îì±)
üéÅ ÏÇ¨Ïö© Î≥¥ÏÉÅ: Ïó∞ÏÜç ÏÇ¨Ïö©ÏùºÏàòÏóê Îî∞Î•∏ ÏûëÏùÄ ÌòúÌÉù Ï†úÍ≥µ
üì± ÏúÑÏ†Ø Ï†úÍ≥µ: Ìôà ÌôîÎ©¥ÏóêÏÑú Î∞îÎ°ú ÌôïÏù∏ Í∞ÄÎä•Ìïú Í∞ÑÎã®Ìïú Ï†ïÎ≥¥ ÌëúÏãú"""
    
    elif category == 'retention':
        return """üîÑ ÏÇ¨Ïö© Ïù¥Î†• Î∞±ÏóÖ: ÌÅ¥ÎùºÏö∞Îìú ÎèôÍ∏∞ÌôîÎ°ú Í∏∞Í∏∞ Î≥ÄÍ≤Ω ÏãúÏóêÎèÑ ÏÑ§Ï†ï Ïú†ÏßÄ
üìä Í∞úÏù∏Ìôî ÎåÄÏãúÎ≥¥Îìú: ÏÇ¨Ïö©ÏûêÎ≥Ñ ÎßûÏ∂§ Ï†ïÎ≥¥ Î∞∞Ïπò Î∞è ÏûêÏ£º Ïì∞Îäî Í∏∞Îä• Ïö∞ÏÑ† ÌëúÏãú
üéØ Îã®Í≥ÑÎ≥Ñ Ïò®Î≥¥Îî©: Ïã†Í∑ú ÏÇ¨Ïö©ÏûêÎ•º ÏúÑÌïú ÏπúÍ∑ºÌïú 3Îã®Í≥Ñ Í∞ÄÏù¥Îìú Ìà¨Ïñ¥
‚ö° Îπ†Î•∏ Î≥µÍµ¨: Ïï± ÏÇ≠Ï†ú ÌõÑ Ïû¨ÏÑ§Ïπò Ïãú Í∏∞Ï°¥ ÏÑ§Ï†ï 1Ï¥à Î≥µÏõê Í∏∞Îä•
üîî Ïû¨Î∞©Î¨∏ Ïú†ÎèÑ: Î©∞Ïπ† ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùÑ Ïãú Ïú†Ïö©Ìïú Í∏∞Îä• ÏÜåÍ∞ú ÏïåÎ¶º"""
    
    elif category == 'adoption':
        return """üöÄ 3Î∂Ñ ÌÄµ Ïä§ÌÉÄÌä∏: ÌïµÏã¨ Í∏∞Îä• 3Í∞úÎßå Ï≤¥ÌóòÌï¥Î≥¥Îäî Í∞ÑÎã®Ìïú ÌäúÌÜ†Î¶¨Ïñº
üì± Î¨¥Î£å Ï≤¥Ìóò: ÌîÑÎ¶¨ÎØ∏ÏóÑ Í∏∞Îä• 7Ïùº Î¨¥Î£å Ï≤¥Ìóò ÌõÑ ÌïÑÏöîÏãú ÏóÖÍ∑∏Î†àÏù¥Îìú
üéØ Î™©Ï†ÅÎ≥Ñ ÏÑ§Ï†ï: 'ÌÜµÌôîÏö©', 'CCTVÏö©', 'Ï¢ÖÌï©Í¥ÄÎ¶¨Ïö©' Îì± ÏÇ¨Ïö© Î™©Ï†ÅÏóê Îî∞Î•∏ Ï¥àÍ∏∞ ÏÑ§Ï†ï
üìû Ïã§ÏãúÍ∞Ñ Ìó¨ÌîÑ: ÎßâÌûàÎäî Î∂ÄÎ∂Ñ ÏûàÏùÑ Îïå Ï±ÑÌåÖÏúºÎ°ú Ï¶âÏãú ÎèÑÏõÄ Î∞õÍ∏∞
üèÉ ÏõêÌÅ¥Î¶≠ ÏãúÏûë: Î≥µÏû°Ìïú ÏÑ§Ï†ï ÏóÜÏù¥ Î∞îÎ°ú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú 'Îπ†Î•∏ ÏãúÏûë' Î™®Îìú"""
    
    else:
        return """üéØ ÏÇ¨Ïö©Ïûê Ï§ëÏã¨ Í∞úÏÑ†: Ïã§Ï†ú ÏÇ¨Ïö© Ìå®ÌÑ¥ Î∂ÑÏÑù Í∏∞Î∞ò UI/UX ÏµúÏ†ÅÌôî
üì± Ï†ëÍ∑ºÏÑ± Ìñ•ÏÉÅ: ÌÅ∞ Î≤ÑÌäº, Î™ÖÌôïÌïú ÎùºÎ≤®, ÏßÅÍ¥ÄÏ†ÅÏù∏ ÏïÑÏù¥ÏΩò ÏÇ¨Ïö©
üîÑ ÌîºÎìúÎ∞± Î£®ÌîÑ: ÏÇ¨Ïö©Ïûê ÏùòÍ≤¨ ÏàòÏßë ‚Üí Í∞úÏÑ† ‚Üí Í≤∞Í≥º Í≥µÏú† ÏàúÌôò Íµ¨Ï°∞
‚ö° ÏÑ±Îä• ÏµúÏ†ÅÌôî: Î°úÎî© ÏãúÍ∞Ñ Îã®Ï∂ï Î∞è Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Í∞úÏÑ†"""

def generate_technical_implementation(category, issue_type, issues, problem_description):
    """
    Generate specific technical implementation based on actual user issues
    """
    # Sample issues for context
    sample_issues = issues[:5] if len(issues) > 5 else issues
    
    if category == 'task_success':
        if 'ÌÜµÌôî' in issue_type or 'Ï†ÑÌôî' in issue_type:
            return """üîß ÌÜµÌôî Ïó∞Í≤∞ Ïã§Ìå® Ïû¨ÌòÑ: ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉÅÌÉúÎ≥Ñ ÌÜµÌôî ÏãúÎèÑ ÏºÄÏù¥Ïä§ 100Í∞ú ÌÖåÏä§Ìä∏
üìä VoIP ÏÑúÎ≤Ñ Î™®ÎãàÌÑ∞ÎßÅ: Ïó∞Í≤∞ ÏÑ±Í≥µÎ•†, ÏùëÎãµ ÏãúÍ∞Ñ, Ìå®ÌÇ∑ ÏÜêÏã§Î•† Ïã§ÏãúÍ∞Ñ Ìä∏ÎûòÌÇπ
üîç ÌÜµÌôî ÌíàÏßà Î°úÍπÖ: ÏùåÏÑ± ÏΩîÎç±, ÏßÄÏó∞ÏãúÍ∞Ñ, ÏóêÏΩî Ï†úÍ±∞ ÏÑ±Îä• Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
‚ö° ÏûêÎèô Ïû¨Ïó∞Í≤∞ ÏïåÍ≥†Î¶¨Ï¶ò: Ïó∞Í≤∞ Ïã§Ìå® Ïãú 3Ï¥à/10Ï¥à/30Ï¥à Í∞ÑÍ≤©ÏúºÎ°ú Ïû¨ÏãúÎèÑ Î°úÏßÅ Íµ¨ÌòÑ"""
        elif 'CCTV' in issue_type or 'ÌôîÎ©¥' in issue_type:
            return """üîß ÌôîÎ©¥ ÌôïÎåÄ Ï†úÏä§Ï≤ò ÎùºÏù¥Î∏åÎü¨Î¶¨ Ï†ÅÏö©: PinchGestureRecognizer Íµ¨ÌòÑ
üìä ÏòÅÏÉÅ Ïä§Ìä∏Î¶¨Î∞ç ÏµúÏ†ÅÌôî: Ìï¥ÏÉÅÎèÑÎ≥Ñ ÏïïÏ∂ïÎ•† Ï°∞Ï†ï Î∞è Î≤ÑÌçºÎßÅ Í∞úÏÑ†
üîç Î©ÄÌã∞ Ïπ¥Î©îÎùº Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨: ÎπÑÌôúÏÑ± Î∑∞ Î¶¨ÏÜåÏä§ Ìï¥Ï†ú Î∞è Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò ÏµúÏ†ÅÌôî
‚ö° Ïã§ÏãúÍ∞Ñ Ïä§Ìä∏Î¶º Ï∫êÏã±: 3Ï¥à Î∞± Î≤ÑÌçºÎßÅÏúºÎ°ú ÎÑ§Ìä∏ÏõåÌÅ¨ ÎÅäÍπÄ Ïãú Î¨¥Ï§ëÎã® Ïû¨ÏÉù"""
        elif 'Ïï±' in issue_type and ('ÌäïÍπÄ' in issue_type or 'ÎÇòÍ∞ÄÎ≤ÑÎ¶º' in issue_type):
            return """üîß ÌÅ¨ÎûòÏãú Ïû¨ÌòÑ ÏãúÎÇòÎ¶¨Ïò§ ÏûëÏÑ±: ÏÇ¨Ïö©Ïûê ÏÑ†ÌÉù Îã®Í≥ÑÎ≥Ñ Î©îÎ™®Î¶¨ ÎàÑÏàò Ï∂îÏ†Å
üìä Ïã§ÏãúÍ∞Ñ ÌÅ¨ÎûòÏãú Î¶¨Ìè¨ÌåÖ: Firebase Crashlytics ÎèÑÏûÖÏúºÎ°ú Ïä§ÌÉù Ìä∏Î†àÏù¥Ïä§ ÏûêÎèô ÏàòÏßë
üîç Ïï± ÏÉÅÌÉú Î≥µÏõê ÏãúÏä§ÌÖú: SharedPreferences/UserDefaults ÌôúÏö© ÏÑ∏ÏÖò ÏûêÎèô Ï†ÄÏû•
‚ö° Î©îÎ™®Î¶¨ Ïò§Î≤ÑÌîåÎ°úÏö∞ Î∞©ÏßÄ: Ïù¥ÎØ∏ÏßÄ Î°úÎî© Ïãú Î©îÎ™®Î¶¨ ÌíÄ Í¥ÄÎ¶¨ Î∞è ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨"""
        elif 'Î°úÍ∑∏Ïù∏' in issue_type or 'Ïù∏Ï¶ù' in issue_type:
            return """üîß Ïù∏Ï¶ù Ïã§Ìå® ÏºÄÏù¥Ïä§ Î∂ÑÏÑù: Í∏∞Í∏∞Î≥Ñ, OSÎ≥Ñ Î°úÍ∑∏Ïù∏ ÏãúÎèÑ Î°úÍ∑∏ ÏàòÏßë
üìä ÌÜ†ÌÅ∞ Í∞±Ïã† Î°úÏßÅ Í∞úÏÑ†: JWT ÎßåÎ£å 10Î∂Ñ Ï†Ñ ÏûêÎèô Í∞±Ïã† Íµ¨ÌòÑ
üîç OAuth Ïó∞Îèô ÎîîÎ≤ÑÍπÖ: Ï†ú3Ïûê Ïù∏Ï¶ù ÏùëÎãµ ÏãúÍ∞Ñ Î∞è ÏóêÎü¨ ÏΩîÎìú Î∂ÑÏÑù
‚ö° Ïò§ÌîÑÎùºÏù∏ Ïù∏Ï¶ù Ï∫êÏã±: ÎßàÏßÄÎßâ ÏÑ±Í≥µ Ïù∏Ï¶ù Ï†ïÎ≥¥ ÏïîÌò∏Ìôî Ï†ÄÏû•"""
        else:
            return """üîß ÌïµÏã¨ Í∏∞Îä• Îã®ÏúÑ ÌÖåÏä§Ìä∏: Ï£ºÏöî ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏûêÎèôÌôî ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ 100Í∞ú ÏûëÏÑ±
üìä ÏÑ±Îä• ÏßÄÌëú Î™®ÎãàÌÑ∞ÎßÅ: ÏùëÎãµ ÏãúÍ∞Ñ, Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ, CPU Ï†êÏú†Ïú® Ïã§ÏãúÍ∞Ñ Ï∂îÏ†Å
üîç ÏóêÎü¨ Ìä∏ÎûòÌÇπ ÏãúÏä§ÌÖú: Sentry ÎèÑÏûÖÏúºÎ°ú Ïã§ÏãúÍ∞Ñ Î≤ÑÍ∑∏ Î¶¨Ìè¨ÌåÖ Î∞è ÏïåÎ¶º
‚ö° Í∏∞Îä•Î≥Ñ Î°§Î∞± ÏãúÏä§ÌÖú: Î¨∏Ï†ú Î∞úÏÉù Ïãú Ïù¥Ï†Ñ ÏïàÏ†ï Î≤ÑÏ†ÑÏúºÎ°ú Ï¶âÏãú Î≥µÍµ¨"""
    
    elif category == 'happiness':
        return """üîß ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ Ï∏°Ï†ï: Ïï± ÎÇ¥ NPS Ï†êÏàò ÏàòÏßë Î∞è ÌîºÎìúÎ∞± Î∂ÑÏÑù ÏãúÏä§ÌÖú
üìä Í∞êÏ†ï Î∂ÑÏÑù API: Î¶¨Î∑∞ ÌÖçÏä§Ìä∏ Í∞êÏ†ï Î∂ÑÏÑùÏúºÎ°ú Î∂àÎßå ÌÇ§ÏõåÎìú ÏûêÎèô Ï∂îÏ∂ú
üîç ÏÇ¨Ïö©Ïûê ÌñâÎèô Î∂ÑÏÑù: ÌûàÌä∏Îßµ Ìà¥ ÎèÑÏûÖÏúºÎ°ú UI ÏÇ¨Ïö© Ìå®ÌÑ¥ ÏãúÍ∞ÅÌôî
‚ö° Í∞úÏù∏Ìôî ÏïåÍ≥†Î¶¨Ï¶ò: ÏÇ¨Ïö© Ìå®ÌÑ¥ Í∏∞Î∞ò ÎßûÏ∂§Ìòï UI Î∞∞Ïπò Î∞è Í∏∞Îä• Ï∂îÏ≤ú"""
    
    elif category == 'engagement':
        return """üîß ÏÇ¨Ïö©Ïûê ÌôúÎèô Îç∞Ïù¥ÌÑ∞ ÏàòÏßë: ÏÑ∏ÏÖò Í∏∏Ïù¥, Í∏∞Îä• ÏÇ¨Ïö© ÎπàÎèÑ, Ïù¥ÌÉà ÏßÄÏ†ê Î∂ÑÏÑù
üìä Ìë∏Ïãú ÏïåÎ¶º ÏµúÏ†ÅÌôî: A/B ÌÖåÏä§Ìä∏Î°ú ÏµúÏ†Å Î∞úÏÜ° ÏãúÍ∞Ñ Î∞è Î©îÏãúÏßÄ Í∞úÏÑ†
üîç ÏÇ¨Ïö©Ïûê Ïó¨Ï†ï Îß§Ìïë: Ï£ºÏöî Í∏∞Îä•Î≥Ñ ÏÇ¨Ïö©Ïûê ÌîåÎ°úÏö∞ Î∂ÑÏÑù Î∞è Î≥ëÎ™© ÏßÄÏ†ê ÏãùÎ≥Ñ
‚ö° Í≤åÏûÑÌôî ÏöîÏÜå Íµ¨ÌòÑ: ÏÇ¨Ïö© Î™©Ìëú Îã¨ÏÑ± Ïãú Ìè¨Ïù∏Ìä∏ ÏßÄÍ∏â Î∞è Î∞∞ÏßÄ ÏãúÏä§ÌÖú"""
    
    elif category == 'retention':
        return """üîß Ïù¥ÌÉà ÏòàÏ∏° Î™®Îç∏: ÏÇ¨Ïö©Ïûê ÌñâÎèô Ìå®ÌÑ¥ Í∏∞Î∞ò Ïù¥ÌÉà Í∞ÄÎä•ÏÑ± Ïä§ÏΩîÏñ¥ÎßÅ
üìä Ïû¨Î∞©Î¨∏ Ïú†ÎèÑ ÏãúÏä§ÌÖú: ÎπÑÌôúÏÑ± ÏÇ¨Ïö©Ïûê ÎåÄÏÉÅ ÎßûÏ∂§Ìòï Ïù¥Î©îÏùº/SMS Ï∫†ÌéòÏù∏
üîç ÏΩîÌò∏Ìä∏ Î∂ÑÏÑù Íµ¨Ï∂ï: Í∞ÄÏûÖ ÏãúÏ†êÎ≥Ñ ÏÇ¨Ïö©Ïûê Í∑∏Î£π ÏÉùÏ°¥ Î∂ÑÏÑù Î∞è Í∞úÏÑ†Ï†ê ÎèÑÏ∂ú
‚ö° Í≥ÑÏ†ï Ïó∞Îèô Í∞ïÌôî: ÏÜåÏÖú Î°úÍ∑∏Ïù∏, ÌÅ¥ÎùºÏö∞Îìú Î∞±ÏóÖÏúºÎ°ú Í∏∞Í∏∞ Î≥ÄÍ≤Ω Ïãú Îç∞Ïù¥ÌÑ∞ Ïú†ÏßÄ"""
    
    elif category == 'adoption':
        return """üîß Ïò®Î≥¥Îî© ÌîåÎ°úÏö∞ Í∞úÏÑ†: Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ï≤´ 7ÏùºÍ∞Ñ ÏÇ¨Ïö© Ìå®ÌÑ¥ Î∂ÑÏÑù
üìä ÌçºÎÑê Î∂ÑÏÑù ÏãúÏä§ÌÖú: Í∞ÄÏûÖÎ∂ÄÌÑ∞ Ï≤´ ÏÑ±Í≥µ Í≤ΩÌóòÍπåÏßÄ Îã®Í≥ÑÎ≥Ñ Ïù¥ÌÉàÎ•† Ï∏°Ï†ï
üîç ÏÇ¨Ïö©Ïûê ÏÑ∏Í∑∏Î®ºÌä∏ Î∂ÑÏÑù: ÏÇ¨Ïö© Î™©Ï†ÅÎ≥Ñ ÎßûÏ∂§Ìòï Ï¥àÍ∏∞ ÏÑ§Ï†ï ÏõåÌÅ¨ÌîåÎ°úÏö∞ Í∞úÎ∞ú
‚ö° ÌîÑÎ°úÍ∑∏Î†àÏãúÎ∏å ÎîîÏä§ÌÅ¥Î°úÏ†Ä: Î≥µÏû°Ìïú Í∏∞Îä•ÏùÑ Îã®Í≥ÑÎ≥ÑÎ°ú Ï†êÏßÑÏ†Å ÎÖ∏Ï∂ú"""
    
    else:
        return """üîß Ï¢ÖÌï© ÌíàÏßà Í¥ÄÎ¶¨: ÏûêÎèôÌôîÎêú ÌöåÍ∑Ä ÌÖåÏä§Ìä∏ Î∞è ÏÑ±Îä• Î≤§ÏπòÎßàÌÇπ ÏãúÏä§ÌÖú
üìä ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± Î∂ÑÏÑù: Î¶¨Î∑∞ ÌÇ§ÏõåÎìú Î∂ÑÏÑù Î∞è Ïö∞ÏÑ†ÏàúÏúÑ Í∏∞Î∞ò Í∞úÎ∞ú Î°úÎìúÎßµ ÏàòÎ¶Ω
üîç ÌÅ¨Î°úÏä§ ÌîåÎû´Ìèº Ìò∏ÌôòÏÑ±: iOS/Android/Web ÏùºÍ¥ÄÎêú ÏÇ¨Ïö©Ïûê Í≤ΩÌóò Î≥¥Ïû•
‚ö° ÏßÄÏÜçÏ†Å Í∞úÏÑ† ÌîÑÎ°úÏÑ∏Ïä§: Ï£ºÍ∞Ñ ÏÇ¨Ïö©Ïûê Îç∞Ïù¥ÌÑ∞ Î¶¨Î∑∞ Î∞è Îπ†Î•∏ Í∞úÏÑ† ÏÇ¨Ïù¥ÌÅ¥ Íµ¨Ï∂ï"""

def generate_ux_improvement_suggestions(category, issue_type, issues, problem_description, quotes_text):
    """
    Generate UX-focused improvement suggestions based on actual user review quotes and issues
    """
    if category == 'task_success':
        if 'ÌÜµÌôî' in issue_type or 'Ï†ÑÌôî' in issue_type:
            return """- ÌÜµÌôî Ïó∞Í≤∞ Ïã§Ìå® Ïãú Ï¶âÏãú 'Îã§Ïãú ÏãúÎèÑÌïòÍ∏∞', 'Îã§Î•∏ Î≤àÌò∏Î°ú Í±∏Í∏∞', 'Î¨∏Ïûê Î≥¥ÎÇ¥Í∏∞' Î≤ÑÌäºÏù¥ Ìè¨Ìï®Îêú ÏòµÏÖò ÌôîÎ©¥ Ï†úÍ≥µ
- ÌÜµÌôî ÌíàÏßàÏù¥ Ï¢ãÏßÄ ÏïäÏùÑ Îïå ÌôîÎ©¥ ÌïòÎã®Ïóê 'ÏùåÏßà Í∞úÏÑ†' ÌÜ†Í∏Ä Î≤ÑÌäº Î∞∞ÏπòÌïòÏó¨ ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÅÏ†ë Ï°∞Ï†ï Í∞ÄÎä•ÌïòÎèÑÎ°ù ÏÑ§Í≥Ñ
- ÌÜµÌôî Ïó∞Í≤∞ Ï§ë Î°úÎî© ÌôîÎ©¥Ïóê 'Ïó∞Í≤∞ Ï§ëÏûÖÎãàÎã§' Î©îÏãúÏßÄÏôÄ Ìï®Íªò ÏòàÏÉÅ ÎåÄÍ∏∞ ÏãúÍ∞Ñ ÌëúÏãú
- ÌÜµÌôî Ïã§Ìå® Î∞òÎ≥µ Ïãú 'ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉÅÌÉú ÌôïÏù∏' Í∞ÄÏù¥Îìú ÌåùÏóÖÍ≥º Ìï®Íªò Í≥†Í∞ùÏÑºÌÑ∞ ÏßÅÏ†ë Ïó∞Í≤∞ Î≤ÑÌäº Ï†úÍ≥µ"""
        
        elif 'CCTV' in issue_type or 'ÌôîÎ©¥' in issue_type:
            return """- CCTV ÌôîÎ©¥ ÌôïÎåÄ Ïïà Îê† Îïå ÌôîÎ©¥ ÏÉÅÎã®Ïóê 'ÌôïÎåÄ/Ï∂ïÏÜå ÎèÑÏõÄÎßê' ÏïÑÏù¥ÏΩò ÏÉÅÏãú ÌëúÏãúÌïòÏó¨ Ï†úÏä§Ï≤ò Í∞ÄÏù¥Îìú Ï†úÍ≥µ
- ÏòÅÏÉÅ ÎÅäÍπÄ Î∞úÏÉù Ïãú ÌôîÎ©¥ Ï§ëÏïôÏóê 'Ïû¨Ïó∞Í≤∞ Ï§ë' ÏÉÅÌÉúÎ•º ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú ÌëúÏãúÌïòÍ≥†, ÏàòÎèô ÏÉàÎ°úÍ≥†Ïπ® Î≤ÑÌäº Î∞∞Ïπò
- Ïó¨Îü¨ Ïπ¥Î©îÎùº ÎèôÏãú Î≥¥Í∏∞ Ïãú Í∞Å ÌôîÎ©¥Ïóê 'Ï†ÑÏ≤¥ÌôîÎ©¥' Î≤ÑÌäºÏùÑ Í∞úÎ≥Ñ Î∞∞ÏπòÌïòÏó¨ ÏõêÌïòÎäî ÌôîÎ©¥Îßå ÌÅ¨Í≤å Î≥¥Í∏∞ Í∞ÄÎä•
- ÏòÅÏÉÅ Î°úÎî© ÏßÄÏó∞ Ïãú 'Ïû†ÏãúÎßå Í∏∞Îã§Î†§Ï£ºÏÑ∏Ïöî' Î©îÏãúÏßÄÏôÄ Ìï®Íªò ÏòàÏÉÅ Î°úÎî© ÏãúÍ∞Ñ ÌîÑÎ°úÍ∑∏Î†àÏä§ Î∞î ÌëúÏãú"""
        
        elif 'Ïï±' in issue_type and ('ÌäïÍπÄ' in issue_type or 'ÎÇòÍ∞ÄÎ≤ÑÎ¶º' in issue_type):
            return """- Ïï± ÏßÑÏûÖ Ïãú ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï/ÏÑ†ÌÉù ÌôîÎ©¥ÏùÑ Îã®Í≥ÑÏ†ÅÏúºÎ°ú Î°úÎî©ÌïòÎèÑÎ°ù Í∞úÏÑ†ÌïòÏó¨ Ïò§Î•ò Î∞úÏÉù Í∞ÄÎä•ÏÑ±ÏùÑ Ï§ÑÏù¥Í≥†, Ïò§Î•ò Î∞úÏÉù ÏãúÏóêÎäî 'Îã§Ïãú ÏãúÎèÑÌïòÍ∏∞' ÎòêÎäî 'Í≥†Í∞ùÏÑºÌÑ∞ Ïó∞Í≤∞' ÏòµÏÖòÏù¥ ÏûàÎäî Fallback ÌôîÎ©¥ Ï†úÍ≥µ
- Ï≤´ Ïã§Ìñâ Ïãú Î°úÎî© Ïï†ÎãàÎ©îÏù¥ÏÖòÍ≥º ÏßÑÌñâ ÏÉÅÌÉúÎ•º ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú ÏïàÎÇ¥Ìï¥, Ïï±Ïù¥ Î©àÏ∂ò ÎìØÌïú Ïù∏ÏÉÅÏùÑ Ï£ºÏßÄ ÏïäÎèÑÎ°ù ÏÑ§Í≥Ñ
- ÎèôÏùºÌïú Î¨∏Ï†ú Î∞úÏÉù Ïãú ÏÇ¨Ïö©ÏûêÏóêÍ≤å ÎπÑÏπ®Ìï¥Ï†Å ÌåùÏóÖÏùÑ ÌÜµÌï¥ ÎåÄÏùë Î∞©Î≤ï ÏïàÎÇ¥ (Ïòà: "ÏùºÏãúÏ†ÅÏù∏ Ïò§Î•òÏûÖÎãàÎã§. Í≥†Í∞ùÏÑºÌÑ∞ Î¨∏Ïùò ÎòêÎäî Ïû¨ÏãúÎèÑÎ•º Í∂åÏû•Ìï©ÎãàÎã§")
- ÌÅ¨ÎûòÏãú Î∞úÏÉù Ï†Ñ ÎßàÏßÄÎßâ ÌôîÎ©¥ÏùÑ ÏûÑÏãú Ï†ÄÏû•ÌïòÏó¨ Ïû¨Ïã§Ìñâ Ïãú Ïù¥Ï†Ñ ÏÉÅÌÉúÎ°ú Î≥µÏõê Í∞ÄÎä•ÌïòÎèÑÎ°ù ÏÑ§Í≥Ñ"""
        
        elif 'Î°úÍ∑∏Ïù∏' in issue_type or 'Ïù∏Ï¶ù' in issue_type:
            return """- Î°úÍ∑∏Ïù∏ Ïã§Ìå® Ïãú 'ÏïÑÏù¥Îîî Ï∞æÍ∏∞', 'ÎπÑÎ∞ÄÎ≤àÌò∏ Ïû¨ÏÑ§Ï†ï', 'Í≥†Í∞ùÏÑºÌÑ∞ Î¨∏Ïùò' Î≤ÑÌäºÏùÑ Ìïú ÌôîÎ©¥Ïóê Î™ÖÌôïÌûà Î∞∞Ïπò
- Ïù∏Ï¶ù Ïò§Î•ò Î∞úÏÉù Ïãú Íµ¨Ï≤¥Ï†ÅÏù∏ Ïò§Î•ò ÏõêÏù∏Í≥º Ìï¥Í≤∞ Î∞©Î≤ïÏùÑ ÏπúÍ∑ºÌïú ÎßêÌà¨Î°ú ÏïàÎÇ¥ (Ïòà: "Ìú¥ÎåÄÌè∞ Î≤àÌò∏Î•º Îã§Ïãú ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî")
- Î°úÍ∑∏Ïù∏ ÌôîÎ©¥Ïóê 'Í∞ÑÌé∏ Î°úÍ∑∏Ïù∏' ÏòµÏÖò Ï∂îÍ∞ÄÌïòÏó¨ ÏÉùÏ≤¥ Ïù∏Ï¶ù, Ìå®ÌÑ¥ Ïù∏Ï¶ù Îì± ÎåÄÏïà Ï†úÍ≥µ
- Î∞òÎ≥µ Î°úÍ∑∏Ïù∏ Ïã§Ìå® Ïãú 'Î°úÍ∑∏Ïù∏ ÎèÑÏõÄÎßê' ÌôîÎ©¥ÏúºÎ°ú ÏûêÎèô Ïù¥ÎèôÌïòÏó¨ Îã®Í≥ÑÎ≥Ñ Ìï¥Í≤∞ Í∞ÄÏù¥Îìú Ï†úÍ≥µ"""
        
        else:
            return """- ÌïµÏã¨ Í∏∞Îä• ÏÇ¨Ïö© Ï§ë Ïò§Î•ò Î∞úÏÉù Ïãú Ï¶âÏãú 'Î¨∏Ï†ú Ïã†Í≥†ÌïòÍ∏∞' Î≤ÑÌäºÍ≥º Ìï®Íªò ÏûÑÏãú Ìï¥Í≤∞ Î∞©Î≤ï ÏïàÎÇ¥
- Í∏∞Îä• Ïã§Ìñâ Ï†Ñ Î°úÎî© ÏãúÍ∞ÑÏù¥ ÏòàÏÉÅÎê† Îïå ÏßÑÌñâ ÏÉÅÌô©ÏùÑ %Î°ú ÌëúÏãúÌïòÍ≥† 'Ï∑®ÏÜå' Î≤ÑÌäº Ï†úÍ≥µ
- ÏûêÏ£º ÏÇ¨Ïö©ÌïòÎäî Í∏∞Îä•ÏùÑ Ìôà ÌôîÎ©¥ ÏÉÅÎã®Ïóê Î∞îÎ°úÍ∞ÄÍ∏∞Î°ú Î∞∞ÏπòÌïòÏó¨ Ï†ëÍ∑ºÏÑ± Ìñ•ÏÉÅ
- Ïò§Î•ò Î∞úÏÉù Ïãú ÏÇ¨Ïö©Ïûê ÏπúÌôîÏ†ÅÏù∏ Î©îÏãúÏßÄÎ°ú ÏÉÅÌô© ÏÑ§Î™Ö Î∞è Îã§Ïùå Îã®Í≥Ñ ÏïàÎÇ¥"""
    
    elif category == 'happiness':
        return """- ÏÇ¨Ïö©Ïûê Î∂àÎßå ÌëúÌòÑ Ïãú Ïï± ÎÇ¥ 'ÏùòÍ≤¨ Î≥¥ÎÇ¥Í∏∞' Í∏∞Îä•ÏùÑ ÏâΩÍ≤å Ï∞æÏùÑ Ïàò ÏûàÎèÑÎ°ù Î©îÎâ¥ ÏÉÅÎã®Ïóê Î∞∞Ïπò
- Í∏çÏ†ïÏ†Å ÌîºÎìúÎ∞± Ïãú 'ÎèÑÏõÄÏù¥ ÎêòÏóàÎã§Î©¥ Î≥ÑÏ†ê ÎÇ®Í∏∞Í∏∞' Îì±Ïùò ÏûêÏó∞Ïä§Îü¨Ïö¥ Ïú†ÎèÑ Î©îÏãúÏßÄ ÌëúÏãú
- ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ Ï°∞ÏÇ¨Î•º ÌåùÏóÖÏù¥ ÏïÑÎãå Ïï± ÏÇ¨Ïö© ÌîåÎ°úÏö∞Ïóê ÏûêÏó∞Ïä§ÎüΩÍ≤å ÌÜµÌï©
- Î¨∏Ï†ú Ìï¥Í≤∞ ÌõÑ 'Ìï¥Í≤∞ÎêòÏóàÎÇòÏöî?' ÌôïÏù∏ Î©îÏãúÏßÄÎ°ú ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ ÌôïÏù∏"""
    
    elif category == 'engagement':
        return """- ÏÇ¨Ïö©ÏûêÍ∞Ä ÌäπÏ†ï Í∏∞Îä•ÏùÑ ÏûêÏ£º ÏÇ¨Ïö©Ìï† Îïå Í¥ÄÎ†® Í∏∞Îä• Ï∂îÏ≤ú Î©îÏãúÏßÄÎ•º Ï†ÅÏ†àÌïú ÌÉÄÏù¥Î∞çÏóê ÌëúÏãú
- Ïï± ÏÇ¨Ïö© Ìå®ÌÑ¥ÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏÇ¨Ïö©ÏûêÎ≥Ñ ÎßûÏ∂§Ìòï Ìôà ÌôîÎ©¥ Íµ¨ÏÑ± Ï†úÏïà
- ÏÉàÎ°úÏö¥ Í∏∞Îä• Ï∂úÏãú Ïãú Í∏∞Ï°¥ ÏÇ¨Ïö© Ìå®ÌÑ¥Í≥º Ïó∞Í≤∞ÌïòÏó¨ ÏûêÏó∞Ïä§ÎüΩÍ≤å ÏÜåÍ∞ú
- ÏÇ¨Ïö©Ïûê ÌôúÎèôÏù¥ Ï§ÑÏñ¥Îì§ Îïå 'ÎÜìÏπú Í∏∞Îä•' ÏïåÎ¶ºÏúºÎ°ú Ïû¨Ï∞∏Ïó¨ Ïú†ÎèÑ"""
    
    elif category == 'retention':
        return """- ÏÇ¨Ïö©ÏûêÍ∞Ä Ïï±ÏùÑ ÏÇ≠Ï†úÌïòÎ†§ Ìï† Îïå 'Ïû†Íπê, Î¨∏Ï†úÍ∞Ä ÏûàÏúºÏã†Í∞ÄÏöî?' ÌåùÏóÖÏúºÎ°ú Ïù¥ÌÉà ÏÇ¨Ïú† ÌååÏïÖ Î∞è Ï¶âÏãú Ìï¥Í≤∞ ÏãúÎèÑ
- Ïû•Í∏∞Í∞Ñ ÎØ∏ÏÇ¨Ïö© Ïãú 'ÏÉàÎ°úÏö¥ Í∏∞Îä• ÏóÖÎç∞Ïù¥Ìä∏' ÏïåÎ¶ºÎ≥¥Îã§Îäî 'ÎßàÏßÄÎßâÏúºÎ°ú ÏÇ¨Ïö©ÌïòÏÖ®Îçò Í∏∞Îä•' Ï§ëÏã¨ÏúºÎ°ú Î≥µÍ∑Ä Ïú†ÎèÑ
- Í≥ÑÏ†ï ÏÇ≠Ï†ú Ï†Ñ 'Îç∞Ïù¥ÌÑ∞ Î∞±ÏóÖ' ÏòµÏÖò Ï†úÍ≥µÌïòÏó¨ Ïû¨ÏÇ¨Ïö© Í∞ÄÎä•ÏÑ± Ïó¥Ïñ¥ÎëêÍ∏∞
- ÏÇ¨Ïö©ÏûêÎ≥Ñ Ïù¥Ïö© Ìå®ÌÑ¥ Í∏∞Î∞ò ÎßûÏ∂§Ìòï 'Îã§Ïãú ÏãúÏûëÌïòÍ∏∞' Í∞ÄÏù¥Îìú Ï†úÍ≥µ"""
    
    elif category == 'adoption':
        return """- Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ïò®Î≥¥Îî© Ïãú '3Î∂Ñ ÎßåÏóê ÏãúÏûëÌïòÍ∏∞' Îì± Î™ÖÌôïÌïú ÏãúÍ∞Ñ ÏòàÏÉÅÏπò Ï†úÏãú
- Î≥µÏû°Ìïú Ï¥àÍ∏∞ ÏÑ§Ï†ïÏùÑ 'ÎÇòÏ§ëÏóê ÌïòÍ∏∞' ÏòµÏÖòÍ≥º Ìï®Íªò Ï†úÍ≥µÌïòÏó¨ ÏßÑÏûÖ Ïû•Î≤Ω ÏôÑÌôî
- Ï≤´ ÏÑ±Í≥µ Í≤ΩÌóò ÌõÑ 'Îã§Ïùå Îã®Í≥Ñ ÏïàÎÇ¥' Î©îÏãúÏßÄÎ°ú ÏûêÏó∞Ïä§Îü¨Ïö¥ Í∏∞Îä• ÌôïÏû• Ïú†ÎèÑ
- ÏÇ¨Ïö© Î™©Ï†ÅÎ≥Ñ 'Îπ†Î•∏ ÏãúÏûë' ÌÖúÌîåÎ¶ø Ï†úÍ≥µ (Ïòà: 'CCTVÎßå ÏÇ¨Ïö©', 'ÌÜµÌôî Í∏∞Îä• Ï§ëÏã¨' Îì±)"""
    
    else:
        return """- ÏÇ¨Ïö©Ïûê Î¶¨Î∑∞ÏóêÏÑú Ïñ∏Í∏âÎêú Íµ¨Ï≤¥Ï†Å Î¨∏Ï†úÏ†êÏùÑ Ìï¥Í≤∞ÌïòÎäî Îã®Í≥ÑÎ≥Ñ Í∞ÄÏù¥Îìú Ï†úÍ≥µ
- ÏûêÏ£º Î∞úÏÉùÌïòÎäî Î¨∏Ï†úÏóê ÎåÄÌïú 'ÏûêÏ£º Î¨ªÎäî ÏßàÎ¨∏' ÏÑπÏÖòÏùÑ Ïï± ÎÇ¥ ÏâΩÍ≤å Ï†ëÍ∑º Í∞ÄÎä•Ìïú ÏúÑÏπòÏóê Î∞∞Ïπò
- ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞±ÏùÑ Ïã§ÏãúÍ∞ÑÏúºÎ°ú ÏàòÏßëÌïòÍ≥† Îπ†Î•∏ Í∞úÏÑ† ÏÇ¨Ìï≠ÏùÑ Ïï± ÎÇ¥ Í≥µÏßÄÎ°ú Ìà¨Î™ÖÌïòÍ≤å Í≥µÏú†
- Í∞Å Í∏∞Îä•Î≥Ñ 'ÎèÑÏõÄÎßê' Î≤ÑÌäºÏùÑ ÏÉÅÌô©Ïóê ÎßûÍ≤å Î∞∞ÏπòÌïòÏó¨ Ï¶âÏãú ÎèÑÏõÄ Î∞õÏùÑ Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥Ñ"""

def generate_realistic_ux_suggestions(category, issue_type, issues, problem_description, quotes_text):
    """
    Generate UX improvement suggestions based on actual user review content and specific problems
    Focus on interface experience improvements, user flow optimization, and concrete UX solutions
    """
    # Analyze specific user expressions and problems from quotes
    specific_suggestions = []
    
    # Analyze actual user quotes to identify specific UX problems
    if 'ÌÜµÌôîÏ§ë ÎåÄÍ∏∞Í∞Ä ÎêòÏßÄ ÏïäÏïÑÏÑú Î∂àÌé∏ÌïòÎÑ§Ïöî' in quotes_text:
        specific_suggestions.extend([
            "ÌÜµÌôî Ï§ë ÌôîÎ©¥ ÌïòÎã®Ïóê 'ÎåÄÍ∏∞' Î≤ÑÌäºÏùÑ Ï∂îÍ∞ÄÌïòÏó¨ ÌòÑÏû¨ ÌÜµÌôîÎ•º ÏùºÏãúÏ†ïÏßÄÌïòÍ≥† Îã§Î•∏ Ï†ÑÌôîÎ•º Î∞õÏùÑ Ïàò ÏûàÎäî Í∏∞Îä• Ï†úÍ≥µ",
            "ÎåÄÍ∏∞ ÏÉÅÌÉú ÏßÑÏûÖ Ïãú 'ÌÜµÌôî ÎåÄÍ∏∞ Ï§ë' ÌëúÏãúÏôÄ Ìï®Íªò 'ÎåÄÍ∏∞ Ìï¥Ï†ú' Î≤ÑÌäºÏùÑ ÌôîÎ©¥ Ï§ëÏïôÏóê Î∞∞ÏπòÌïòÏó¨ ÏßÅÍ¥ÄÏ†Å Ï°∞Ïûë Í∞ÄÎä•"
        ])
    
    if 'Î≥ºÎ•®Î≤ÑÌäº ÎàÑÎ•¥Î©¥ ÏßÑÎèôÏù¥ Í∫ºÏßÄÎ©¥ Ï¢ãÍ≤†ÎÑ§Ïöî' in quotes_text and 'ÎãπÌô©Ïä§Îü¨Ïö¥ Í≤ΩÌóò' in quotes_text:
        specific_suggestions.extend([
            "ÌÜµÌôî ÏàòÏã† Ïãú Î≥ºÎ•®Î≤ÑÌäº ÌÑ∞Ïπò ÏòÅÏó≠ÏùÑ ÌôîÎ©¥Ïóê ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú ÌëúÏãúÌïòÏó¨ 'Î≥ºÎ•® Î≤ÑÌäºÏùÑ ÎàÑÎ•¥Î©¥ Î¨¥Ïùå Î™®Îìú'ÏûÑÏùÑ ÎØ∏Î¶¨ ÏïàÎÇ¥",
            "Î≥ºÎ•® Î≤ÑÌäº ÌÑ∞Ïπò Ïãú Ï¶âÏãú ÏßÑÎèô Ï§ëÎã®Í≥º Ìï®Íªò 'Î¨¥Ïùå Î™®ÎìúÎ°ú Ï†ÑÌôòÎê®' ÌîºÎìúÎ∞± Î©îÏãúÏßÄÎ•º ÌôîÎ©¥ ÏÉÅÎã®Ïóê ÏßßÍ≤å ÌëúÏãú"
        ])
    
    if 'ÌÜµÌôîÏó∞Í≤∞ÏùåÏ¢Ä Î∞îÍøâÏãúÎã§ ÏãúÎÅÑÎü¨ÏõåÏ£ΩÍ≤†ÏäµÎãàÎã§' in quotes_text:
        specific_suggestions.extend([
            "ÏÑ§Ï†ï Î©îÎâ¥ Ï≤´ Î≤àÏß∏ Ìï≠Î™©Ïóê 'ÌÜµÌôîÏùå ÏÑ§Ï†ï' Î∞∞ÏπòÌïòÍ≥† Î≥ºÎ•® Ï°∞Ï†à Ïä¨ÎùºÏù¥ÎçîÏôÄ Ìï®Íªò 'Î¨¥Ïùå', 'ÏßÑÎèô', 'Î≤®ÏÜåÎ¶¨' ÏòµÏÖòÏùÑ Ìïú ÌôîÎ©¥Ïóê ÌëúÏãú",
            "ÌÜµÌôî Ïó∞Í≤∞Ïùå Î≥ÄÍ≤Ω Ïãú Ï¶âÏãú ÎØ∏Î¶¨Îì£Í∏∞ Í∏∞Îä•Í≥º Ìï®Íªò 'Ïù¥ ÏÜåÎ¶¨Î°ú ÏÑ§Ï†ïÌïòÏãúÍ≤†Ïñ¥Ïöî?' ÌôïÏù∏ ÌåùÏóÖ Ï†úÍ≥µ"
        ])
    
    if 'ÌôîÎ©¥ ÌôïÎåÄ ÏïàÎêòÎäî Í≤É Ï¢Ä Ïñ¥ÎñªÍ≤å Ìï¥Ï£ºÏÑ∏Ïöî ÎãµÎãµÌïòÎÑ§Ïöî' in quotes_text:
        specific_suggestions.extend([
            "CCTV ÌôîÎ©¥ Ïö∞Ï∏° ÌïòÎã®Ïóê ÎèãÎ≥¥Í∏∞ ÏïÑÏù¥ÏΩò(+/-) Î≤ÑÌäºÏùÑ Í≥†Ï†ï Î∞∞ÏπòÌïòÏó¨ ÌïÄÏπò Ï†úÏä§Ï≤òÍ∞Ä Ïñ¥Î†§Ïö¥ ÏÇ¨Ïö©ÏûêÎèÑ ÏâΩÍ≤å ÌôïÎåÄ/Ï∂ïÏÜå Í∞ÄÎä•",
            "ÌôîÎ©¥ ÌôïÎåÄ Ïã§Ìå® Ïãú 'ÌôïÎåÄÍ∞Ä Ïïà ÎêòÏãúÎÇòÏöî? ÏïÑÎûò + Î≤ÑÌäºÏùÑ ÎàåÎü¨Î≥¥ÏÑ∏Ïöî' ÎßêÌíçÏÑ† ÏïàÎÇ¥Î•º ÌôîÎ©¥ Ï§ëÏïôÏóê 3Ï¥àÍ∞Ñ ÌëúÏãú"
        ])
    
    if 'Ïù∏Ï¶ùÏóêÎü¨Î°ú ÏÇ¨Ïö©Î∂àÍ∞ÄÌï©ÎãàÎã§' in quotes_text:
        specific_suggestions.extend([
            "ÌäπÏ†ï Í∏∞Ï¢Ö Ïù∏Ï¶ù Ïò§Î•ò Î∞úÏÉù Ïãú 'Ïù¥ Í∏∞Ï¢ÖÏóêÏÑú ÏùºÏãúÏ†Å Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§' ÏïàÎÇ¥ÏôÄ Ìï®Íªò 'ÏûÑÏãú Ï†ëÏÜç Î∞©Î≤ï' Í∞ÄÏù¥ÎìúÎ•º Îã®Í≥ÑÎ≥ÑÎ°ú Ï†úÍ≥µ",
            "Ïù∏Ï¶ù Ïû¨ÏãúÎèÑ Ïãú 'Îã§Ïãú Ïù∏Ï¶ù Ï§ëÏûÖÎãàÎã§...' ÏßÑÌñâÎ•† Î∞îÏôÄ Ìï®Íªò ÏòàÏÉÅ ÏÜåÏöîÏãúÍ∞Ñ 'ÏïΩ 30Ï¥à' ÌëúÏãúÎ°ú ÎåÄÍ∏∞ Î∂àÏïàÍ∞ê Ìï¥ÏÜå"
        ])
    
    if 'Ïï± Ïó¥Î©¥ Í∑∏ÎÉ• ÎÇòÍ∞ÄÎ≤ÑÎ¶º' in quotes_text or 'ÎÇòÍ∞ÄÎ≤ÑÎ¶º' in quotes_text:
        specific_suggestions.extend([
            "Ïï± Ï≤´ Ïã§Ìñâ Ïãú Î°úÎî© ÌôîÎ©¥Ïóê 'Ïï±ÏùÑ Ï§ÄÎπÑÌïòÍ≥† ÏûàÏäµÎãàÎã§' Î©îÏãúÏßÄÏôÄ Ìï®Íªò Í∞ÑÎã®Ìïú ÏßÑÌñâÎ•† ÌëúÏãúÎ°ú Ïï±Ïù¥ Î©àÏ∂ò Í≤ÉÏ≤òÎüº Î≥¥Ïù¥ÏßÄ ÏïäÎèÑÎ°ù ÏÑ§Í≥Ñ",
            "Ïï± ÌÅ¨ÎûòÏãú ÌõÑ Ïû¨Ïã§Ìñâ Ïãú 'Ïù¥Ï†Ñ ÌôîÎ©¥ÏóêÏÑú Îã§Ïãú ÏãúÏûëÌïòÏãúÍ≤†Ïñ¥Ïöî?' ÏòµÏÖòÏúºÎ°ú ÎßàÏßÄÎßâ ÏÇ¨Ïö© ÏúÑÏπòÎ°ú Î∞îÎ°ú Ïù¥Îèô Í∞ÄÎä•"
        ])
    
    if 'Ìï¥ÏßÄÌïòÍ≥†Ïã∂ÎÑ§Ïöî' in quotes_text or 'ÏÇ≠Ï†ú' in quotes_text:
        specific_suggestions.extend([
            "ÏÑ§Ï†ï Î©îÎâ¥ÏóêÏÑú 'ÏÑúÎπÑÏä§ Ìï¥ÏßÄ' ÏÑ†ÌÉù Ïãú Ï¶âÏãú Ìï¥ÏßÄ ÌôîÎ©¥ÏúºÎ°ú Ïù¥ÎèôÌïòÏßÄ ÏïäÍ≥† 'Î¨∏Ï†úÍ∞Ä ÏûàÏúºÏã†Í∞ÄÏöî?' Ï§ëÍ∞Ñ Îã®Í≥ÑÎ•º Í±∞Ï≥ê Ìï¥Í≤∞ ÏãúÎèÑ",
            "Ìï¥ÏßÄ ÏùòÏÇ¨ ÌëúÌòÑ Ïãú '30Ïùº Î¨¥Î£å Ïó∞Ïû•' ÎòêÎäî '1:1 ÎßûÏ∂§ ÏÉÅÎã¥' Í∞ôÏùÄ ÎåÄÏïàÏùÑ Ïπ¥Îìú ÌòïÌÉúÎ°ú Ï†úÏãúÌïòÏó¨ Ïù¥ÌÉà Î∞©ÏßÄ"
        ])
    
    if 'Î°úÍ∑∏ÏïÑÏõÉÎêòÏÑú' in quotes_text and 'ÏßÑÌñâÏù¥ ÏïàÎê©ÎãàÎã§' in quotes_text:
        specific_suggestions.extend([
            "ÏòàÍ∏∞Ïπò ÏïäÏùÄ Î°úÍ∑∏ÏïÑÏõÉ Î∞úÏÉù Ïãú ÏûêÎèô Î°úÍ∑∏Ïù∏ ÏãúÎèÑ Ï§ëÏûÑÏùÑ ÏïåÎ¶¨Îäî 'ÏûêÎèôÏúºÎ°ú Îã§Ïãú Î°úÍ∑∏Ïù∏ÌïòÍ≥† ÏûàÏäµÎãàÎã§' Î©îÏãúÏßÄÏôÄ Ìï®Íªò ÏàòÎèô Î°úÍ∑∏Ïù∏ Î≤ÑÌäº Î≥ëÌñâ Ï†úÍ≥µ",
            "Î°úÍ∑∏Ïù∏ ÌôîÎ©¥ÏóêÏÑú 'Ïù¥Ï†Ñ Í≥ÑÏ†ïÏúºÎ°ú Îπ†Î•∏ Î°úÍ∑∏Ïù∏' Î≤ÑÌäºÏùÑ ID ÏûÖÎ†•Ï∞Ω ÏúÑÏóê Î∞∞ÏπòÌïòÏó¨ Ïû¨ÏûÖÎ†• Î∂ÄÎã¥ Í∞êÏÜå"
        ])
    
    # If no specific quotes matched, provide category-based generic suggestions
    if not specific_suggestions:
        if category == 'task_success':
            if 'ÌÜµÌôî' in issue_type or 'Ï†ÑÌôî' in issue_type:
                specific_suggestions = [
                    "ÌÜµÌôî Ïó∞Í≤∞ Ïã§Ìå® Ïãú 'Îã§Ïãú Ïó∞Í≤∞' Î≤ÑÌäºÏùÑ ÌôîÎ©¥ Ï§ëÏïôÏóê ÌÅ¨Í≤å Î∞∞ÏπòÌïòÍ≥† ÏùºÎ∞ò Ï†ÑÌôîÏï±ÏúºÎ°ú Ïó∞Í≤∞ÌïòÎäî 'ÏùºÎ∞ò ÌÜµÌôî' ÏòµÏÖòÏùÑ Ìï®Íªò Ï†úÍ≥µ",
                    "ÌÜµÌôî ÌíàÏßà Î¨∏Ï†ú Î∞úÏÉù Ïãú ÌôîÎ©¥ ÏÉÅÎã®Ïóê Ïã†Ìò∏ Í∞ïÎèÑ ÌëúÏãúÍ∏∞Î•º Ï∂îÍ∞ÄÌïòÏó¨ ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉÅÌÉúÎ•º Ïã§ÏãúÍ∞ÑÏúºÎ°ú ÌôïÏù∏ Í∞ÄÎä•"
                ]
            elif 'ÎÑ§Ìä∏ÏõåÌÅ¨' in issue_type or 'Ïó∞Í≤∞' in issue_type:
                specific_suggestions = [
                    "Ïó∞Í≤∞ ÎÅäÍπÄ Î∞úÏÉù Ïãú 'ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ÏùÑ ÌôïÏù∏ÌïòÍ≥† ÏûàÏäµÎãàÎã§' Î©îÏãúÏßÄÏôÄ Ìï®Íªò ÏûêÎèô Ïû¨Ïó∞Í≤∞ ÏßÑÌñâÎ•† ÌëúÏãú",
                    "Ïó∞Í≤∞ Ïã§Ìå® Î∞òÎ≥µ Ïãú 'ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÑ§Ï†ï ÌôïÏù∏' Í∞ÄÏù¥ÎìúÎ•º Îã®Í≥ÑÎ≥ÑÎ°ú Ï†úÍ≥µÌïòÎäî ÎèÑÏõÄÎßê ÌéòÏù¥ÏßÄÎ°ú Ïó∞Í≤∞"
                ]
            else:
                specific_suggestions = [
                    "ÌïµÏã¨ Í∏∞Îä• Ïò§Î•ò Ïãú 'Î¨∏Ï†ú Ïã†Í≥†ÌïòÍ∏∞' Î≤ÑÌäºÏùÑ ÏóêÎü¨ Î©îÏãúÏßÄÏôÄ Ìï®Íªò ÌëúÏãúÌïòÏó¨ ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± ÏàòÏßë",
                    "Í∏∞Îä• ÏÇ¨Ïö© Ï§ë Î¨∏Ï†ú Î∞úÏÉù Ïãú 'Ïù¥Ï†Ñ Îã®Í≥ÑÎ°ú ÎèåÏïÑÍ∞ÄÍ∏∞' ÏòµÏÖò Ï†úÍ≥µÌïòÏó¨ ÏûëÏóÖ Ïó∞ÏÜçÏÑ± Î≥¥Ïû•"
                ]
        
        elif category == 'happiness':
            specific_suggestions = [
                "ÏÇ¨Ïö©Ïûê Î∂àÎßå Í∞êÏßÄ Ïãú Ïï± ÏÉÅÎã®Ïóê 'ÏùòÍ≤¨ Î≥¥ÎÇ¥Í∏∞' ÏïåÎ¶º Î∞∞ÎÑàÎ•º ÏùºÏãúÏ†ÅÏúºÎ°ú ÌëúÏãúÌïòÏó¨ ÌîºÎìúÎ∞± Í≤ΩÎ°ú Ï†úÍ≥µ",
                "Í∏çÏ†ïÏ†Å Í≤ΩÌóò ÌõÑ ÏûêÏó∞Ïä§ÎüΩÍ≤å 'Ïù¥ Í∏∞Îä•Ïù¥ ÎèÑÏõÄÎêòÏÖ®ÎÇòÏöî?' ÏóÑÏßÄÏ≤ô Î≤ÑÌäºÏúºÎ°ú ÎßåÏ°±ÎèÑ ÏàòÏßë"
            ]
        
        elif category == 'engagement':
            specific_suggestions = [
                "ÏûêÏ£º ÏÇ¨Ïö©ÌïòÎäî Í∏∞Îä•ÏùÑ Ìôà ÌôîÎ©¥ 'Ï¶êÍ≤®Ï∞æÍ∏∞' ÏòÅÏó≠Ïóê ÏûêÎèô Î∞∞ÏπòÌïòÍ≥† ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÅÏ†ë Ìé∏Ïßë Í∞ÄÎä•Ìïú ÏòµÏÖò Ï†úÍ≥µ",
                "ÏÉà Í∏∞Îä• Ï∂îÍ∞Ä Ïãú Í∏∞Ï°¥ ÏÇ¨Ïö© Ìå®ÌÑ¥Í≥º Ïó∞Í¥ÄÎêú Í∏∞Îä•Îßå 'ÏÉàÎ°úÏö¥ Í∏∞Îä•' Î∞∞ÏßÄÏôÄ Ìï®Íªò Ï∂îÏ≤ú"
            ]
        
        elif category == 'retention':
            specific_suggestions = [
                "Ïû•Í∏∞Í∞Ñ ÎØ∏ÏÇ¨Ïö© Ïãú 'ÎßàÏßÄÎßâ ÏÇ¨Ïö© Í∏∞Îä•'ÏùÑ Î©îÏù∏ ÌôîÎ©¥Ïóê Ïö∞ÏÑ† ÌëúÏãúÌïòÏó¨ ÏùµÏàôÌïú Í∏∞Îä•Î∂ÄÌÑ∞ Ïû¨ÏãúÏûë Ïú†ÎèÑ",
                "Í≥ÑÏ†ï ÏÇ≠Ï†ú ÏãúÎèÑ Ïãú 'Îç∞Ïù¥ÌÑ∞ Î∞±ÏóÖ ÌõÑ ÏÇ≠Ï†ú' ÏòµÏÖòÏúºÎ°ú Ìñ•ÌõÑ Î≥µÍµ¨ Í∞ÄÎä•ÏÑ± Ï†úÍ≥µ"
            ]
        
        elif category == 'adoption':
            specific_suggestions = [
                "Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ïò®Î≥¥Îî©ÏóêÏÑú '3Î∂Ñ Îπ†Î•∏ ÏãúÏûë' Í≤ΩÎ°úÏôÄ 'ÏûêÏÑ∏Ìïú ÏÑ§Ï†ï' Í≤ΩÎ°úÎ•º ÏÑ†ÌÉùÏßÄÎ°ú Ï†úÍ≥µ",
                "Ï≤´ ÏÇ¨Ïö© Ïãú 'Í∞ÄÏû• ÎßéÏù¥ ÏÇ¨Ïö©ÎêòÎäî Í∏∞Îä• 3Í∞ÄÏßÄ'Î•º Ïπ¥Îìú ÌòïÌÉúÎ°ú Ï†úÏãúÌïòÏó¨ ÏÑ†ÌÉùÏ†Å ÌïôÏäµ Í∞ÄÎä•"
            ]
    
    return "\n".join([f"- {suggestion}" for suggestion in specific_suggestions])

def generate_specific_problem_from_quotes(quotes_text, category):
    """
    Generate specific problem description based on actual user quotes
    """
    # Extract emotional expressions and specific issues
    if "ÎãµÎãµÌïòÎÑ§Ïöî" in quotes_text and "ÌôîÎ©¥ ÌôïÎåÄ" in quotes_text:
        return "ÌôîÎ©¥ ÌôïÎåÄ Í∏∞Îä•Ïù¥ ÏûëÎèôÌïòÏßÄ ÏïäÏïÑ ÏÇ¨Ïö©ÏûêÍ∞Ä ÎãµÎãµÌï®ÏùÑ ÎäêÎÅºÎäî ÏÉÅÌô©ÏúºÎ°ú Ïù∏Ìïú ÌïµÏã¨ Í∏∞Îä• ÏàòÌñâ Î∂àÍ∞Ä"
    elif "ÌäïÍ≤®ÏÑú" in quotes_text and "Ïù∏Ï¶ùÎã§ÏãúÌï¥ÏïºÎêòÍ≥†" in quotes_text:
        return "Ïï±Ïù¥ ÎπàÎ≤àÌïòÍ≤å ÌäïÍ∏∞Î©∞ Îß§Î≤à Ïû¨Ïù∏Ï¶ùÏùÑ ÏöîÍµ¨ÌïòÎäî ÏÉÅÌô©ÏúºÎ°ú Ïù∏Ìïú ÌïµÏã¨ Í∏∞Îä• ÏàòÌñâ Î∂àÍ∞Ä"
    elif "Ìï¥ÏßÄÌïòÍ≥†Ïã∂ÎÑ§Ïöî" in quotes_text:
        return "ÏÑúÎπÑÏä§Ïóê ÎåÄÌïú Ï†ÑÎ∞òÏ†ÅÏù∏ Ïã†Î¢∞ÎèÑ Í∏âÍ≤© Ï†ÄÌïòÎ°ú Ïù∏Ìïú Í≥ÑÏïΩ Ìï¥ÏßÄ Í≥†Î†§"
    elif "Î°úÍ∑∏ÏïÑÏõÉÎêòÏÑú" in quotes_text and "ÏßÑÌñâÏù¥ ÏïàÎê©ÎãàÎã§" in quotes_text:
        return "Î°úÍ∑∏Ïù∏ Í≥ºÏ†ïÏóêÏÑú Î∞úÏÉùÌïòÎäî ÏãúÏä§ÌÖú Ïò§Î•òÎ°ú Ïù∏Ìïú ÏÑúÎπÑÏä§ Ï†ëÍ∑º Î∂àÍ∞Ä"
    elif "ÎÅäÏñ¥Ïßê" in quotes_text and "ÎÅäÏñ¥ÏßÄÍ≥†" in quotes_text:
        return "ÌÜµÌôî Ïó∞Í≤∞ Ïã§Ìå®ÏôÄ Ï§ëÎèÑ ÎÅäÍπÄ ÌòÑÏÉÅ Î∞òÎ≥µÏúºÎ°ú Ïù∏Ìïú ÌïµÏã¨ Í∏∞Îä• ÏàòÌñâ Î∂àÍ∞Ä"
    elif "ÏÇ≠Ï†ú" in quotes_text:
        return "ÏßÄÏÜçÏ†ÅÏù∏ Í∏∞Îä• Ïò§Î•òÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ïù¥ÌÉàÍ≥º Ïï± ÏôÑÏ†Ñ ÏÇ≠Ï†ú"
    else:
        return f"{category} Í¥ÄÎ†® ÏÇ¨Ïö©Ïûê Î∂àÎßåÏúºÎ°ú Ïù∏Ìïú ÏÑúÎπÑÏä§ Ïù¥Ïö© Ï†ÄÌïò"
    if "Ïó∞Í≤∞" in quotes_text and "ÎÅä" in quotes_text:
        user_expressions.append("Ïó∞Í≤∞_ÎÅäÍπÄ")
    if "cctv" in quotes_text.lower() or "CCTV" in quotes_text:
        user_expressions.append("CCTV_Î¨∏Ï†ú")
    if "ÏÇ¨Ïö©Î∂àÍ∞Ä" in quotes_text or "ÏïàÎê©ÎãàÎã§" in quotes_text:
        user_expressions.append("Í∏∞Îä•_ÏÇ¨Ïö©Î∂àÍ∞Ä")
    if "Ïò§Î•ò" in quotes_text or "ÏóêÎü¨" in quotes_text:
        user_expressions.append("Ïò§Î•ò_Î∞úÏÉù")
    if "ÎãµÎãµ" in quotes_text or "ÏßúÏ¶ù" in quotes_text:
        user_expressions.append("ÏÇ¨Ïö©Ïûê_Î∂àÎßå")
    
    if category == 'task_success':
        if "Ïï±_ÌÅ¨ÎûòÏãú" in user_expressions:
            return """- Ïï± Ï≤´ Ïã§Ìñâ Ïãú, ÏÇ¨Ïö©Ïûê ÏÑ†ÌÉù Îã®Í≥ÑÎ•º Í±∞ÏπòÍ∏∞ Ï†ÑÏóê 'Ïï± Ï¥àÍ∏∞ Î°úÎî© ÏïàÎÇ¥ ÌôîÎ©¥'ÏùÑ Î≥ÑÎèÑÎ°ú Íµ¨ÏÑ±Ìï¥ Ïï±Ïù¥ Î©àÏ∂ò ÎìØÌïú Ïù∏ÏÉÅÏùÑ Ï§ÑÏù¥ÏßÄ ÏïäÎèÑÎ°ù ÏÑ§Í≥Ñ
- ÏÇ¨Ïö©Ïûê ÏÑ†ÌÉù Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïòÎ©¥ Ï¶âÏãú "Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏñ¥Ïöî. Îã§Ïãú ÏãúÎèÑÌïòÍ±∞ÎÇò Í≥†Í∞ùÏÑºÌÑ∞Î°ú Î¨∏ÏùòÌï¥Ï£ºÏÑ∏Ïöî" Í∞ôÏùÄ ÎπÑÏπ®Ìï¥ÏÑ± ÏïàÎÇ¥ ÌôîÎ©¥ÏúºÎ°ú Ïú†ÎèÑ
- Î∞òÎ≥µ Í∞ïÏ†ú Ï¢ÖÎ£å Ïãú, ÏÇ¨Ïö©ÏûêÏóêÍ≤å "Ïï±Ïù¥ Ï†ïÏÉÅ ÏûëÎèôÌïòÏßÄ ÏïäÎÇòÏöî?"ÎùºÎäî ÌîºÎìúÎ∞± ÏòµÏÖòÏùÑ Ï†úÍ≥µÌï¥ ÏÇ¨Ïö©ÏûêÎèÑ Î¨∏Ï†ú Ïù∏ÏãùÏóê Í∏∞Ïó¨ÌïòÎèÑÎ°ù Ïú†ÎèÑ
- ÌÅ¨ÎûòÏãú Î∞úÏÉù Ï†Ñ ÎßàÏßÄÎßâ ÌôîÎ©¥ÏùÑ ÏûÑÏãú Ï†ÄÏû•ÌïòÏó¨ Ïû¨Ïã§Ìñâ Ïãú Ìï¥Îãπ ÏúÑÏπòÏóêÏÑú Îã§Ïãú ÏãúÏûëÌï† Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥Ñ"""
        
        elif "ÌôîÎ©¥_ÌôïÎåÄ_Î∂àÍ∞Ä" in user_expressions:
            return """- CCTV ÌôîÎ©¥ Ïö∞Ï∏° ÌïòÎã®Ïóê 'ÌôïÎåÄ/Ï∂ïÏÜå ÎèÑÏõÄÎßê' ÏïÑÏù¥ÏΩòÏùÑ ÏÉÅÏãú ÌëúÏãúÌïòÏó¨ ÏÇ¨Ïö©ÏûêÍ∞Ä Ï†úÏä§Ï≤ò Î∞©Î≤ïÏùÑ ÏâΩÍ≤å ÌôïÏù∏Ìï† Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥Ñ
- ÌôîÎ©¥ ÌôïÎåÄÍ∞Ä Ïïà Îê† Îïå "ÌôîÎ©¥ÏùÑ Îëê ÏÜêÍ∞ÄÎùΩÏúºÎ°ú Î≤åÎ†§ÏÑú ÌôïÎåÄÌï¥Î≥¥ÏÑ∏Ïöî" Í∞ôÏùÄ ÏßÅÍ¥ÄÏ†ÅÏù∏ ÏïàÎÇ¥ Î©îÏãúÏßÄÎ•º ÌôîÎ©¥ Ï§ëÏïôÏóê ÌëúÏãú
- ÌôïÎåÄ Í∏∞Îä• Ïã§Ìå® Ïãú "ÌôïÎåÄÍ∞Ä Ïïà ÎêòÏãúÎÇòÏöî? ÏÉàÎ°úÍ≥†Ïπ®ÏùÑ ÏãúÎèÑÌï¥Î≥¥ÏÑ∏Ïöî" Î≤ÑÌäºÍ≥º Ìï®Íªò ÏàòÎèô ÏÉàÎ°úÍ≥†Ïπ® ÏòµÏÖò Ï†úÍ≥µ
- Íµ¨Ìòï Í∏∞Í∏∞ÏóêÏÑú ÌôïÎåÄ Í∏∞Îä•Ïù¥ Ï†úÌïúÏ†ÅÏùº Îïå "Ïù¥ Í∏∞Í∏∞ÏóêÏÑúÎäî ÌôïÎåÄ Í∏∞Îä•Ïù¥ Ï†úÌïúÎê† Ïàò ÏûàÏäµÎãàÎã§" ÏïàÎÇ¥ Î©îÏãúÏßÄ ÌëúÏãú"""
        
        elif "Î°úÍ∑∏Ïù∏_Î¨∏Ï†ú" in user_expressions:
            return """- Î°úÍ∑∏Ïù∏ Ïã§Ìå® Ïãú "ÏïÑÏù¥ÎîîÎÇò ÎπÑÎ∞ÄÎ≤àÌò∏Î•º Îã§Ïãú ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî" Î©îÏãúÏßÄÏôÄ Ìï®Íªò 'ÏïÑÏù¥Îîî Ï∞æÍ∏∞', 'ÎπÑÎ∞ÄÎ≤àÌò∏ Ïû¨ÏÑ§Ï†ï' Î≤ÑÌäºÏùÑ Î™ÖÌôïÌûà Î∞∞Ïπò
- Ïù∏Ï¶ù Ïò§Î•ò Î∞úÏÉù Ïãú "Ïù∏Ï¶ùÏóê Î¨∏Ï†úÍ∞Ä ÏûàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî" Í∞ôÏùÄ ÏπúÍ∑ºÌïú ÏïàÎÇ¥ Î©îÏãúÏßÄ ÌëúÏãú
- Î∞òÎ≥µ Î°úÍ∑∏Ïù∏ Ïã§Ìå® Ïãú "Î°úÍ∑∏Ïù∏Ïóê Í≥ÑÏÜç Î¨∏Ï†úÍ∞Ä ÏûàÏúºÏãúÎ©¥ Í≥†Í∞ùÏÑºÌÑ∞Î°ú Î¨∏ÏùòÌï¥Ï£ºÏÑ∏Ïöî" ÏïàÎÇ¥ÏôÄ Ìï®Íªò Í≥†Í∞ùÏÑºÌÑ∞ Ïó∞Í≤∞ Î≤ÑÌäº Ï†úÍ≥µ
- Î°úÍ∑∏Ïù∏ ÌôîÎ©¥ÏóêÏÑú "Í∞ÑÌé∏ Î°úÍ∑∏Ïù∏" ÏòµÏÖòÏùÑ Ï∂îÍ∞ÄÌïòÏó¨ ÏÉùÏ≤¥ Ïù∏Ï¶ù Îì± ÎåÄÏïà Ï†úÍ≥µ"""
        
        elif "Ïó∞Í≤∞_ÎÅäÍπÄ" in user_expressions:
            return """- Ïó∞Í≤∞Ïù¥ ÎÅäÍ≤ºÏùÑ Îïå "Ïó∞Í≤∞Ïù¥ ÎÅäÏñ¥Ï°åÏäµÎãàÎã§" Î©îÏãúÏßÄÏôÄ Ìï®Íªò 'Ïû¨Ïó∞Í≤∞' Î≤ÑÌäºÏùÑ ÌôîÎ©¥ Ï§ëÏïôÏóê ÌÅ¨Í≤å Î∞∞Ïπò
- ÏûêÏ£º Ïó∞Í≤∞Ïù¥ ÎÅäÍ∏∞Îäî Í≤ΩÏö∞ "ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉÅÌÉúÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî" ÏïàÎÇ¥ Î©îÏãúÏßÄÏôÄ Ìï®Íªò ÎÑ§Ìä∏ÏõåÌÅ¨ ÏßÑÎã® Í∞ÄÏù¥Îìú Ï†úÍ≥µ
- Ïó∞Í≤∞ Ï§ë ÌôîÎ©¥Ïóê "Ïó∞Í≤∞ Ï§ëÏûÖÎãàÎã§..." Î©îÏãúÏßÄÏôÄ Ìï®Íªò ÏßÑÌñâÎ•† ÌëúÏãúÌïòÏó¨ ÏÇ¨Ïö©ÏûêÍ∞Ä Í∏∞Îã§Î¶ºÏùò Ïù¥Ïú†Î•º Ïïå Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥Ñ
- Ïó∞Í≤∞ Ïã§Ìå® Î∞òÎ≥µ Ïãú "Í≥ÑÏÜç Ïó∞Í≤∞Ïóê Î¨∏Ï†úÍ∞Ä ÏûàÏúºÏãúÎ©¥ Í≥†Í∞ùÏÑºÌÑ∞Î°ú Î¨∏ÏùòÌï¥Ï£ºÏÑ∏Ïöî" ÏòµÏÖò Ï†úÍ≥µ"""
        
        elif "CCTV_Î¨∏Ï†ú" in user_expressions:
            return """- CCTV ÌôîÎ©¥Ïù¥ Ïïà Î≥¥Ïùº Îïå "CCTV Ïó∞Í≤∞ÏùÑ ÌôïÏù∏ÌïòÍ≥† ÏûàÏäµÎãàÎã§" Î©îÏãúÏßÄÏôÄ Ìï®Íªò Î°úÎî© ÏÉÅÌÉú ÌëúÏãú
- ÏòÅÏÉÅÏù¥ ÎÅäÍ∏∏ Îïå "ÏòÅÏÉÅ Ïó∞Í≤∞Ïù¥ Î∂àÏïàÏ†ïÌï©ÎãàÎã§. ÏÉàÎ°úÍ≥†Ïπ®ÏùÑ ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî" Î≤ÑÌäºÍ≥º Ìï®Íªò ÏàòÎèô ÏÉàÎ°úÍ≥†Ïπ® ÏòµÏÖò Ï†úÍ≥µ
- Ïó¨Îü¨ Ïπ¥Î©îÎùº ÌôîÎ©¥ÏóêÏÑú Í∞ÅÍ∞Å "Ï†ÑÏ≤¥ÌôîÎ©¥" Î≤ÑÌäºÏùÑ Í∞úÎ≥Ñ Î∞∞ÏπòÌïòÏó¨ ÏõêÌïòÎäî Ïπ¥Î©îÎùºÎßå ÌÅ¨Í≤å Î≥º Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥Ñ
- CCTV Í∏∞Îä• ÏÇ¨Ïö© Ï§ë Î¨∏Ï†ú Î∞úÏÉù Ïãú "CCTVÏóê Î¨∏Ï†úÍ∞Ä ÏûàÎÇòÏöî?" ÌîºÎìúÎ∞± Î≤ÑÌäºÏúºÎ°ú ÏÇ¨Ïö©Ïûê ÏùòÍ≤¨ ÏàòÏßë"""
        
        else:
            return """- ÌïµÏã¨ Í∏∞Îä• ÏÇ¨Ïö© Ï§ë Î¨∏Ï†ú Î∞úÏÉù Ïãú "Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî" Î©îÏãúÏßÄÏôÄ Ìï®Íªò 'Ïû¨ÏãúÎèÑ' Î≤ÑÌäº Ï†úÍ≥µ
- Í∏∞Îä• Ïã§ÌñâÏù¥ Ïò§Îûò Í±∏Î¶¥ Îïå "Ï≤òÎ¶¨ Ï§ëÏûÖÎãàÎã§. Ïû†ÏãúÎßå Í∏∞Îã§Î†§Ï£ºÏÑ∏Ïöî" Î©îÏãúÏßÄÏôÄ Ìï®Íªò ÏßÑÌñâÎ•† ÌëúÏãú
- ÏûêÏ£º ÏÇ¨Ïö©ÌïòÎäî Í∏∞Îä•ÏùÑ Ìôà ÌôîÎ©¥ ÏÉÅÎã®Ïóê ÌÅ∞ Î≤ÑÌäºÏúºÎ°ú Î∞∞ÏπòÌïòÏó¨ ÏâΩÍ≤å Ï†ëÍ∑ºÌï† Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥Ñ
- Ïò§Î•ò Î∞úÏÉù Ïãú "Î¨∏Ï†úÍ∞Ä Í≥ÑÏÜçÎêòÎ©¥ Í≥†Í∞ùÏÑºÌÑ∞Î°ú Î¨∏ÏùòÌï¥Ï£ºÏÑ∏Ïöî" ÏïàÎÇ¥ÏôÄ Ìï®Íªò Í≥†Í∞ùÏÑºÌÑ∞ Ïó∞Í≤∞ Î≤ÑÌäº Ï†úÍ≥µ"""
    
    elif category == 'happiness':
        if "ÏÇ¨Ïö©Ïûê_Î∂àÎßå" in user_expressions:
            return """- ÏÇ¨Ïö©Ïûê Î∂àÎßå ÌëúÌòÑ Ïãú Ïï± ÎÇ¥ "ÏùòÍ≤¨ Î≥¥ÎÇ¥Í∏∞" Í∏∞Îä•ÏùÑ Î©îÎâ¥ ÏÉÅÎã®Ïóê ÏâΩÍ≤å Ï∞æÏùÑ Ïàò ÏûàÎèÑÎ°ù Î∞∞Ïπò
- Î¨∏Ï†ú Ìï¥Í≤∞ ÌõÑ "Î¨∏Ï†úÍ∞Ä Ìï¥Í≤∞ÎêòÏóàÎÇòÏöî?" ÌôïÏù∏ Î©îÏãúÏßÄÎ°ú ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ ÌôïÏù∏
- ÎãµÎãµÌï®ÏùÑ ÌëúÌòÑÌïòÎäî ÏÇ¨Ïö©ÏûêÎ•º ÏúÑÌï¥ "ÎèÑÏõÄÏù¥ ÌïÑÏöîÌïòÏãúÎ©¥ Ïñ∏Ï†úÎì† Î¨∏ÏùòÌï¥Ï£ºÏÑ∏Ïöî" Í∞ôÏùÄ Îî∞ÎúªÌïú Î©îÏãúÏßÄ Ï†úÍ≥µ
- Í∏çÏ†ïÏ†Å ÌîºÎìúÎ∞± Ïãú "ÎèÑÏõÄÏù¥ ÎêòÏóàÎã§Î©¥ Î≥ÑÏ†ê ÎÇ®Í∏∞Í∏∞" Îì±Ïùò ÏûêÏó∞Ïä§Îü¨Ïö¥ Ïú†ÎèÑ Î©îÏãúÏßÄ ÌëúÏãú"""
        else:
            return """- ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ Ï°∞ÏÇ¨Î•º ÌåùÏóÖÏù¥ ÏïÑÎãå Ïï± ÏÇ¨Ïö© ÌîåÎ°úÏö∞Ïóê ÏûêÏó∞Ïä§ÎüΩÍ≤å ÌÜµÌï©
- Î¨∏Ï†ú Ìï¥Í≤∞ ÌõÑ "Ìï¥Í≤∞ÎêòÏóàÎÇòÏöî?" ÌôïÏù∏ Î©îÏãúÏßÄÎ°ú ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ ÌôïÏù∏
- ÏÇ¨Ïö©Ïûê Î∂àÎßå ÌëúÌòÑ Ïãú Ïï± ÎÇ¥ "ÏùòÍ≤¨ Î≥¥ÎÇ¥Í∏∞" Í∏∞Îä•ÏùÑ ÏâΩÍ≤å Ï∞æÏùÑ Ïàò ÏûàÎèÑÎ°ù Î©îÎâ¥ ÏÉÅÎã®Ïóê Î∞∞Ïπò
- Í∏çÏ†ïÏ†Å ÌîºÎìúÎ∞± Ïãú "ÎèÑÏõÄÏù¥ ÎêòÏóàÎã§Î©¥ Î≥ÑÏ†ê ÎÇ®Í∏∞Í∏∞" Îì±Ïùò ÏûêÏó∞Ïä§Îü¨Ïö¥ Ïú†ÎèÑ Î©îÏãúÏßÄ ÌëúÏãú"""
    
    elif category == 'engagement':
        return """- ÏÇ¨Ïö©ÏûêÍ∞Ä ÌäπÏ†ï Í∏∞Îä•ÏùÑ ÏûêÏ£º ÏÇ¨Ïö©Ìï† Îïå "Ïù¥ Í∏∞Îä•ÎèÑ Ïú†Ïö©Ìï† Í≤É Í∞ôÏïÑÏöî" Í∞ôÏùÄ Í¥ÄÎ†® Í∏∞Îä• Ï∂îÏ≤ú Î©îÏãúÏßÄÎ•º Ï†ÅÏ†àÌïú ÌÉÄÏù¥Î∞çÏóê ÌëúÏãú
- Ïï± ÏÇ¨Ïö© Ìå®ÌÑ¥ÏùÑ Î∂ÑÏÑùÌïòÏó¨ "ÏûêÏ£º ÏÇ¨Ïö©ÌïòÎäî Í∏∞Îä•" ÏÑπÏÖòÏùÑ Ìôà ÌôîÎ©¥Ïóê Î∞∞Ïπò
- ÏÉàÎ°úÏö¥ Í∏∞Îä• Ï∂úÏãú Ïãú "ÏÉàÎ°úÏö¥ Í∏∞Îä•Ïù¥ Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§" ÏïåÎ¶ºÏùÑ Í∏∞Ï°¥ ÏÇ¨Ïö© Ìå®ÌÑ¥Í≥º Ïó∞Í≤∞ÌïòÏó¨ ÏûêÏó∞Ïä§ÎüΩÍ≤å ÏÜåÍ∞ú
- ÏÇ¨Ïö©Ïûê ÌôúÎèôÏù¥ Ï§ÑÏñ¥Îì§ Îïå "ÎÜìÏπòÏã† Í∏∞Îä•Ïù¥ ÏûàÏñ¥Ïöî" ÏïåÎ¶ºÏúºÎ°ú Ïû¨Ï∞∏Ïó¨ Ïú†ÎèÑ"""
    
    elif category == 'retention':
        return """- ÏÇ¨Ïö©ÏûêÍ∞Ä Ïï±ÏùÑ ÏÇ≠Ï†úÌïòÎ†§ Ìï† Îïå "Ïû†Íπê, Î¨∏Ï†úÍ∞Ä ÏûàÏúºÏã†Í∞ÄÏöî?" ÌåùÏóÖÏúºÎ°ú Ïù¥ÌÉà ÏÇ¨Ïú† ÌååÏïÖ Î∞è Ï¶âÏãú Ìï¥Í≤∞ ÏãúÎèÑ
- Ïû•Í∏∞Í∞Ñ ÎØ∏ÏÇ¨Ïö© Ïãú "ÎßàÏßÄÎßâÏúºÎ°ú ÏÇ¨Ïö©ÌïòÏÖ®Îçò Í∏∞Îä•" Ï§ëÏã¨ÏúºÎ°ú "Îã§Ïãú ÏãúÏûëÌï¥Î≥¥ÏÑ∏Ïöî" Î©îÏãúÏßÄÎ°ú Î≥µÍ∑Ä Ïú†ÎèÑ
- Í≥ÑÏ†ï ÏÇ≠Ï†ú Ï†Ñ "Îç∞Ïù¥ÌÑ∞Î•º Î∞±ÏóÖÌï¥ÎëêÏãúÍ≤†Ïñ¥Ïöî?" ÏòµÏÖò Ï†úÍ≥µÌïòÏó¨ Ïû¨ÏÇ¨Ïö© Í∞ÄÎä•ÏÑ± Ïó¥Ïñ¥ÎëêÍ∏∞
- ÏÇ¨Ïö©ÏûêÎ≥Ñ Ïù¥Ïö© Ìå®ÌÑ¥ Í∏∞Î∞ò "Ïù¥Îü∞ Í∏∞Îä•ÎèÑ ÏûàÏñ¥Ïöî" ÎßûÏ∂§Ìòï ÏïàÎÇ¥Î°ú ÏßÄÏÜç ÏÇ¨Ïö© Ïú†ÎèÑ"""
    
    elif category == 'adoption':
        return """- Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ïò®Î≥¥Îî© Ïãú "3Î∂Ñ ÎßåÏóê ÏãúÏûëÌïòÍ∏∞" Îì± Î™ÖÌôïÌïú ÏãúÍ∞Ñ ÏòàÏÉÅÏπò Ï†úÏãú
- Î≥µÏû°Ìïú Ï¥àÍ∏∞ ÏÑ§Ï†ïÏùÑ "ÎÇòÏ§ëÏóê ÏÑ§Ï†ïÌïòÍ∏∞" ÏòµÏÖòÍ≥º Ìï®Íªò Ï†úÍ≥µÌïòÏó¨ ÏßÑÏûÖ Ïû•Î≤Ω ÏôÑÌôî
- Ï≤´ ÏÑ±Í≥µ Í≤ΩÌóò ÌõÑ "ÏûòÌïòÏÖ®Ïñ¥Ïöî! Îã§Ïùå Îã®Í≥ÑÎ•º ÏïàÎÇ¥Ìï¥ÎìúÎ¶¥Í≤åÏöî" Î©îÏãúÏßÄÎ°ú ÏûêÏó∞Ïä§Îü¨Ïö¥ Í∏∞Îä• ÌôïÏû• Ïú†ÎèÑ
- ÏÇ¨Ïö© Î™©Ï†ÅÎ≥Ñ "Îπ†Î•∏ ÏãúÏûë" ÌÖúÌîåÎ¶ø Ï†úÍ≥µ (Ïòà: "CCTVÎßå ÏÇ¨Ïö©ÌïòÍ∏∞", "ÌÜµÌôî Í∏∞Îä• Ï§ëÏã¨ÏúºÎ°ú ÏãúÏûëÌïòÍ∏∞" Îì±)"""
    
    else:
        return """- ÏÇ¨Ïö©Ïûê Î¶¨Î∑∞ÏóêÏÑú Ïñ∏Í∏âÎêú Íµ¨Ï≤¥Ï†Å Î¨∏Ï†úÏ†êÏùÑ Ìï¥Í≤∞ÌïòÎäî "Îã®Í≥ÑÎ≥Ñ Í∞ÄÏù¥Îìú" Ï†úÍ≥µ
- ÏûêÏ£º Î∞úÏÉùÌïòÎäî Î¨∏Ï†úÏóê ÎåÄÌïú "ÏûêÏ£º Î¨ªÎäî ÏßàÎ¨∏" ÏÑπÏÖòÏùÑ Ïï± ÎÇ¥ ÏâΩÍ≤å Ï†ëÍ∑º Í∞ÄÎä•Ìïú ÏúÑÏπòÏóê Î∞∞Ïπò
- ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞±ÏùÑ Ïã§ÏãúÍ∞ÑÏúºÎ°ú ÏàòÏßëÌïòÍ≥† Îπ†Î•∏ Í∞úÏÑ† ÏÇ¨Ìï≠ÏùÑ "ÏóÖÎç∞Ïù¥Ìä∏ ÏÜåÏãù"ÏúºÎ°ú Ìà¨Î™ÖÌïòÍ≤å Í≥µÏú†
- Í∞Å Í∏∞Îä•Î≥Ñ "ÎèÑÏõÄÎßê" Î≤ÑÌäºÏùÑ ÏÉÅÌô©Ïóê ÎßûÍ≤å Î∞∞ÏπòÌïòÏó¨ Ï¶âÏãú ÎèÑÏõÄ Î∞õÏùÑ Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥Ñ"""

def main():
    """Main function to run the scraper"""
    try:
        # Parse command line arguments
        analyze_mode = '--analyze' in sys.argv
        
        if analyze_mode:
            # Remove --analyze flag from arguments
            args = [arg for arg in sys.argv[1:] if arg != '--analyze']
            
            if len(args) >= 3:
                # Format: python scraper.py --analyze google_app_id apple_app_id count sources
                app_id_google = args[0]
                app_id_apple = args[1]
                count = int(args[2])
                sources = args[3].split(',') if len(args) > 3 else ['google_play']
            else:
                # Legacy format: python scraper.py --analyze app_id count
                app_id_google = args[0] if len(args) > 0 else 'com.lguplus.sohoapp'
                app_id_apple = '1571096278'
                count = int(args[1]) if len(args) > 1 else 100
                sources = ['google_play']
        else:
            if len(sys.argv) > 3:
                # Format: python scraper.py google_app_id apple_app_id count sources
                app_id_google = sys.argv[1]
                app_id_apple = sys.argv[2]
                count = int(sys.argv[3])
                sources = sys.argv[4].split(',') if len(sys.argv) > 4 else ['google_play']
            else:
                # Legacy format: python scraper.py app_id count
                app_id_google = sys.argv[1] if len(sys.argv) > 1 else 'com.lguplus.sohoapp'
                app_id_apple = '1571096278'
                count = int(sys.argv[2]) if len(sys.argv) > 2 else 100
                sources = ['google_play']
        
        # Get reviews from specified sources
        service_name = 'ÏùµÏãúÏò§'  # Default service name
        reviews_data = scrape_reviews(app_id_google, app_id_apple, count, sources, service_name)
        
        if analyze_mode:
            # Perform analysis
            analysis_result = analyze_sentiments(reviews_data)
            result = {
                'success': True,
                'message': f'{len(reviews_data)}Í∞úÏùò Î¶¨Î∑∞Î•º Î∂ÑÏÑùÌñàÏäµÎãàÎã§.',
                'reviewCount': len(reviews_data),
                'insights': analysis_result['insights'],
                'wordCloud': analysis_result['wordCloud']
            }
        else:
            # Always include analysis for collection
            analysis_result = analyze_sentiments(reviews_data)
            result = {
                'success': True,
                'reviews': reviews_data,
                'reviewCount': len(reviews_data),
                'message': f'{len(reviews_data)}Í∞úÏùò Î¶¨Î∑∞Î•º ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏàòÏßëÌñàÏäµÎãàÎã§.',
                'analysis': analysis_result,
                'sources': sources,
                'counts': {
                    'google_play': len([r for r in reviews_data if r['source'] == 'google_play']),
                    'app_store': len([r for r in reviews_data if r['source'] == 'app_store']),
                    'naver_blog': len([r for r in reviews_data if r['source'] == 'naver_blog']),
                    'naver_cafe': len([r for r in reviews_data if r['source'] == 'naver_cafe'])
                }
            }
        
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
    except Exception as e:
        error_result = {
            'success': False,
            'error': str(e),
            'message': 'Î¶¨Î∑∞ ÏàòÏßë Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.'
        }
        print(json.dumps(error_result, ensure_ascii=False, indent=2))
        sys.exit(1)

if __name__ == "__main__":
    main()