#!/usr/bin/env python3
"""
Google Play Store Review Scraper for Ïö∞Î¶¨Í∞ÄÍ≤å Ìå®ÌÇ§ÏßÄ
"""

import json
import sys
import requests
from datetime import datetime
from google_play_scraper import Sort, reviews
import pandas as pd
import xml.etree.ElementTree as ET

def scrape_google_play_reviews(app_id='com.lguplus.sohoapp', count=100, lang='ko', country='kr'):
    """
    Scrape reviews from Google Play Store
    
    Args:
        app_id: Google Play Store app ID
        count: Number of reviews to fetch
        lang: Language code (default: 'ko')
        country: Country code (default: 'kr')
        
    Returns:
        List of review dictionaries
    """
    try:
        # Fetch reviews from Google Play Store
        result, _ = reviews(
            app_id,
            lang=lang,
            country=country,
            sort=Sort.NEWEST,
            count=count
        )
        
        # Process and clean the data
        processed_reviews = []
        for review in result:
            # Simple sentiment analysis based on score
            sentiment = "positive" if review['score'] >= 4 else "negative"
            
            processed_review = {
                'userId': review['userName'] if review['userName'] else 'ÏùµÎ™Ö',
                'source': 'google_play',
                'rating': review['score'],
                'content': review['content'],
                'sentiment': sentiment,
                'createdAt': review['at'].isoformat() if review['at'] else datetime.now().isoformat()
            }
            processed_reviews.append(processed_review)
        
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping Google Play reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_app_store_reviews(app_id='1571096278', count=100):
    """
    Scrape reviews from Apple App Store
    
    Args:
        app_id: Apple App Store app ID
        count: Number of reviews to fetch (limited by RSS feed)
        
    Returns:
        List of review dictionaries
    """
    try:
        # Construct RSS URL for App Store reviews
        rss_url = f'https://itunes.apple.com/kr/rss/customerreviews/id={app_id}/sortBy=mostRecent/xml'
        
        response = requests.get(rss_url, timeout=10)
        if response.status_code != 200:
            print(f"Failed to fetch App Store reviews: HTTP {response.status_code}", file=sys.stderr)
            return []
        
        root = ET.fromstring(response.content)
        
        # Process reviews (skip first entry which is just metadata)
        processed_reviews = []
        entries = root.findall('.//{http://www.w3.org/2005/Atom}entry')[1:]
        
        for entry in entries[:count]:  # Limit to requested count
            try:
                # Extract review data
                author_elem = entry.find('{http://www.w3.org/2005/Atom}author')
                author = author_elem[0].text if author_elem is not None and len(author_elem) > 0 else 'ÏùµÎ™Ö'
                
                title_elem = entry.find('{http://www.w3.org/2005/Atom}title')
                title = title_elem.text if title_elem is not None else ''
                
                content_elem = entry.find('{http://www.w3.org/2005/Atom}content')
                content = content_elem.text if content_elem is not None else title  # Use title if no content
                
                rating_elem = entry.find('{http://itunes.apple.com/rss}rating')
                rating = int(rating_elem.text) if rating_elem is not None else 3
                
                updated_elem = entry.find('{http://www.w3.org/2005/Atom}updated')
                created_at = updated_elem.text if updated_elem is not None else datetime.now().isoformat()
                
                # Simple sentiment analysis based on rating
                sentiment = "positive" if rating >= 4 else "negative"
                
                processed_review = {
                    'userId': author,
                    'source': 'app_store',
                    'rating': rating,
                    'content': content,
                    'sentiment': sentiment,
                    'createdAt': created_at
                }
                processed_reviews.append(processed_review)
                
            except Exception as entry_error:
                print(f"Error processing App Store review entry: {entry_error}", file=sys.stderr)
                continue
        
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping App Store reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_reviews(app_id_google='com.lguplus.sohoapp', app_id_apple='1571096278', count=100, sources=['google_play']):
    """
    Scrape reviews from multiple sources
    
    Args:
        app_id_google: Google Play Store app ID
        app_id_apple: Apple App Store app ID
        count: Number of reviews to fetch per source
        sources: List of sources to scrape from
        
    Returns:
        List of review dictionaries
    """
    all_reviews = []
    
    if 'google_play' in sources:
        google_reviews = scrape_google_play_reviews(app_id_google, count)
        all_reviews.extend(google_reviews)
        print(f"Collected {len(google_reviews)} reviews from Google Play", file=sys.stderr)
    
    if 'app_store' in sources:
        apple_reviews = scrape_app_store_reviews(app_id_apple, count)
        all_reviews.extend(apple_reviews)
        print(f"Collected {len(apple_reviews)} reviews from App Store", file=sys.stderr)
    
    return all_reviews

def analyze_sentiments(reviews):
    """
    Enhanced HEART framework analysis with dynamic insights generation
    
    Args:
        reviews: List of review dictionaries
        
    Returns:
        Dictionary with insights and word frequency data
    """
    if not reviews:
        return {'insights': [], 'wordCloud': {'positive': [], 'negative': []}}
    
    print(f"Starting enhanced HEART analysis on {len(reviews)} reviews...", file=sys.stderr)
    
    # HEART framework analysis with detailed issue tracking
    heart_analysis = {
        'task_success': {'issues': [], 'details': []},
        'happiness': {'issues': [], 'details': []},
        'engagement': {'issues': [], 'details': []},
        'adoption': {'issues': [], 'details': []},
        'retention': {'issues': [], 'details': []}
    }
    
    # Pattern matching for specific issues
    for review in reviews:
        content = review['content'].lower()
        rating = review.get('rating', 3)
        user_id = review.get('userId', 'Unknown')
        
        # Only analyze negative sentiment reviews for problems
        if rating < 4:
            # Task Success - Core functionality problems
            if any(keyword in content for keyword in ['Ïò§Î•ò', 'ÏóêÎü¨', 'Î≤ÑÍ∑∏', 'Ìäï', 'Í∫ºÏßê', 'ÏûëÎèôÏïàÌï®', 'Ïã§ÌñâÏïàÎê®', 'ÎÅäÍπÄ', 'Ïó∞Í≤∞ÏïàÎê®', 'ÏïàÎì§Î¶º', 'ÏÜåÎ¶¨ÏïàÎÇ®', 'ÏïàÎê®', 'ÏïàÎêò', 'ÌÅ¨ÎûòÏãú', 'Ï¢ÖÎ£å', 'Ïû¨ÏãúÏûë']):
                heart_analysis['task_success']['issues'].append(content)
                if 'Ìäï' in content or 'Í∫ºÏßê' in content or 'ÌÅ¨ÎûòÏãú' in content:
                    heart_analysis['task_success']['details'].append('Ïï± ÌÅ¨ÎûòÏãú')
                elif 'Ïó∞Í≤∞' in content and ('ÏïàÎê®' in content or 'ÎÅäÍπÄ' in content):
                    heart_analysis['task_success']['details'].append('ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞')
                elif 'ÏÜåÎ¶¨' in content and 'ÏïàÎÇ®' in content:
                    heart_analysis['task_success']['details'].append('ÏùåÏÑ± Í∏∞Îä•')
                else:
                    heart_analysis['task_success']['details'].append('Í∏∞Îä• Ïò§Î•ò')
            
            # Happiness - User satisfaction issues
            elif any(keyword in content for keyword in ['ÏßúÏ¶ù', 'ÏµúÏïÖ', 'Ïã§Îßù', 'ÌôîÎÇ®', 'Î∂àÎßå', 'Î≥ÑÎ°ú', 'Íµ¨Î¶º', 'Ïã´Ïñ¥', 'ÎãµÎãµ', 'Ïä§Ìä∏Î†àÏä§']):
                heart_analysis['happiness']['issues'].append(content)
                if 'ÏµúÏïÖ' in content or 'ÌôîÎÇ®' in content:
                    heart_analysis['happiness']['details'].append('Í∞ïÌïú Î∂àÎßå')
                else:
                    heart_analysis['happiness']['details'].append('ÎßåÏ°±ÎèÑ Ï†ÄÌïò')
            
            # Engagement - Usage patterns
            elif any(keyword in content for keyword in ['ÏïàÏç®', 'ÏÇ¨Ïö©ÏïàÌï®', 'Ïû¨ÎØ∏ÏóÜ', 'ÏßÄÎ£®', 'Ìù•ÎØ∏ÏóÜ', 'Î≥ÑÎ°úÏïàÏì¥', 'Í∞ÄÎÅîÎßå']):
                heart_analysis['engagement']['issues'].append(content)
                heart_analysis['engagement']['details'].append('ÏÇ¨Ïö© ÎπàÎèÑ Ï†ÄÌïò')
            
            # Retention - Churn indicators
            elif any(keyword in content for keyword in ['ÏÇ≠Ï†ú', 'Ìï¥ÏßÄ', 'Í∑∏Îßå', 'ÏïàÏì∏', 'Îã§Î•∏Í±∞', 'Î∞îÍøÄ', 'ÌÉàÌá¥', 'Ìè¨Í∏∞', 'Ï§ëÎã®']):
                heart_analysis['retention']['issues'].append(content)
                heart_analysis['retention']['details'].append('Ïù¥ÌÉà ÏúÑÌóò')
            
            # Adoption - Onboarding difficulties
            elif any(keyword in content for keyword in ['Ïñ¥Î†§ÏõÄ', 'Î≥µÏû°', 'Î™®Î•¥Í≤†', 'Ìó∑Í∞à', 'Ïñ¥ÎñªÍ≤å', 'ÏÑ§Î™ÖÎ∂ÄÏ°±', 'ÏÇ¨Ïö©Î≤ï', 'Í∞ÄÏù¥Îìú', 'ÎèÑÏõÄÎßê']):
                heart_analysis['adoption']['issues'].append(content)
                heart_analysis['adoption']['details'].append('ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú')
    
    # Generate insights based on actual review content analysis
    insights = []
    insight_id = 1
    
    # Business impact weights
    impact_weights = {
        'task_success': 5,  # Critical - core functionality
        'retention': 4,     # High - user churn
        'happiness': 3,     # Medium - satisfaction
        'engagement': 2,    # Low-Medium - usage
        'adoption': 1       # Low - onboarding
    }
    
    for category, data in heart_analysis.items():
        if data['issues']:
            count = len(data['issues'])
            impact_score = count * impact_weights[category]
            
            # Priority calculation
            if impact_score >= 15 or (category == 'task_success' and count >= 3):
                priority = "critical"
                priority_emoji = "üî¥"
            elif impact_score >= 8 or count >= 3:
                priority = "major"
                priority_emoji = "üü†"
            else:
                priority = "minor"
                priority_emoji = "üü¢"
            
            # Analyze actual review content to identify specific problems and solutions
            actual_issues = []
            for issue_text in data['issues']:
                # Extract key phrases and issues from actual reviews
                if 'ÌÅ¨ÎûòÏãú' in issue_text or 'Í∫ºÏ†∏' in issue_text or 'Í∫ºÏßÄ' in issue_text or 'ÌäïÍ≤®' in issue_text or 'ÌäïÍπÄ' in issue_text or 'ÎÇòÍ∞ÄÎ≤ÑÎ¶º' in issue_text:
                    actual_issues.append('Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å')
                elif ('Ï†ÑÌôî' in issue_text or 'ÌÜµÌôî' in issue_text) and ('ÎÅäÏñ¥' in issue_text or 'Î∞õ' in issue_text or 'ÏïàÎê®' in issue_text or 'ÎÅäÍπÄ' in issue_text):
                    actual_issues.append('ÌÜµÌôî Í∏∞Îä• Ïò§Î•ò')
                elif 'Ïó∞ÎùΩÏ≤ò' in issue_text and ('Í≤ÄÏÉâ' in issue_text or 'Ï°∞Ìöå' in issue_text or 'ÏïàÎ≥¥ÏûÖÎãàÎã§' in issue_text or 'Î™ªÌï¥Ïöî' in issue_text):
                    actual_issues.append('Ïó∞ÎùΩÏ≤ò Í≤ÄÏÉâ/Ï°∞Ìöå Î∂àÍ∞Ä')
                elif 'Ï∞®Îã®' in issue_text and ('ÏûêÎèô' in issue_text or 'Í∞ÄÏ°±' in issue_text or 'Ìï¥Ï†ú' in issue_text):
                    actual_issues.append('ÏûêÎèô Ï∞®Îã® Ïò§Î•ò')
                elif 'Ï†ÑÌôî' in issue_text and ('ÎÅäÍ∏¥Îã§' in issue_text or 'ÎÅäÍπÄ' in issue_text or 'ÏùºÎ¶º' in issue_text):
                    actual_issues.append('ÌÜµÌôî ÎÅäÍπÄ/ÏïåÎ¶º Ïã§Ìå®')
                elif 'Ïï†ÌîåÏõåÏπò' in issue_text or ('ÏõåÏπò' in issue_text and 'Ìò∏Ìôò' in issue_text):
                    actual_issues.append('Ïï†ÌîåÏõåÏπò Ìò∏ÌôòÏÑ± Î¨∏Ï†ú')
                elif 'Î≤ÑÎ≤Ö' in issue_text or ('ÏïÑÏù¥Ìè∞' in issue_text and 'ÌîÑÎ°ú' in issue_text and 'Î≤ÑÎ≤Ö' in issue_text):
                    actual_issues.append('ÏÑ±Îä• Ï†ÄÌïò (ÏµúÏã† Í∏∞Í∏∞)')
                elif 'Îã®Ï∂ïÎ≤àÌò∏' in issue_text or ('Îã®Ï∂ï' in issue_text and 'ÏÑ§Ï†ï' in issue_text):
                    actual_issues.append('Îã®Ï∂ïÎ≤àÌò∏ ÏÑ§Ï†ï Ïò§Î•ò')
                elif 'Î°úÍ∑∏Ïù∏' in issue_text or 'Ïù∏Ï¶ù' in issue_text or 'Î°úÍ∑∏' in issue_text:
                    actual_issues.append('Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú')
                elif 'ÎäêÎ¶º' in issue_text or 'ÏßÄÏó∞' in issue_text:
                    actual_issues.append('ÏÑ±Îä• Ï†ÄÌïò')
                elif 'Ïó∞Í≤∞' in issue_text or 'ÎÑ§Ìä∏ÏõåÌÅ¨' in issue_text or 'Ï†ëÏÜç' in issue_text:
                    actual_issues.append('ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ Î¨∏Ï†ú')
                elif 'Î∏îÎ£®Ìà¨Ïä§' in issue_text or 'ÏùåÏßà' in issue_text or 'ÏÜåÎ¶¨' in issue_text:
                    actual_issues.append('Ïò§ÎîîÏò§ ÌíàÏßà Î¨∏Ï†ú')
                elif 'ÏóÖÎç∞Ïù¥Ìä∏' in issue_text or 'Í∞úÏÑ†' in issue_text:
                    actual_issues.append('ÏóÖÎç∞Ïù¥Ìä∏ Í¥ÄÎ†® Î¨∏Ï†ú')
                elif 'Í∏∞Í∏∞' in issue_text or 'Ìè∞' in issue_text or 'Ìò∏Ìôò' in issue_text:
                    actual_issues.append('Í∏∞Í∏∞ Ìò∏ÌôòÏÑ± Î¨∏Ï†ú')
                elif 'Î∂àÌé∏' in issue_text or 'Î≥µÏû°' in issue_text or 'Ïñ¥Î†§ÏõÄ' in issue_text:
                    actual_issues.append('ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú')
                elif 'ÏÇ≠Ï†ú' in issue_text or 'Ìï¥ÏßÄ' in issue_text or 'Í∑∏Îßå' in issue_text:
                    actual_issues.append('ÏÑúÎπÑÏä§ Ï§ëÎã® ÏùòÎèÑ')
                elif 'Î≤ïÏù∏' in issue_text or 'Ïù¥Ïö©Ï†úÌïú' in issue_text or 'Ï†úÌïú' in issue_text:
                    actual_issues.append('Ïù¥Ïö© Ï†úÌïú Î¨∏Ï†ú')
                elif 'Í≤ÄÏÉâ' in issue_text or 'Ï°∞Ìöå' in issue_text or 'Ï∞æÍ∏∞' in issue_text:
                    actual_issues.append('Í≤ÄÏÉâ/Ï°∞Ìöå Í∏∞Îä• Ïò§Î•ò')
                else:
                    actual_issues.append('Í∏∞ÌÉÄ Î¨∏Ï†ú')
            
            # Find most common actual issue
            if actual_issues:
                most_common_issue = max(set(actual_issues), key=actual_issues.count)
                issue_count = actual_issues.count(most_common_issue)
            else:
                most_common_issue = 'Í∏∞ÌÉÄ Î¨∏Ï†ú'
                issue_count = count
            
            # Generate specific problems and solutions based on actual review content
            if category == 'task_success':
                title = "Task Success: ÌïµÏã¨ Í∏∞Îä• ÏïàÏ†ïÏÑ±"
                problem = f"{most_common_issue} {issue_count}Í±¥ Î∞úÏÉù"
                
                if most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å':
                    solution = "ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î©îÎ™®Î¶¨ ÎàÑÏàò ÏàòÏ†ï, ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî"
                elif most_common_issue == 'ÌÜµÌôî Í∏∞Îä• Ïò§Î•ò':
                    solution = "ÌÜµÌôî Ïó∞Í≤∞ Î°úÏßÅ Í∞úÏÑ†, Í∂åÌïú Í¥ÄÎ¶¨ ÏµúÏ†ÅÌôî, ÌÜµÌôî ÌíàÏßà ÌÖåÏä§Ìä∏"
                elif most_common_issue == 'Ïó∞ÎùΩÏ≤ò Í≤ÄÏÉâ/Ï°∞Ìöå Î∂àÍ∞Ä':
                    solution = "Ïó∞ÎùΩÏ≤ò DB Ïù∏Îç±Ïã± Ïû¨Íµ¨Ï∂ï, Í≤ÄÏÉâ ÏïåÍ≥†Î¶¨Ï¶ò ÏµúÏ†ÅÌôî, Í∂åÌïú Í¥ÄÎ¶¨ Ï†êÍ≤Ä"
                elif most_common_issue == 'ÏûêÎèô Ï∞®Îã® Ïò§Î•ò':
                    solution = "Ï∞®Îã® ÏïåÍ≥†Î¶¨Ï¶ò Î°úÏßÅ ÏàòÏ†ï, Í∞ÄÏ°±/ÏßÄÏù∏ Î≤àÌò∏ ÌôîÏù¥Ìä∏Î¶¨Ïä§Ìä∏ Í∏∞Îä• Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÌÜµÌôî ÎÅäÍπÄ/ÏïåÎ¶º Ïã§Ìå®':
                    solution = "ÌÜµÌôî Ïó∞Í≤∞ ÏïàÏ†ïÏÑ± Í∞ïÌôî, Ìë∏Ïãú ÏïåÎ¶º ÏãúÏä§ÌÖú Ï†êÍ≤Ä, Î∞±Í∑∏ÎùºÏö¥Îìú Ï≤òÎ¶¨ Í∞úÏÑ†"
                elif most_common_issue == 'Ïï†ÌîåÏõåÏπò Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    solution = "WatchOS Ïó∞Îèô API ÏóÖÎç∞Ïù¥Ìä∏, ÏõåÏπò Ï†ÑÏö© UI/UX Í∞úÎ∞ú, Í±∞Ï†à/ÏäπÏù∏ Î≤ÑÌäº Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò (ÏµúÏã† Í∏∞Í∏∞)':
                    solution = "ÏµúÏã† iOS ÏµúÏ†ÅÌôî, Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Î∂ÑÏÑù Î∞è Í∞úÏÑ†, GPU Î†åÎçîÎßÅ ÏµúÏ†ÅÌôî"
                elif most_common_issue == 'Îã®Ï∂ïÎ≤àÌò∏ ÏÑ§Ï†ï Ïò§Î•ò':
                    solution = "Îã®Ï∂ïÎ≤àÌò∏ API Î°úÏßÅ Ïû¨ÏûëÏÑ±, ÏÑ§Ï†ï Ï†ÄÏû•/Î∂àÎü¨Ïò§Í∏∞ Í∏∞Îä• Í∞úÏÑ†"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    solution = "Ïù∏Ï¶ù ÏÑúÎ≤Ñ ÏïàÏ†ïÏÑ± Í∞úÏÑ†, ÌÜ†ÌÅ∞ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú Ï†êÍ≤Ä"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò':
                    solution = "UI Î†åÎçîÎßÅ ÏµúÏ†ÅÌôî, Î∞±Í∑∏ÎùºÏö¥Îìú Ï≤òÎ¶¨ Í∞úÏÑ†"
                elif most_common_issue == 'ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ Î¨∏Ï†ú':
                    solution = "ÎÑ§Ìä∏ÏõåÌÅ¨ Ïû¨ÏãúÎèÑ Î°úÏßÅ Ï∂îÍ∞Ä, Ïò§ÌîÑÎùºÏù∏ Î™®Îìú ÏßÄÏõê"
                elif most_common_issue == 'Ïò§ÎîîÏò§ ÌíàÏßà Î¨∏Ï†ú':
                    solution = "Ïò§ÎîîÏò§ ÏΩîÎç± ÏµúÏ†ÅÌôî, Î∏îÎ£®Ìà¨Ïä§ Ìò∏ÌôòÏÑ± Í∞úÏÑ†"
                elif most_common_issue == 'Í≤ÄÏÉâ/Ï°∞Ìöå Í∏∞Îä• Ïò§Î•ò':
                    solution = "Í≤ÄÏÉâ Ïù∏Îç±Ïä§ ÏµúÏ†ÅÌôî, Ï°∞Ìöå ÏøºÎ¶¨ ÏÑ±Îä• Í∞úÏÑ†, Îç∞Ïù¥ÌÑ∞ Ï∫êÏã± Í∞ïÌôî"
                elif most_common_issue == 'Ïù¥Ïö© Ï†úÌïú Î¨∏Ï†ú':
                    solution = "Î≤ïÏù∏ ÏÇ¨Ïö©Ïûê Ï†ïÏ±Ö Í≤ÄÌÜ†, Ïù¥Ïö© Ï†úÌïú Ï°∞Í±¥ ÏôÑÌôî, ÏÇ¨Ïö©Ïûê Í∂åÌïú Í¥ÄÎ¶¨ Í∞úÏÑ†"
                else:
                    solution = "ÌïµÏã¨ Í∏∞Îä• QA ÌÖåÏä§Ìä∏ Í∞ïÌôî, Î≤ÑÍ∑∏ ÏàòÏ†ï ÌîÑÎ°úÏÑ∏Ïä§ Í∞úÏÑ†"
                    
            elif category == 'happiness':
                title = "Happiness: ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ Í∞úÏÑ†"
                problem = f"{most_common_issue} {issue_count}Í±¥ÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Î∂àÎßå"
                
                if most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å':
                    solution = "ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î©îÎ™®Î¶¨ ÎàÑÏàò ÏàòÏ†ï, ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    solution = "Ïù∏Ï¶ù ÏÑúÎ≤Ñ ÏïàÏ†ïÏÑ± Í∞úÏÑ†, ÌÜ†ÌÅ∞ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú Ï†êÍ≤Ä"
                elif most_common_issue == 'Ïï†ÌîåÏõåÏπò Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    solution = "WatchOS Ïó∞Îèô API ÏóÖÎç∞Ïù¥Ìä∏, ÏõåÏπò Ï†ÑÏö© UI/UX Í∞úÎ∞ú, Í±∞Ï†à/ÏäπÏù∏ Î≤ÑÌäº Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò (ÏµúÏã† Í∏∞Í∏∞)':
                    solution = "ÏµúÏã† iOS ÏµúÏ†ÅÌôî, Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Î∂ÑÏÑù Î∞è Í∞úÏÑ†, GPU Î†åÎçîÎßÅ ÏµúÏ†ÅÌôî"
                elif most_common_issue == 'ÏûêÎèô Ï∞®Îã® Ïò§Î•ò':
                    solution = "Ï∞®Îã® ÏïåÍ≥†Î¶¨Ï¶ò Î°úÏßÅ ÏàòÏ†ï, Í∞ÄÏ°±/ÏßÄÏù∏ Î≤àÌò∏ ÌôîÏù¥Ìä∏Î¶¨Ïä§Ìä∏ Í∏∞Îä• Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    solution = "UI/UX Í∞úÏÑ†, ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± Î∞òÏòÅÌïú Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïû¨ÏÑ§Í≥Ñ"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò':
                    solution = "Ïï± ÏÑ±Îä• ÏµúÏ†ÅÌôî, Î°úÎî© ÏãúÍ∞Ñ Îã®Ï∂ï, Î∞òÏùëÏÑ± Í∞úÏÑ†"
                elif most_common_issue == 'Í∏∞Í∏∞ Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    solution = "Îã§ÏñëÌïú Í∏∞Í∏∞ ÌÖåÏä§Ìä∏ ÌôïÎåÄ, Ìò∏ÌôòÏÑ± Îß§Ìä∏Î¶≠Ïä§ Íµ¨Ï∂ï"
                else:
                    solution = "ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ Ï°∞ÏÇ¨ Ïã§Ïãú, Ï£ºÏöî Î∂àÎßå ÏÇ¨Ìï≠ Ïö∞ÏÑ† Ìï¥Í≤∞"
                
            elif category == 'engagement':
                title = "Engagement: ÏÇ¨Ïö©Ïûê Ï∞∏Ïó¨ÎèÑ Ï¶ùÎåÄ"
                problem = f"{most_common_issue} {issue_count}Í±¥ÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö© ÎπàÎèÑ Ï†ÄÌïò"
                
                if most_common_issue == 'ÏóÖÎç∞Ïù¥Ìä∏ Í¥ÄÎ†® Î¨∏Ï†ú':
                    solution = "Ï†ïÍ∏∞Ï†ÅÏù∏ Í∏∞Îä• ÏóÖÎç∞Ïù¥Ìä∏, ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ Í∏∞Îä• Ïö∞ÏÑ† Í∞úÎ∞ú"
                elif most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    solution = "ÌïµÏã¨ Í∏∞Îä• Ï†ëÍ∑ºÏÑ± Í∞úÏÑ†, ÏßÅÍ¥ÄÏ†ÅÏù∏ ÎÑ§ÎπÑÍ≤åÏù¥ÏÖò Ï†úÍ≥µ"
                else:
                    solution = "ÏÇ¨Ïö©Ïûê Ï∞∏Ïó¨Î•º ÎÜíÏù¥Îäî ÏÉàÎ°úÏö¥ Í∏∞Îä• Ï∂îÍ∞Ä, Í∞úÏù∏Ìôî ÏÑúÎπÑÏä§ Í∞ïÌôî"
                
            elif category == 'retention':
                title = "Retention: ÏÇ¨Ïö©Ïûê Ïú†ÏßÄÏú® Í∞úÏÑ†"
                problem = f"{most_common_issue} {issue_count}Í±¥ÏúºÎ°ú Ïù∏Ìïú Ïù¥ÌÉà ÏúÑÌóò"
                
                if most_common_issue == 'ÏÑúÎπÑÏä§ Ï§ëÎã® ÏùòÎèÑ':
                    solution = "Ïù¥ÌÉà ÏòàÎ∞© ÌîÑÎ°úÍ∑∏Îû® Ïö¥ÏòÅ, Í≥†Í∞ù ÏÑúÎπÑÏä§ Í∞ïÌôî, ÌïµÏã¨ Í∞ÄÏπò Ïû¨Í∞ïÏ°∞"
                elif most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å':
                    solution = "ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î©îÎ™®Î¶¨ ÎàÑÏàò ÏàòÏ†ï, ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    solution = "Ïù∏Ï¶ù ÏÑúÎ≤Ñ ÏïàÏ†ïÏÑ± Í∞úÏÑ†, ÌÜ†ÌÅ∞ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú Ï†êÍ≤Ä"
                elif most_common_issue == 'Ïó∞ÎùΩÏ≤ò Í≤ÄÏÉâ/Ï°∞Ìöå Î∂àÍ∞Ä':
                    solution = "Ïó∞ÎùΩÏ≤ò DB Ïù∏Îç±Ïã± Ïû¨Íµ¨Ï∂ï, Í≤ÄÏÉâ ÏïåÍ≥†Î¶¨Ï¶ò ÏµúÏ†ÅÌôî, Í∂åÌïú Í¥ÄÎ¶¨ Ï†êÍ≤Ä"
                elif most_common_issue == 'ÏûêÎèô Ï∞®Îã® Ïò§Î•ò':
                    solution = "Ï∞®Îã® ÏïåÍ≥†Î¶¨Ï¶ò Î°úÏßÅ ÏàòÏ†ï, Í∞ÄÏ°±/ÏßÄÏù∏ Î≤àÌò∏ ÌôîÏù¥Ìä∏Î¶¨Ïä§Ìä∏ Í∏∞Îä• Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÌÜµÌôî ÎÅäÍπÄ/ÏïåÎ¶º Ïã§Ìå®':
                    solution = "ÌÜµÌôî Ïó∞Í≤∞ ÏïàÏ†ïÏÑ± Í∞ïÌôî, Ìë∏Ïãú ÏïåÎ¶º ÏãúÏä§ÌÖú Ï†êÍ≤Ä, Î∞±Í∑∏ÎùºÏö¥Îìú Ï≤òÎ¶¨ Í∞úÏÑ†"
                elif most_common_issue == 'Ïï†ÌîåÏõåÏπò Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    solution = "WatchOS Ïó∞Îèô API ÏóÖÎç∞Ïù¥Ìä∏, ÏõåÏπò Ï†ÑÏö© UI/UX Í∞úÎ∞ú, Í±∞Ï†à/ÏäπÏù∏ Î≤ÑÌäº Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò (ÏµúÏã† Í∏∞Í∏∞)':
                    solution = "ÏµúÏã† iOS ÏµúÏ†ÅÌôî, Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Î∂ÑÏÑù Î∞è Í∞úÏÑ†, GPU Î†åÎçîÎßÅ ÏµúÏ†ÅÌôî"
                elif most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    solution = "ÏÇ¨Ïö©Ïûê Ïò®Î≥¥Îî© Í∞úÏÑ†, ÏßÄÏÜçÏ†ÅÏù∏ Í∞ÄÏπò Ï†úÍ≥µ Î∞©Ïïà ÎßàÎ†®"
                else:
                    solution = "ÏÇ¨Ïö©Ïûê Ïú†ÏßÄÏú® Î∂ÑÏÑù, ÎßûÏ∂§Ìòï Î¶¨ÌÖêÏÖò Ï†ÑÎûµ ÏàòÎ¶Ω"
                
            elif category == 'adoption':
                title = "Adoption: Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ï†ÅÏùë ÏßÄÏõê"
                problem = f"{most_common_issue} {issue_count}Í±¥ÏúºÎ°ú Ïù∏Ìïú Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ï†ÅÏùë Ïñ¥Î†§ÏõÄ"
                
                if most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å':
                    solution = "ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î©îÎ™®Î¶¨ ÎàÑÏàò ÏàòÏ†ï, ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    solution = "Ïù∏Ï¶ù ÏÑúÎ≤Ñ ÏïàÏ†ïÏÑ± Í∞úÏÑ†, ÌÜ†ÌÅ∞ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú Ï†êÍ≤Ä"
                elif most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    solution = "Ïò®Î≥¥Îî© ÌîÑÎ°úÏÑ∏Ïä§ Í∞ÑÏÜåÌôî, Îã®Í≥ÑÎ≥Ñ Í∞ÄÏù¥Îìú Ï†úÍ≥µ, ÌäúÌÜ†Î¶¨Ïñº Í∞úÏÑ†"
                elif most_common_issue == 'Í∏∞Í∏∞ Ìò∏ÌôòÏÑ± Î¨∏Ï†ú':
                    solution = "Îã§ÏñëÌïú Í∏∞Í∏∞ ÏßÄÏõê ÌôïÎåÄ, ÏÑ§Ïπò Í∞ÄÏù¥Îìú Í∞úÏÑ†"
                else:
                    solution = "Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Í≤ΩÌóò ÏµúÏ†ÅÌôî, Ï¥àÍ∏∞ ÏÇ¨Ïö© Ïû•Î≤Ω Ï†úÍ±∞"
            
            # Generate realistic problem prediction and solution based on HEART category
            predicted_problem = ""
            realistic_solution = ""
            
            if category == 'task_success':
                if most_common_issue == 'ÌÜµÌôî Í∏∞Îä• Ïò§Î•ò':
                    predicted_problem = "ÌÜµÌôî Ïó∞Í≤∞ Ïã§Ìå®Î°ú Ïù∏Ìïú ÌïµÏã¨ Í∏∞Îä• ÏàòÌñâ Î∂àÍ∞Ä"
                    realistic_solution = "ÌÜµÌôî Ïó∞Í≤∞ Î°úÏßÅ Ï†êÍ≤Ä, VoIP ÏÑúÎ≤Ñ ÏïàÏ†ïÏÑ± Í∞ïÌôî, ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÉÅÌÉúÎ≥Ñ ÎåÄÏùë Î°úÏßÅ Í∞úÎ∞ú"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    predicted_problem = "Î°úÍ∑∏Ïù∏ Ïã§Ìå®Î°ú Ïù∏Ìïú ÏÑúÎπÑÏä§ Ï†ëÍ∑º Î∂àÍ∞Ä"
                    realistic_solution = "Ïù∏Ï¶ù ÏÑúÎ≤Ñ Î™®ÎãàÌÑ∞ÎßÅ Í∞ïÌôî, Îã§Ï§ë Ïù∏Ï¶ù Î∞©Ïãù Ï†úÍ≥µ, Î°úÍ∑∏Ïù∏ Ïã§Ìå® Ïãú Î™ÖÌôïÌïú ÏïàÎÇ¥ Î©îÏãúÏßÄ"
                else:
                    predicted_problem = "ÌïµÏã¨ Í∏∞Îä• Ïò§Î•òÎ°ú Ïù∏Ìïú ÏûëÏóÖ ÏôÑÎ£å Î∂àÍ∞Ä"
                    realistic_solution = "Í∏∞Îä•Î≥Ñ ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî, Ïò§Î•ò Î∞úÏÉù Ïãú Î≥µÍµ¨ Î©îÏª§ÎãàÏ¶ò Íµ¨Ï∂ï"
                    
            elif category == 'happiness':
                if most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    predicted_problem = "ÏßÅÍ¥ÄÏ†ÅÏù¥ÏßÄ ÏïäÏùÄ UI/UXÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ïä§Ìä∏Î†àÏä§"
                    realistic_solution = "ÏÇ¨Ïö©Ïûê ÌÖåÏä§Ìä∏ Ïã§Ïãú, ÎÑ§ÎπÑÍ≤åÏù¥ÏÖò Íµ¨Ï°∞ Îã®ÏàúÌôî, Ï£ºÏöî Í∏∞Îä• Ï†ëÍ∑ºÏÑ± Í∞úÏÑ†"
                elif most_common_issue == 'ÏÑ±Îä• Ï†ÄÌïò':
                    predicted_problem = "Ïï± Î°úÎî© ÏßÄÏó∞ÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê ÎãµÎãµÌï®"
                    realistic_solution = "ÏΩîÎìú ÏµúÏ†ÅÌôî, Ïù¥ÎØ∏ÏßÄ ÏïïÏ∂ï, Ï∫êÏã± Ï†ÑÎûµ Í∞úÏÑ†, Î°úÎî© Ïù∏ÎîîÏºÄÏù¥ÌÑ∞ Ï∂îÍ∞Ä"
                else:
                    predicted_problem = "ÏÇ¨Ïö©Ïûê Í∏∞ÎåÄÏôÄ Ïã§Ï†ú Í≤ΩÌóò Í∞ÑÏùò Í¥¥Î¶¨"
                    realistic_solution = "ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± Ï†ïÍ∏∞ ÏàòÏßë, ÌïµÏã¨ Î∂àÎßå ÏÇ¨Ìï≠ Ïö∞ÏÑ† Ìï¥Í≤∞, UX Í∞úÏÑ† ÌîÑÎ°úÏÑ∏Ïä§ Íµ¨Ï∂ï"
                    
            elif category == 'engagement':
                if most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    predicted_problem = "Î≥µÏû°Ìïú Í∏∞Îä• Íµ¨Ï°∞Î°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ï∞∏Ïó¨ÎèÑ Í∞êÏÜå"
                    realistic_solution = "ÌïµÏã¨ Í∏∞Îä• Ï†ëÍ∑ºÏÑ± Í∞úÏÑ†, Í∞úÏù∏Ìôî ÏïåÎ¶º ÏÑ§Ï†ï, ÏÇ¨Ïö© Ìå®ÌÑ¥ Î∂ÑÏÑù Í∏∞Î∞ò Í∏∞Îä• Ï∂îÏ≤ú"
                else:
                    predicted_problem = "Ïû¨Î∞©Î¨∏ ÎèôÍ∏∞ Î∂ÄÏ°±ÏúºÎ°ú Ïù∏Ìïú ÏÇ¨Ïö© ÎπàÎèÑ Ï†ÄÌïò"
                    realistic_solution = "Ìë∏Ïãú ÏïåÎ¶º Í∞úÏù∏Ìôî, ÏÇ¨Ïö©ÏûêÎ≥Ñ ÎßûÏ∂§ ÏΩòÌÖêÏ∏† Ï†úÍ≥µ, Ï†ïÍ∏∞Ï†Å ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Ïù¥Î≤§Ìä∏ ÏßÑÌñâ"
                    
            elif category == 'retention':
                if most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú/Í∞ïÏ†ú Ï¢ÖÎ£å':
                    predicted_problem = "Ïï± ÏïàÏ†ïÏÑ± Î¨∏Ï†úÎ°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ïù¥ÌÉà"
                    realistic_solution = "ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î≤ÑÍ∑∏ ÏàòÏ†ï, ÏïàÏ†ïÏÑ± ÌÖåÏä§Ìä∏ Í∞ïÌôî, Í∏¥Í∏â Ìå®Ïπò ÌîÑÎ°úÏÑ∏Ïä§ Íµ¨Ï∂ï"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    predicted_problem = "Î∞òÎ≥µÏ†ÅÏù∏ Î°úÍ∑∏Ïù∏ Ïã§Ìå®Î°ú Ïù∏Ìïú Ïû¨Î∞©Î¨∏Ïú® Í∞êÏÜå"
                    realistic_solution = "ÏûêÎèô Î°úÍ∑∏Ïù∏ Í∏∞Îä• Í∞úÏÑ†, ÏÜåÏÖú Î°úÍ∑∏Ïù∏ Ïó∞Îèô, ÎπÑÎ∞ÄÎ≤àÌò∏ Ï∞æÍ∏∞ ÌîÑÎ°úÏÑ∏Ïä§ Í∞ÑÏÜåÌôî"
                else:
                    predicted_problem = "ÏßÄÏÜçÏ†ÅÏù∏ Í∞ÄÏπò Ï†úÍ≥µ Ïã§Ìå®Î°ú Ïù∏Ìïú ÏÇ¨Ïö©Ïûê Ïù¥ÌÉà"
                    realistic_solution = "ÏÇ¨Ïö©Ïûê ÏÉùÎ™ÖÏ£ºÍ∏∞Î≥Ñ ÎßûÏ∂§ ÏÑúÎπÑÏä§ Ï†úÍ≥µ, Ïû¨Î∞©Î¨∏ Ïú†ÎèÑ ÏïåÎ¶º ÏµúÏ†ÅÌôî"
                    
            elif category == 'adoption':
                if most_common_issue == 'ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú':
                    predicted_problem = "Î≥µÏû°Ìïú Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î°ú Ïù∏Ìïú Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ïò®Î≥¥Îî© Ïù¥ÌÉà"
                    realistic_solution = "Ïò®Î≥¥Îî© ÌîåÎ°úÏö∞ Îã®ÏàúÌôî, Îã®Í≥ÑÎ≥Ñ Í∞ÄÏù¥Îìú Ï†úÍ≥µ, ÌïÑÏàò Í∏∞Îä• Ï§ëÏã¨ ÌäúÌÜ†Î¶¨Ïñº Íµ¨ÏÑ±"
                elif most_common_issue == 'Î°úÍ∑∏Ïù∏/Ïù∏Ï¶ù Î¨∏Ï†ú':
                    predicted_problem = "Ïù∏Ï¶ù Ïò§Î•òÎ°ú Ïù∏Ìïú Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ïú†ÏûÖ Ïã§Ìå®"
                    realistic_solution = "Í∞ÑÌé∏ ÌöåÏõêÍ∞ÄÏûÖ ÏòµÏÖò Ï†úÍ≥µ, Ïù∏Ï¶ù Í≥ºÏ†ï ÏµúÏÜåÌôî, ÏÑ§Ï†ï ÏûêÎèôÌôî Í∏∞Îä• Í∞ïÌôî"
                else:
                    predicted_problem = "ÌïµÏã¨ Í∞ÄÏπò Ïù¥Ìï¥ Î∂ÄÏ°±ÏúºÎ°ú Ïù∏Ìïú Ï¥àÍ∏∞ Ïù¥ÌÉàÎ•† Ï¶ùÍ∞Ä"
                    realistic_solution = "ÌïµÏã¨ Í∏∞Îä• Ïö∞ÏÑ† ÎÖ∏Ï∂ú, ÏÇ¨Ïö©Ïûê Ïú†ÌòïÎ≥Ñ ÎßûÏ∂§ Ïò®Î≥¥Îî©, Ï≤´ ÏÑ±Í≥µ Í≤ΩÌóò Î≥¥Ïû•"
            
            insights.append({
                'id': insight_id,
                'title': title,
                'description': f"üì¢ ÏòàÏ∏°ÎêòÎäî Î¨∏Ï†úÏ†ê\n{predicted_problem}\n\nüí° Ìï¥Í≤∞ Î∞©Î≤ï\n{realistic_solution}\n\n----------------------------------------\nHEART ÏöîÏÜå: {category.title().replace('_', ' ')}\nÎ¨∏Ï†ú ÏöîÏïΩ: {problem}\nÌï¥Í≤∞ Î∞©Î≤ï: {solution}\nÏö∞ÏÑ†ÏàúÏúÑ: {priority_emoji} {priority.title()}\n----------------------------------------",
                'priority': priority,
                'mentionCount': count,
                'trend': 'stable',
                'category': category
            })
            insight_id += 1
    
    # Sort by priority and impact
    priority_order = {'critical': 3, 'major': 2, 'minor': 1}
    insights.sort(key=lambda x: (priority_order[x['priority']], x['mentionCount']), reverse=True)
    
    # Limit to top 5 insights
    insights = insights[:5]
    
    # Enhanced Korean word frequency analysis (limit to top 10 each)
    positive_words = {}
    negative_words = {}
    
    # Stop words to exclude
    stop_words = ['Ïù¥Í≤É', 'Í∑∏Í≤É', 'Ï†ÄÍ≤É', 'ÏûàÎäî', 'ÏóÜÎäî', 'Í∞ôÏùÄ', 'Îã§Î•∏', 'Ïù¥Îü∞', 'Í∑∏Îü∞', 'Ï†ÄÎü∞', 'ÏóêÏÑú', 'ÏúºÎ°ú', 'ÏóêÍ≤å', 'ÌïúÌÖå', 'ÏóêÏÑúÎäî', 'Í∑∏Î¶¨Í≥†', 'ÌïòÏßÄÎßå', 'Í∑∏ÎûòÏÑú', 'Í∑∏Îü∞Îç∞']
    
    import re
    
    for review in reviews:
        content = review['content']
        rating = review.get('rating', 3)
        
        # Clean Korean text
        cleaned = re.sub(r'[^\w\sÍ∞Ä-Ìû£]', ' ', content)
        words = cleaned.split()
        
        # Filter meaningful Korean words
        korean_words = []
        for word in words:
            word = word.strip()
            if len(word) >= 2 and not word.isdigit() and word not in stop_words:
                # Check if word contains Korean characters
                if any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in word):
                    korean_words.append(word)
        
        # Classify by sentiment
        if rating >= 4:  # Positive
            for word in korean_words:
                positive_words[word] = positive_words.get(word, 0) + 1
        else:  # Negative
            for word in korean_words:
                negative_words[word] = negative_words.get(word, 0) + 1
    
    # Convert to word cloud format (top 10 each as requested)
    positive_cloud = [{'word': word, 'frequency': freq, 'sentiment': 'positive'} 
                      for word, freq in sorted(positive_words.items(), key=lambda x: x[1], reverse=True)[:10]]
    negative_cloud = [{'word': word, 'frequency': freq, 'sentiment': 'negative'} 
                      for word, freq in sorted(negative_words.items(), key=lambda x: x[1], reverse=True)[:10]]
    
    print(f"Generated {len(insights)} HEART insights, {len(positive_cloud)} positive words, {len(negative_cloud)} negative words", file=sys.stderr)
    
    return {
        'insights': insights,
        'wordCloud': {
            'positive': positive_cloud,
            'negative': negative_cloud
        }
    }

def main():
    """Main function to run the scraper"""
    try:
        # Parse command line arguments
        analyze_mode = '--analyze' in sys.argv
        
        if analyze_mode:
            # Remove --analyze flag from arguments
            args = [arg for arg in sys.argv[1:] if arg != '--analyze']
            
            if len(args) >= 3:
                # Format: python scraper.py --analyze google_app_id apple_app_id count sources
                app_id_google = args[0]
                app_id_apple = args[1]
                count = int(args[2])
                sources = args[3].split(',') if len(args) > 3 else ['google_play']
            else:
                # Legacy format: python scraper.py --analyze app_id count
                app_id_google = args[0] if len(args) > 0 else 'com.lguplus.sohoapp'
                app_id_apple = '1571096278'
                count = int(args[1]) if len(args) > 1 else 100
                sources = ['google_play']
        else:
            if len(sys.argv) > 3:
                # Format: python scraper.py google_app_id apple_app_id count sources
                app_id_google = sys.argv[1]
                app_id_apple = sys.argv[2]
                count = int(sys.argv[3])
                sources = sys.argv[4].split(',') if len(sys.argv) > 4 else ['google_play']
            else:
                # Legacy format: python scraper.py app_id count
                app_id_google = sys.argv[1] if len(sys.argv) > 1 else 'com.lguplus.sohoapp'
                app_id_apple = '1571096278'
                count = int(sys.argv[2]) if len(sys.argv) > 2 else 100
                sources = ['google_play']
        
        # Get reviews from specified sources
        reviews_data = scrape_reviews(app_id_google, app_id_apple, count, sources)
        
        if analyze_mode:
            # Perform analysis
            analysis_result = analyze_sentiments(reviews_data)
            result = {
                'success': True,
                'message': f'{len(reviews_data)}Í∞úÏùò Î¶¨Î∑∞Î•º Î∂ÑÏÑùÌñàÏäµÎãàÎã§.',
                'reviewCount': len(reviews_data),
                'insights': analysis_result['insights'],
                'wordCloud': analysis_result['wordCloud']
            }
        else:
            # Always include analysis for collection
            analysis_result = analyze_sentiments(reviews_data)
            result = {
                'success': True,
                'reviews': reviews_data,
                'reviewCount': len(reviews_data),
                'message': f'{len(reviews_data)}Í∞úÏùò Î¶¨Î∑∞Î•º ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏàòÏßëÌñàÏäµÎãàÎã§.',
                'analysis': analysis_result,
                'sources': sources,
                'counts': {
                    'google_play': len([r for r in reviews_data if r['source'] == 'google_play']),
                    'app_store': len([r for r in reviews_data if r['source'] == 'app_store'])
                }
            }
        
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
    except Exception as e:
        error_result = {
            'success': False,
            'error': str(e),
            'message': 'Î¶¨Î∑∞ ÏàòÏßë Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.'
        }
        print(json.dumps(error_result, ensure_ascii=False, indent=2))
        sys.exit(1)

if __name__ == "__main__":
    main()