#!/usr/bin/env python3
"""
Google Play Store Review Scraper for Ïö∞Î¶¨Í∞ÄÍ≤å Ìå®ÌÇ§ÏßÄ
"""

import json
import sys
import requests
from datetime import datetime
from google_play_scraper import Sort, reviews
import pandas as pd
import xml.etree.ElementTree as ET

def scrape_google_play_reviews(app_id='com.lguplus.sohoapp', count=100, lang='ko', country='kr'):
    """
    Scrape reviews from Google Play Store
    
    Args:
        app_id: Google Play Store app ID
        count: Number of reviews to fetch
        lang: Language code (default: 'ko')
        country: Country code (default: 'kr')
        
    Returns:
        List of review dictionaries
    """
    try:
        # Fetch reviews from Google Play Store
        result, _ = reviews(
            app_id,
            lang=lang,
            country=country,
            sort=Sort.NEWEST,
            count=count
        )
        
        # Process and clean the data
        processed_reviews = []
        for review in result:
            # Simple sentiment analysis based on score
            sentiment = "positive" if review['score'] >= 4 else "negative"
            
            processed_review = {
                'userId': review['userName'] if review['userName'] else 'ÏùµÎ™Ö',
                'source': 'google_play',
                'rating': review['score'],
                'content': review['content'],
                'sentiment': sentiment,
                'createdAt': review['at'].isoformat() if review['at'] else datetime.now().isoformat()
            }
            processed_reviews.append(processed_review)
        
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping Google Play reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_app_store_reviews(app_id='1571096278', count=100):
    """
    Scrape reviews from Apple App Store
    
    Args:
        app_id: Apple App Store app ID
        count: Number of reviews to fetch (limited by RSS feed)
        
    Returns:
        List of review dictionaries
    """
    try:
        # Construct RSS URL for App Store reviews
        rss_url = f'https://itunes.apple.com/kr/rss/customerreviews/id={app_id}/sortBy=mostRecent/xml'
        
        response = requests.get(rss_url, timeout=10)
        if response.status_code != 200:
            print(f"Failed to fetch App Store reviews: HTTP {response.status_code}", file=sys.stderr)
            return []
        
        root = ET.fromstring(response.content)
        
        # Process reviews (skip first entry which is just metadata)
        processed_reviews = []
        entries = root.findall('.//{http://www.w3.org/2005/Atom}entry')[1:]
        
        for entry in entries[:count]:  # Limit to requested count
            try:
                # Extract review data
                author_elem = entry.find('{http://www.w3.org/2005/Atom}author')
                author = author_elem[0].text if author_elem is not None and len(author_elem) > 0 else 'ÏùµÎ™Ö'
                
                title_elem = entry.find('{http://www.w3.org/2005/Atom}title')
                title = title_elem.text if title_elem is not None else ''
                
                content_elem = entry.find('{http://www.w3.org/2005/Atom}content')
                content = content_elem.text if content_elem is not None else title  # Use title if no content
                
                rating_elem = entry.find('{http://itunes.apple.com/rss}rating')
                rating = int(rating_elem.text) if rating_elem is not None else 3
                
                updated_elem = entry.find('{http://www.w3.org/2005/Atom}updated')
                created_at = updated_elem.text if updated_elem is not None else datetime.now().isoformat()
                
                # Simple sentiment analysis based on rating
                sentiment = "positive" if rating >= 4 else "negative"
                
                processed_review = {
                    'userId': author,
                    'source': 'app_store',
                    'rating': rating,
                    'content': content,
                    'sentiment': sentiment,
                    'createdAt': created_at
                }
                processed_reviews.append(processed_review)
                
            except Exception as entry_error:
                print(f"Error processing App Store review entry: {entry_error}", file=sys.stderr)
                continue
        
        return processed_reviews
        
    except Exception as e:
        print(f"Error scraping App Store reviews: {str(e)}", file=sys.stderr)
        return []

def scrape_reviews(app_id_google='com.lguplus.sohoapp', app_id_apple='1571096278', count=100, sources=['google_play']):
    """
    Scrape reviews from multiple sources
    
    Args:
        app_id_google: Google Play Store app ID
        app_id_apple: Apple App Store app ID
        count: Number of reviews to fetch per source
        sources: List of sources to scrape from
        
    Returns:
        List of review dictionaries
    """
    all_reviews = []
    
    if 'google_play' in sources:
        google_reviews = scrape_google_play_reviews(app_id_google, count)
        all_reviews.extend(google_reviews)
        print(f"Collected {len(google_reviews)} reviews from Google Play", file=sys.stderr)
    
    if 'app_store' in sources:
        apple_reviews = scrape_app_store_reviews(app_id_apple, count)
        all_reviews.extend(apple_reviews)
        print(f"Collected {len(apple_reviews)} reviews from App Store", file=sys.stderr)
    
    return all_reviews

def analyze_sentiments(reviews):
    """
    Enhanced HEART framework analysis with dynamic insights generation
    
    Args:
        reviews: List of review dictionaries
        
    Returns:
        Dictionary with insights and word frequency data
    """
    if not reviews:
        return {'insights': [], 'wordCloud': {'positive': [], 'negative': []}}
    
    print(f"Starting enhanced HEART analysis on {len(reviews)} reviews...", file=sys.stderr)
    
    # HEART framework analysis with detailed issue tracking
    heart_analysis = {
        'task_success': {'issues': [], 'details': []},
        'happiness': {'issues': [], 'details': []},
        'engagement': {'issues': [], 'details': []},
        'adoption': {'issues': [], 'details': []},
        'retention': {'issues': [], 'details': []}
    }
    
    # Pattern matching for specific issues
    for review in reviews:
        content = review['content'].lower()
        rating = review.get('rating', 3)
        user_id = review.get('userId', 'Unknown')
        
        # Only analyze negative sentiment reviews for problems
        if rating < 4:
            # Task Success - Core functionality problems
            if any(keyword in content for keyword in ['Ïò§Î•ò', 'ÏóêÎü¨', 'Î≤ÑÍ∑∏', 'ÌäïÍπÄ', 'Í∫ºÏßê', 'ÏûëÎèôÏïàÌï®', 'Ïã§ÌñâÏïàÎê®', 'ÎÅäÍπÄ', 'Ïó∞Í≤∞ÏïàÎê®', 'ÏïàÎì§Î¶º', 'ÏÜåÎ¶¨ÏïàÎÇ®']):
                heart_analysis['task_success']['issues'].append(content)
                if 'ÌäïÍπÄ' in content or 'Í∫ºÏßê' in content:
                    heart_analysis['task_success']['details'].append('Ïï± ÌÅ¨ÎûòÏãú')
                elif 'Ïó∞Í≤∞' in content and ('ÏïàÎê®' in content or 'ÎÅäÍπÄ' in content):
                    heart_analysis['task_success']['details'].append('ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞')
                elif 'ÏÜåÎ¶¨' in content and 'ÏïàÎÇ®' in content:
                    heart_analysis['task_success']['details'].append('ÏùåÏÑ± Í∏∞Îä•')
                else:
                    heart_analysis['task_success']['details'].append('Í∏∞Îä• Ïò§Î•ò')
            
            # Happiness - User satisfaction issues
            elif any(keyword in content for keyword in ['ÏßúÏ¶ù', 'ÏµúÏïÖ', 'Ïã§Îßù', 'ÌôîÎÇ®', 'Î∂àÎßå', 'Î≥ÑÎ°ú', 'Íµ¨Î¶º', 'Ïã´Ïñ¥', 'ÎãµÎãµ', 'Ïä§Ìä∏Î†àÏä§']):
                heart_analysis['happiness']['issues'].append(content)
                if 'ÏµúÏïÖ' in content or 'ÌôîÎÇ®' in content:
                    heart_analysis['happiness']['details'].append('Í∞ïÌïú Î∂àÎßå')
                else:
                    heart_analysis['happiness']['details'].append('ÎßåÏ°±ÎèÑ Ï†ÄÌïò')
            
            # Engagement - Usage patterns
            elif any(keyword in content for keyword in ['ÏïàÏç®', 'ÏÇ¨Ïö©ÏïàÌï®', 'Ïû¨ÎØ∏ÏóÜ', 'ÏßÄÎ£®', 'Ìù•ÎØ∏ÏóÜ', 'Î≥ÑÎ°úÏïàÏì¥', 'Í∞ÄÎÅîÎßå']):
                heart_analysis['engagement']['issues'].append(content)
                heart_analysis['engagement']['details'].append('ÏÇ¨Ïö© ÎπàÎèÑ Ï†ÄÌïò')
            
            # Retention - Churn indicators
            elif any(keyword in content for keyword in ['ÏÇ≠Ï†ú', 'Ìï¥ÏßÄ', 'Í∑∏Îßå', 'ÏïàÏì∏', 'Îã§Î•∏Í±∞', 'Î∞îÍøÄ', 'ÌÉàÌá¥', 'Ìè¨Í∏∞', 'Ï§ëÎã®']):
                heart_analysis['retention']['issues'].append(content)
                heart_analysis['retention']['details'].append('Ïù¥ÌÉà ÏúÑÌóò')
            
            # Adoption - Onboarding difficulties
            elif any(keyword in content for keyword in ['Ïñ¥Î†§ÏõÄ', 'Î≥µÏû°', 'Î™®Î•¥Í≤†', 'Ìó∑Í∞à', 'Ïñ¥ÎñªÍ≤å', 'ÏÑ§Î™ÖÎ∂ÄÏ°±', 'ÏÇ¨Ïö©Î≤ï', 'Í∞ÄÏù¥Îìú', 'ÎèÑÏõÄÎßê']):
                heart_analysis['adoption']['issues'].append(content)
                heart_analysis['adoption']['details'].append('ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú')
    
    # Generate insights based on analysis
    insights = []
    insight_id = 1
    
    # Business impact weights
    impact_weights = {
        'task_success': 5,  # Critical - core functionality
        'retention': 4,     # High - user churn
        'happiness': 3,     # Medium - satisfaction
        'engagement': 2,    # Low-Medium - usage
        'adoption': 1       # Low - onboarding
    }
    
    for category, data in heart_analysis.items():
        if data['issues']:
            count = len(data['issues'])
            impact_score = count * impact_weights[category]
            
            # Priority calculation
            if impact_score >= 15 or (category == 'task_success' and count >= 3):
                priority = "critical"
                priority_emoji = "üî¥"
            elif impact_score >= 8 or count >= 3:
                priority = "major"
                priority_emoji = "üü†"
            else:
                priority = "minor"
                priority_emoji = "üü¢"
            
            # Generate specific insights with solutions
            if category == 'task_success':
                most_common_issue = max(set(data['details']), key=data['details'].count) if data['details'] else 'Í∏∞Îä• Ïò§Î•ò'
                title = "Task Success: ÌïµÏã¨ Í∏∞Îä• ÏïàÏ†ïÏÑ±"
                problem = f"{most_common_issue} {data['details'].count(most_common_issue)}Í±¥ Î∞úÏÉù"
                if most_common_issue == 'Ïï± ÌÅ¨ÎûòÏãú':
                    solution = "Ï¶âÏãú ÌÅ¨ÎûòÏãú Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ Í∞úÏÑ†"
                elif most_common_issue == 'ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞':
                    solution = "ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ ÏïàÏ†ïÏÑ± Í∞úÏÑ† Î∞è Ïû¨ÏãúÎèÑ Î°úÏßÅ Ï∂îÍ∞Ä"
                elif most_common_issue == 'ÏùåÏÑ± Í∏∞Îä•':
                    solution = "Ïò§ÎîîÏò§ Í∂åÌïú Î∞è ÏΩîÎç± Ìò∏ÌôòÏÑ± Ï†êÍ≤Ä"
                else:
                    solution = "ÌïµÏã¨ Í∏∞Îä• QA ÌÖåÏä§Ìä∏ Í∞ïÌôî Î∞è Î≤ÑÍ∑∏ ÏàòÏ†ï"
                    
            elif category == 'happiness':
                title = "Happiness: ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ Í∞úÏÑ†"
                strong_complaints = data['details'].count('Í∞ïÌïú Î∂àÎßå')
                problem = f"ÏÇ¨Ïö©Ïûê Î∂àÎßå {count}Í±¥ (Í∞ïÌïú Î∂àÎßå {strong_complaints}Í±¥)"
                solution = "Î∂àÎßå ÏÇ¨Ïö©Ïûê ÏßÅÏ†ë ÏÜåÌÜµ, Ï£ºÏöî Í∞úÏÑ†ÏÇ¨Ìï≠ Ïö∞ÏÑ† Ï†ÅÏö©"
                
            elif category == 'engagement':
                title = "Engagement: ÏÇ¨Ïö©Ïûê Ï∞∏Ïó¨ÎèÑ Ï¶ùÎåÄ"
                problem = f"ÏÇ¨Ïö© ÎπàÎèÑ Ï†ÄÌïò {count}Í±¥ ÌôïÏù∏"
                solution = "ÌïµÏã¨ Í∏∞Îä• Ï†ëÍ∑ºÏÑ± Í∞úÏÑ†, ÏÇ¨Ïö©Ïûê ÎßûÏ∂§ ÏΩòÌÖêÏ∏† Ï†úÍ≥µ"
                
            elif category == 'retention':
                title = "Retention: ÏÇ¨Ïö©Ïûê Ïú†ÏßÄÏú® Í∞úÏÑ†"
                problem = f"Ïù¥ÌÉà ÏúÑÌóò ÏÇ¨Ïö©Ïûê {count}Í±¥ Í∞êÏßÄ"
                solution = "Ïù¥ÌÉà ÏòàÎ∞© ÌîÑÎ°úÍ∑∏Îû® Ïö¥ÏòÅ, ÌïµÏã¨ Í∞ÄÏπò Ïû¨Í∞ïÏ°∞"
                
            elif category == 'adoption':
                title = "Adoption: Ïã†Í∑ú ÏÇ¨Ïö©Ïûê Ï†ÅÏùë ÏßÄÏõê"
                problem = f"ÏÇ¨Ïö©ÏÑ± Î¨∏Ï†ú {count}Í±¥ Ï†ëÏàò"
                solution = "Ïò®Î≥¥Îî© ÌîÑÎ°úÏÑ∏Ïä§ Í∞ÑÏÜåÌôî, Í∞ÄÏù¥Îìú Í∞úÏÑ†"
            
            insights.append({
                'id': insight_id,
                'title': title,
                'description': f"----------------------------------------\nHEART ÏöîÏÜå: {category.title().replace('_', ' ')}\nÎ¨∏Ï†ú ÏöîÏïΩ: {problem}\nÌï¥Í≤∞ Î∞©Î≤ï: {solution}\nÏö∞ÏÑ†ÏàúÏúÑ: {priority_emoji} {priority.title()}\n----------------------------------------",
                'priority': priority,
                'mentionCount': count,
                'trend': 'stable',
                'category': category
            })
            insight_id += 1
    
    # Sort by priority and impact
    priority_order = {'critical': 3, 'major': 2, 'minor': 1}
    insights.sort(key=lambda x: (priority_order[x['priority']], x['mentionCount']), reverse=True)
    
    # Limit to top 5 insights
    insights = insights[:5]
    
    # Enhanced Korean word frequency analysis (limit to top 10 each)
    positive_words = {}
    negative_words = {}
    
    # Stop words to exclude
    stop_words = ['Ïù¥Í≤É', 'Í∑∏Í≤É', 'Ï†ÄÍ≤É', 'ÏûàÎäî', 'ÏóÜÎäî', 'Í∞ôÏùÄ', 'Îã§Î•∏', 'Ïù¥Îü∞', 'Í∑∏Îü∞', 'Ï†ÄÎü∞', 'ÏóêÏÑú', 'ÏúºÎ°ú', 'ÏóêÍ≤å', 'ÌïúÌÖå', 'ÏóêÏÑúÎäî', 'Í∑∏Î¶¨Í≥†', 'ÌïòÏßÄÎßå', 'Í∑∏ÎûòÏÑú', 'Í∑∏Îü∞Îç∞']
    
    import re
    
    for review in reviews:
        content = review['content']
        rating = review.get('rating', 3)
        
        # Clean Korean text
        cleaned = re.sub(r'[^\w\sÍ∞Ä-Ìû£]', ' ', content)
        words = cleaned.split()
        
        # Filter meaningful Korean words
        korean_words = []
        for word in words:
            word = word.strip()
            if len(word) >= 2 and not word.isdigit() and word not in stop_words:
                # Check if word contains Korean characters
                if any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in word):
                    korean_words.append(word)
        
        # Classify by sentiment
        if rating >= 4:  # Positive
            for word in korean_words:
                positive_words[word] = positive_words.get(word, 0) + 1
        else:  # Negative
            for word in korean_words:
                negative_words[word] = negative_words.get(word, 0) + 1
    
    # Convert to word cloud format (top 10 each as requested)
    positive_cloud = [{'word': word, 'frequency': freq, 'sentiment': 'positive'} 
                      for word, freq in sorted(positive_words.items(), key=lambda x: x[1], reverse=True)[:10]]
    negative_cloud = [{'word': word, 'frequency': freq, 'sentiment': 'negative'} 
                      for word, freq in sorted(negative_words.items(), key=lambda x: x[1], reverse=True)[:10]]
    
    print(f"Generated {len(insights)} HEART insights, {len(positive_cloud)} positive words, {len(negative_cloud)} negative words", file=sys.stderr)
    
    return {
        'insights': insights,
        'wordCloud': {
            'positive': positive_cloud,
            'negative': negative_cloud
        }
    }

def main():
    """Main function to run the scraper"""
    try:
        # Parse command line arguments
        if len(sys.argv) > 3:
            # Format: python scraper.py google_app_id apple_app_id count sources
            app_id_google = sys.argv[1]
            app_id_apple = sys.argv[2]
            count = int(sys.argv[3])
            sources = sys.argv[4].split(',') if len(sys.argv) > 4 else ['google_play']
        else:
            # Legacy format: python scraper.py app_id count
            app_id_google = sys.argv[1] if len(sys.argv) > 1 else 'com.lguplus.sohoapp'
            app_id_apple = '1571096278'  # Default Apple App Store ID for Ïö∞Î¶¨Í∞ÄÍ≤å Ìå®ÌÇ§ÏßÄ
            count = int(sys.argv[2]) if len(sys.argv) > 2 else 100
            sources = ['google_play']
        
        # Scrape reviews
        reviews_data = scrape_reviews(app_id_google, app_id_apple, count, sources)
        
        # Analyze sentiments and generate insights
        analysis = analyze_sentiments(reviews_data)
        
        # Output results as JSON
        result = {
            'success': True,
            'reviews': reviews_data,
            'analysis': analysis,
            'message': f'{len(reviews_data)}Í∞úÏùò Î¶¨Î∑∞Î•º ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏàòÏßëÌñàÏäµÎãàÎã§.',
            'sources': sources,
            'counts': {
                'google_play': len([r for r in reviews_data if r['source'] == 'google_play']),
                'app_store': len([r for r in reviews_data if r['source'] == 'app_store'])
            }
        }
        
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
    except Exception as e:
        error_result = {
            'success': False,
            'error': str(e),
            'message': 'Î¶¨Î∑∞ ÏàòÏßë Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.'
        }
        print(json.dumps(error_result, ensure_ascii=False, indent=2))
        sys.exit(1)

if __name__ == "__main__":
    main()