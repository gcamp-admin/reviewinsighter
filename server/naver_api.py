#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Naver API Integration for Blog and Cafe Search
"""

import requests
import urllib.parse
import sys
import re

import os
NAVER_CLIENT_ID = os.environ.get('NAVER_CLIENT_ID')
NAVER_CLIENT_SECRET = os.environ.get('NAVER_CLIENT_SECRET')

def extract_user_id_from_url(bloggerlink, link, search_type):
    """
    Extract user ID from Naver Blog or Cafe URL
    
    Args:
        bloggerlink: Blogger link (for blog)
        link: Direct link to the content
        search_type: 'blog' or 'cafe'
        
    Returns:
        Extracted user ID or default value
    """
    try:
        if search_type == "blog":
            # For blog, try to extract from bloggerlink first
            if bloggerlink:
                # Pattern: https://blog.naver.com/USERNAME
                match = re.search(r'blog\.naver\.com/([^/?]+)', bloggerlink)
                if match:
                    return match.group(1)
            
            # If bloggerlink fails, try from link
            if link:
                # Pattern: https://blog.naver.com/USERNAME/POST_ID
                match = re.search(r'blog\.naver\.com/([^/?]+)', link)
                if match:
                    return match.group(1)
                    
        elif search_type == "cafe":
            # For cafe, extract from link
            if link:
                # Pattern: https://cafe.naver.com/CAFE_NAME/ARTICLE_ID
                match = re.search(r'cafe\.naver\.com/([^/?]+)', link)
                if match:
                    return f"ì¹´í˜_{match.group(1)}"
                    
        return None
        
    except Exception as e:
        print(f"Error extracting user ID: {str(e)}", file=sys.stderr)
        return None

def search_naver(keyword, search_type="blog", display=10):
    """
    Search Naver Blog or Cafe articles
    
    Args:
        keyword: Search keyword
        search_type: 'blog' or 'cafe'
        display: Number of results to return (max 100)
        
    Returns:
        List of search results
    """
    base_url = {
        "blog": "https://openapi.naver.com/v1/search/blog",
        "cafe": "https://openapi.naver.com/v1/search/cafearticle"
    }.get(search_type)

    if not base_url:
        raise ValueError("search_typeì€ 'blog' ë˜ëŠ” 'cafe' ì´ì–´ì•¼ í•©ë‹ˆë‹¤")

    # ğŸ” ì •í™• ê²€ìƒ‰ê³¼ ì¼ë°˜ ê²€ìƒ‰ì„ ë³‘í–‰í•˜ì—¬ ë” ë§ì€ ê²°ê³¼ í™•ë³´
    queries = [
        urllib.parse.quote(f'"{keyword}"'),  # ì •í™• ê²€ìƒ‰
        urllib.parse.quote(keyword),  # ì¼ë°˜ ê²€ìƒ‰
        urllib.parse.quote(f"{keyword} ì•±"),  # ì•± ê´€ë ¨ ê²€ìƒ‰
        urllib.parse.quote(f"{keyword} ë¦¬ë·°")  # ë¦¬ë·° ê´€ë ¨ ê²€ìƒ‰
    ]
    
    all_results = []
    seen_links = set()
    
    for query in queries:
        # display ê°’ ë²”ìœ„ ì œí•œ (1-100)
        safe_display = min(max(1, display), 100)
        url = f"{base_url}?query={query}&display={safe_display}&sort=date"  # ìµœì‹ ìˆœ ì •ë ¬ ì¶”ê°€

        headers = {
            "X-Naver-Client-Id": NAVER_CLIENT_ID,
            "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
        }

        try:
            res = requests.get(url, headers=headers, timeout=10)
            
            if res.status_code == 200:
                items = res.json().get("items", [])
                # Extract user IDs from URLs and avoid duplicates
                for item in items:
                    link = item.get('link', '')
                    if link not in seen_links:
                        seen_links.add(link)
                        user_id = extract_user_id_from_url(item.get('bloggerlink', ''), link, search_type)
                        item['extracted_user_id'] = user_id
                        all_results.append(item)
                        
                        # ëª©í‘œ ìˆ˜ëŸ‰ì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ
                        if len(all_results) >= display:
                            break
            else:
                print(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {res.status_code} - {res.text}", file=sys.stderr)
                
        except requests.exceptions.RequestException as e:
            print(f"ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨: {str(e)}", file=sys.stderr)
            continue
        
        # ëª©í‘œ ìˆ˜ëŸ‰ì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ
        if len(all_results) >= display:
            break
    
    return all_results[:display]

def extract_text_from_html(html_content):
    """
    Extract plain text from HTML content
    
    Args:
        html_content: HTML string
        
    Returns:
        Plain text string
    """
    import re
    
    if not html_content:
        return ""
    
    # Remove HTML tags including <b>, <i>, <strong>, etc.
    clean_text = re.sub(r'<[^>]+>', '', html_content)
    
    # Decode HTML entities
    clean_text = clean_text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
    clean_text = clean_text.replace('&quot;', '"').replace('&#39;', "'")
    clean_text = clean_text.replace('&nbsp;', ' ').replace('&hellip;', '...')
    
    # Remove extra whitespace
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    return clean_text

def strip_html(html_content):
    """
    Strip HTML tags from content
    
    Args:
        html_content: HTML string
        
    Returns:
        Plain text string
    """
    import re
    
    if not html_content:
        return ""
    
    # Remove HTML tags including <b>, <i>, <strong>, etc.
    clean_text = re.sub(r'<[^>]+>', '', html_content)
    
    # Decode HTML entities
    clean_text = clean_text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
    clean_text = clean_text.replace('&quot;', '"').replace('&#39;', "'")
    clean_text = clean_text.replace('&nbsp;', ' ').replace('&hellip;', '...')
    
    # Remove extra whitespace
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    return clean_text

def is_likely_user_review(item, service_keywords):
    """
    Check if the search result item is likely a genuine user review
    
    Args:
        item: Search result item from Naver API
        service_keywords: List of service-related keywords
        
    Returns:
        Boolean indicating if this is likely a user review
    """
    try:
        # HTML ì œê±°
        title = strip_html(item.get("title", "")).lower()
        desc = strip_html(item.get("description", "")).lower()
        text = title + " " + desc

        # 1. ê°•í™”ëœ ë‰´ìŠ¤ê¸°ì‚¬ ë° ì •ë³´ì„± ê¸€ ì œì™¸
        exclusion_keywords = [
            "ë‰´ìŠ¤", "ê¸°ì‚¬", "ë³´ë„", "ë³´ë„ìë£Œ", "press", "ì–¸ë¡ ", "ë¯¸ë””ì–´", 
            "ê¸°ì", "ì·¨ì¬", "ì‹ ë¬¸", "ë°©ì†¡", "ë‰´ìŠ¤ë£¸", "ë³´ë„êµ­", "í¸ì§‘ë¶€", "news",
            "ê´€ë ¨ ê¸°ì‚¬", "ì†ë³´", "ë‹¨ë…", "íŠ¹ë³´", "ì¼ë³´", "íƒ€ì„ì¦ˆ", "í—¤ëŸ´ë“œ",
            # ì„œí‰/ë„ì„œ ê´€ë ¨ (ë„¤ì´ë²„ ì¹´í˜ì—ì„œ í”í•¨)
            "ì„œí‰", "ë„ì„œ", "ì±…", "ë…ì„œ", "ë…í›„ê°", "ë¦¬ë·° ì´ë²¤íŠ¸", "ì„œí‰ë‹¨",
            # ë°°ê²½í™”ë©´/í…Œë§ˆ ê´€ë ¨
            "ë°°ê²½í™”ë©´", "í…Œë§ˆ", "ë‹¤ìš´ë¡œë“œ", "ë¼ì´ë¸Œ ë°°ê²½í™”ë©´", "í”Œë¼ë°ê³ ",
            # ì¼ë°˜ì ì¸ ì •ë³´ì„± í¬ìŠ¤íŠ¸
            "ì •ë³´", "ì†Œì‹", "ì—…ë°ì´íŠ¸", "ë²„ì „", "ê¸°ëŠ¥", "íŠ¹ì§•", "ì„œë¹„ìŠ¤ ì†Œê°œ",
            "ìŠ¤í™", "ì‚¬ì–‘", "ê°€ê²©", "ìš”ê¸ˆ", "í”Œëœ", "êµ¬ë…", "ì„¤ì¹˜", "ì¶œì‹œ",
            # í™ë³´/ë§ˆì¼€íŒ…
            "ì´ë²¤íŠ¸", "í”„ë¡œëª¨ì…˜", "ê´‘ê³ ", "í™ë³´", "ë§ˆì¼€íŒ…", "ëŸ°ì¹­", "ê³µì‹",
            "ìº í˜ì¸", "ì•ˆë‚´", "ì•Œë¦¼", "ê³µì§€", "ë°œí‘œ"
        ]
        
        if any(keyword in text for keyword in exclusion_keywords):
            return False

        # 2. ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ í¬í•¨ í™•ì¸
        if not any(sk.lower() in text for sk in service_keywords):
            return False

        # 3. ê°•í™”ëœ ë¦¬ë·° ì§€í‘œ í™•ì¸
        review_indicators = [
            # ì‚¬ìš© ê²½í—˜ ê´€ë ¨
            "ì‚¬ìš©í•´ë³´ë‹ˆ", "ì¨ë³´ë‹ˆ", "ì²´í—˜í•´ë³´ë‹ˆ", "í…ŒìŠ¤íŠ¸í•´ë³´ë‹ˆ", "ì‚¬ìš©ê¸°", "ì²´í—˜ê¸°", "ì‚¬ìš©í›„ê¸°",
            # í‰ê°€ ê´€ë ¨
            "í›„ê¸°", "ë¦¬ë·°", "í‰ê°€", "í‰ì ", "ë³„ì ", "ë§Œì¡±", "ë¶ˆë§Œì¡±", "ë§Œì¡±ë„",
            # ì¶”ì²œ/ë¹„ì¶”ì²œ
            "ì¶”ì²œ", "ë¹„ì¶”ì²œ", "ê¶Œì¥", "ë¹„ê¶Œì¥", "ì¢‹ì•„ìš”", "ì‹«ì–´ìš”", "ê´œì°®ì•„ìš”",
            # ì¥ë‹¨ì 
            "ì¥ì ", "ë‹¨ì ", "ì•„ì‰¬ìš´", "ì¢‹ì€ì ", "ë‚˜ìœì ", "ë¬¸ì œì ", "ê°œì„ ì ",
            # ê°ì •ì  ë°˜ì‘
            "í¸ë¦¬", "ë¶ˆí¸", "ìœ ìš©", "ë„ì›€", "ì§œì¦", "ë‹µë‹µ", "ë§Œì¡±ìŠ¤ëŸ½", "ì‹¤ë§",
            # êµ¬ì²´ì ì¸ ì‚¬ìš© ìƒí™©
            "í†µí™”", "ì „í™”", "ë…¹ìŒ", "ìŒì„±", "ë³´ì´ìŠ¤í”¼ì‹±", "ë¹„ì„œ",
            # ì¼ë°˜ì ì¸ ì‚¬ìš©ì ë¦¬ë·° í‘œí˜„
            "ì •ë§", "ë„ˆë¬´", "ì™„ì „", "ì§„ì§œ", "ì†”ì§íˆ", "ê°œì¸ì ìœ¼ë¡œ"
        ]
        
        if not any(indicator in text for indicator in review_indicators):
            return False

        # 4. ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ì§§ìœ¼ë©´ ì œì™¸)
        if len(text) < 30:
            return False

        return True
        
    except Exception as e:
        print(f"Error in review filtering: {e}")
        return False