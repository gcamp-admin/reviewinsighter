#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Naver API Integration for Blog and Cafe Search
"""

import requests
import urllib.parse
import sys
import re

NAVER_CLIENT_ID = "YINpbvMCsck1Vr0PwwJd"
NAVER_CLIENT_SECRET = "gdi7lnyV1Z"

def extract_user_id_from_url(bloggerlink, link, search_type):
    """
    Extract user ID from Naver Blog or Cafe URL
    
    Args:
        bloggerlink: Blogger link (for blog)
        link: Direct link to the content
        search_type: 'blog' or 'cafe'
        
    Returns:
        Extracted user ID or default value
    """
    try:
        if search_type == "blog":
            # For blog, try to extract from bloggerlink first
            if bloggerlink:
                # Pattern: https://blog.naver.com/USERNAME
                match = re.search(r'blog\.naver\.com/([^/?]+)', bloggerlink)
                if match:
                    return match.group(1)
            
            # If bloggerlink fails, try from link
            if link:
                # Pattern: https://blog.naver.com/USERNAME/POST_ID
                match = re.search(r'blog\.naver\.com/([^/?]+)', link)
                if match:
                    return match.group(1)
                    
        elif search_type == "cafe":
            # For cafe, extract from link
            if link:
                # Pattern: https://cafe.naver.com/CAFE_NAME/ARTICLE_ID
                match = re.search(r'cafe\.naver\.com/([^/?]+)', link)
                if match:
                    return f"Ïπ¥Ìéò_{match.group(1)}"
                    
        return None
        
    except Exception as e:
        print(f"Error extracting user ID: {str(e)}", file=sys.stderr)
        return None

def search_naver(keyword, search_type="blog", display=10):
    """
    Search Naver Blog or Cafe articles
    
    Args:
        keyword: Search keyword
        search_type: 'blog' or 'cafe'
        display: Number of results to return (max 100)
        
    Returns:
        List of search results
    """
    base_url = {
        "blog": "https://openapi.naver.com/v1/search/blog",
        "cafe": "https://openapi.naver.com/v1/search/cafearticle"
    }.get(search_type)

    if not base_url:
        raise ValueError("search_typeÏùÄ 'blog' ÎòêÎäî 'cafe' Ïù¥Ïñ¥Ïïº Ìï©ÎãàÎã§")

    # üîê Ï†ïÌôï Í≤ÄÏÉâÍ≥º ÏùºÎ∞ò Í≤ÄÏÉâÏùÑ Î≥ëÌñâÌïòÏó¨ Îçî ÎßéÏùÄ Í≤∞Í≥º ÌôïÎ≥¥
    queries = [
        urllib.parse.quote(f'"{keyword}"'),  # Ï†ïÌôï Í≤ÄÏÉâ
        urllib.parse.quote(keyword),  # ÏùºÎ∞ò Í≤ÄÏÉâ
        urllib.parse.quote(f"{keyword} Ïï±"),  # Ïï± Í¥ÄÎ†® Í≤ÄÏÉâ
        urllib.parse.quote(f"{keyword} Î¶¨Î∑∞")  # Î¶¨Î∑∞ Í¥ÄÎ†® Í≤ÄÏÉâ
    ]
    
    all_results = []
    seen_links = set()
    
    for query in queries:
        url = f"{base_url}?query={query}&display={display}&sort=date"  # ÏµúÏã†Ïàú Ï†ïÎ†¨ Ï∂îÍ∞Ä

        headers = {
            "X-Naver-Client-Id": NAVER_CLIENT_ID,
            "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
        }

        try:
            res = requests.get(url, headers=headers, timeout=10)
            
            if res.status_code == 200:
                items = res.json().get("items", [])
                # Extract user IDs from URLs and avoid duplicates
                for item in items:
                    link = item.get('link', '')
                    if link not in seen_links:
                        seen_links.add(link)
                        user_id = extract_user_id_from_url(item.get('bloggerlink', ''), link, search_type)
                        item['extracted_user_id'] = user_id
                        all_results.append(item)
                        
                        # Î™©Ìëú ÏàòÎüâÏóê ÎèÑÎã¨ÌïòÎ©¥ Ï¢ÖÎ£å
                        if len(all_results) >= display:
                            break
            else:
                print(f"ÎÑ§Ïù¥Î≤Ñ API Ïò§Î•ò: {res.status_code} - {res.text}", file=sys.stderr)
                
        except requests.exceptions.RequestException as e:
            print(f"ÎÑ§Ïù¥Î≤Ñ API ÏöîÏ≤≠ Ïã§Ìå®: {str(e)}", file=sys.stderr)
            continue
        
        # Î™©Ìëú ÏàòÎüâÏóê ÎèÑÎã¨ÌïòÎ©¥ Ï¢ÖÎ£å
        if len(all_results) >= display:
            break
    
    return all_results[:display]

def extract_text_from_html(html_content):
    """
    Extract plain text from HTML content
    
    Args:
        html_content: HTML string
        
    Returns:
        Plain text string
    """
    import re
    
    if not html_content:
        return ""
    
    # Remove HTML tags including <b>, <i>, <strong>, etc.
    clean_text = re.sub(r'<[^>]+>', '', html_content)
    
    # Decode HTML entities
    clean_text = clean_text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
    clean_text = clean_text.replace('&quot;', '"').replace('&#39;', "'")
    clean_text = clean_text.replace('&nbsp;', ' ').replace('&hellip;', '...')
    
    # Remove extra whitespace
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    return clean_text

def strip_html(html_content):
    """
    Strip HTML tags from content
    
    Args:
        html_content: HTML string
        
    Returns:
        Plain text string
    """
    import re
    
    if not html_content:
        return ""
    
    # Remove HTML tags including <b>, <i>, <strong>, etc.
    clean_text = re.sub(r'<[^>]+>', '', html_content)
    
    # Decode HTML entities
    clean_text = clean_text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
    clean_text = clean_text.replace('&quot;', '"').replace('&#39;', "'")
    clean_text = clean_text.replace('&nbsp;', ' ').replace('&hellip;', '...')
    
    # Remove extra whitespace
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    return clean_text

def is_likely_user_review(item, service_keywords):
    """
    Check if the search result item is likely a genuine user review
    
    Args:
        item: Search result item from Naver API
        service_keywords: List of service-related keywords
        
    Returns:
        Boolean indicating if this is likely a user review
    """
    # HTML Ï†úÍ±∞
    title = strip_html(item.get("title", "")).lower()
    desc = strip_html(item.get("description", "")).lower()
    text = title + " " + desc

    # 1. ÌÇ§ÏõåÎìú Ìè¨Ìï® Î¨∏Ïû•
    review_signals = ["ÌõÑÍ∏∞", "Î¶¨Î∑∞", "ÏÇ¨Ïö©Í∏∞", "Ïç®Î¥§Ïñ¥Ïöî", "Ï∂îÏ≤ú", "Îã®Ï†ê", "Ïû•Ï†ê", "Î∂àÌé∏", "Ï¢ãÏïòÎçò Ï†ê"]
    if not any(kw in text for kw in review_signals):
        return False

    # 2. Í∏∞ÏÇ¨/Î∏åÎûúÎìú Ï§ëÏã¨ ÌÇ§ÏõåÎìú Î∞òÎ≥µ ‚Üí Ï†úÏô∏
    if text.count("press") > 0 or text.count("Î≥¥ÎèÑÏûêÎ£å") > 0:
        return False

    # 3. Í∏∏Ïù¥ Ï†úÌïú (ÎÑàÎ¨¥ ÏßßÏúºÎ©¥ Ï†úÏô∏)
    if len(text) < 30:
        return False

    # 4. ÏÑúÎπÑÏä§ ÌÇ§ÏõåÎìúÍ∞Ä Ìè¨Ìï®Îêú Í≥†Í∞ù Ïñ∏Ïñ¥Î°ú Ïì∞ÏòÄÎäîÏßÄ ÌôïÏù∏ (Î≥¥ÏôÑ)
    if not any(sk.lower() in text for sk in service_keywords):
        return False

    return True